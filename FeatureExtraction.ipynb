{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from pyarabic.araby import strip_tashkeel\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import qalsadi.lemmatizer \n",
    "import qalsadi.analex as qa\n",
    "\n",
    "from farasa.pos import FarasaPOSTagger \n",
    "from farasa.ner import FarasaNamedEntityRecognizer \n",
    "from farasa.diacratizer import FarasaDiacritizer \n",
    "from farasa.segmenter import FarasaSegmenter \n",
    "from farasa.stemmer import FarasaStemmer\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment words feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_words(words: list) -> list:\n",
    "    res = []\n",
    "    for word in words:\n",
    "        if len(word) == 1:\n",
    "            res.append('S')\n",
    "        else:\n",
    "            new_word = ''\n",
    "            new_word += 'B'\n",
    "            for i in range(1, len(word)-1):\n",
    "                new_word += 'I'\n",
    "            new_word += 'E'\n",
    "            res.append(new_word)\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_sentences(sentence: str) -> list:\n",
    "    words = sentence.split()\n",
    "    res = []\n",
    "    for word in words:\n",
    "        if word in ['،', '.']:\n",
    "            res.append(word)\n",
    "        else:\n",
    "            if len(word) == 1:\n",
    "                res.append('S')\n",
    "            else:\n",
    "                new_word = ''\n",
    "                for i in range(0, len(word)):\n",
    "                    if word[i] in ['،', '.']:\n",
    "                        new_word += word[i]\n",
    "                    elif i== 0:\n",
    "                        new_word += 'B'\n",
    "                    elif i == len(word)-1:\n",
    "                        new_word += 'E'\n",
    "                    else:\n",
    "                        new_word += 'I'\n",
    "                res.append(new_word)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words_stripped = []\n",
    "\n",
    "train_data = None\n",
    "with open('./Dataset/training/train_words_stripped.txt', 'r', encoding='utf-8') as file:\n",
    "    train_data = file.readlines()\n",
    "for line in train_data:\n",
    "    train_words_stripped.append(line.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_train_words = segment_words(train_words_stripped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BIIE', 'BE', 'BIE', 'BIIIE', 'BIE', 'BIE', 'BIE', 'BIIIIIE', 'BIE', 'BIIE']\n",
      "['عرفة', 'ابن', 'الزركشي', 'قال', 'إلخ', 'يده', 'الأول', 'قطع', 'أو', 'قوله']\n",
      "قوله\n",
      "BIIE\n",
      "أو\n",
      "BE\n"
     ]
    }
   ],
   "source": [
    "print(segmented_train_words[:10])\n",
    "print(train_words_stripped[9::-1]) #reverse so English and Arabic align (only printing purpose)\n",
    "print(train_words_stripped[0])\n",
    "print(segmented_train_words[0])\n",
    "print(train_words_stripped[1])\n",
    "print(segmented_train_words[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diacritics\n",
    "# {  ْ   , ّ   ,  ً   ,  َ   ,    ُ   ,  ِ    ,  ٍ   , ٌ    }\n",
    "\n",
    "# Golden\n",
    "## { َ  : 0, ً : 1, ُ : 2, ٌ : 3, ِ  : 4, ٍ  : 5, ْ : 6, ّ  : 7, ّ َ  : 8, ّ ً : 9, ّ ُ : 10, ّ ٌ : 11, ّ ِ  : 12,  ّ ٍ : 13, '': 14}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'َ', 'ُ', 'ً', 'ِ', 'ٍ', 'ّ', 'ْ', 'ٌ'}\n",
      "{'َ': 0, 'ً': 1, 'ُ': 2, 'ٌ': 3, 'ِ': 4, 'ٍ': 5, 'ْ': 6, 'ّ': 7, 'َّ': 8, 'ًّ': 9, 'ُّ': 10, 'ٌّ': 11, 'ِّ': 12, 'ٍّ': 13, '': 14}\n",
      "{'ل', 'ض', 'ي', 'ا', 'ح', 'ش', 'أ', 'ز', 'غ', 'ظ', 'ء', 'ب', 'ؤ', 'و', 'خ', 'ر', 'إ', 'م', 'ذ', 'ى', 'ن', 'ئ', 'ق', 'ص', 'ت', 'ط', 'ج', 'ك', 'ث', 'ف', 'ع', 'آ', 'س', 'د', 'ة', 'ه'}\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('./pickles/diacritics.pickle', 'rb') as file:\n",
    "    diacritics = pickle.load(file)\n",
    "\n",
    "with open('./pickles/diacritic2id.pickle', 'rb') as file:\n",
    "    diacritic2id = pickle.load(file)\n",
    "\n",
    "with open('./pickles/arabic_letters.pickle', 'rb') as file:\n",
    "    arabic_letters = pickle.load(file)\n",
    "\n",
    "print(diacritics)\n",
    "print(diacritic2id)\n",
    "print(arabic_letters)\n",
    "\n",
    "print(len(arabic_letters))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is used to get a dictionary of letters and a binary value if a certain dicritic appears after it\n",
    "#length of letters is 36 (28 letters + 8 special characters) and length of dicritics is 14\n",
    "#the function returns a dictionary of letters and a list of 14 binary values\n",
    "#utf-8 encoding for letters is used\n",
    "#for double diacritics we will checkk for arabic numerals \n",
    "#١ is shadda + tanween fatha\n",
    "#٢ is shadda + tanween damma\n",
    "#٣ is shadda + tanween kasra\n",
    "#٤ is shadda + fatha\n",
    "#٥ is shadda + damma\n",
    "#٦ is shadda + kasra\n",
    "\n",
    "#diacritic2id has 15 keys and values from 0 to 14 of the diacritics + \"\" (none)\n",
    "#arabic_letters has 36 keys and values from 0 to 35 of the letters\n",
    "\n",
    "with open('./Dataset/training/train_words_replaced.txt', 'r', encoding='utf-8') as file:\n",
    "    train_replace = file.readlines()\n",
    "list_of_words = []\n",
    "for sentence in train_replace:\n",
    "    list_of_words.append(sentence.strip())\n",
    "\n",
    "def get_letter_diacritics_appearance(list_of_words: list) -> dict:\n",
    "    dictionary = {}\n",
    "    for letters in arabic_letters:\n",
    "        dictionary[letters] = [0 for i in range(15)]\n",
    "\n",
    "\n",
    "    for word in list_of_words:\n",
    "        for i in range(len(word)):\n",
    "            if word[i] in arabic_letters:\n",
    "                if word[i] not in dictionary:# if the letter is not in the dictionary (mesh mohem awy laken mesh damen el dataset be amana)\n",
    "                    dictionary[word[i]] = [0 for i in range(15)]\n",
    "                if i+1 < len(word):\n",
    "                    if word[i+1] in diacritics:\n",
    "                        dictionary[word[i]][diacritic2id[word[i+1]]] = 1\n",
    "                    elif word[i+1] == '١':\n",
    "                        dictionary[word[i]][9] = 1\n",
    "                    elif word[i+1] == '٢':\n",
    "                        dictionary[word[i]][11] = 1\n",
    "                    elif word[i+1] == '٣':\n",
    "                        dictionary[word[i]][13] = 1\n",
    "                    elif word[i+1] == '٤':\n",
    "                        dictionary[word[i]][8] = 1\n",
    "                    elif word[i+1] == '٥':\n",
    "                        dictionary[word[i]][10] = 1\n",
    "                    elif word[i+1] == '٦':\n",
    "                        dictionary[word[i]][12] = 1\n",
    "                    elif word[i+1] not in diacritics:\n",
    "                        dictionary[word[i]][14] = 1\n",
    "    \n",
    "    for key in dictionary:\n",
    "        dictionary[key] = ''.join(map(str, dictionary[key]))\n",
    "\n",
    "    return dictionary\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Golden\n",
    "## { َ  : 0, ً : 1, ُ : 2, ٌ : 3, ِ  : 4, ٍ  : 5, ْ : 6, ّ  : 7, ّ َ  : 8, ّ ً : 9, ّ ُ : 10, ّ ٌ : 11, ّ ِ  : 12,  ّ ٍ : 13, '': 14}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ل': '111111111111111', 'ض': '111111111111111', 'ي': '111111111111111', 'ا': '101010101000001', 'ح': '111111111010111', 'ش': '111111111111111', 'أ': '111111100000001', 'ز': '111111111111111', 'غ': '111111101010101', 'ظ': '111111111111111', 'ء': '111111000000000', 'ب': '111111111111111', 'ؤ': '111111100000000', 'و': '111111111111111', 'خ': '111111111010111', 'ر': '111111111111111', 'إ': '000011000000001', 'م': '111111111111111', 'ذ': '111111111111111', 'ى': '101111101011111', 'ن': '111111111111111', 'ئ': '111111100000001', 'ق': '111111111111111', 'ص': '111111111111111', 'ت': '111111111111111', 'ط': '111111111111111', 'ج': '111111111111111', 'ك': '111111111111111', 'ث': '111111111111111', 'ف': '111111111111111', 'ع': '111111111010111', 'آ': '001000000000001', 'س': '111111111111111', 'د': '111111111111111', 'ة': '111111100000000', 'ه': '111111111011101'}\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dictionary = get_letter_diacritics_appearance(list_of_words)\n",
    "\n",
    "with open('./pickles/letter_diacritics_appearance.pickle', 'wb') as file:\n",
    "    pickle.dump(dictionary, file)\n",
    "\n",
    "\n",
    "print(dictionary)\n",
    "print(len(dictionary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "قوله أو قطع الأول يده إلخ قال الزركشي\n"
     ]
    }
   ],
   "source": [
    "# for each sentence in in stripped sentences for each letter in the sentence we put its corresponding diacritic appearance list in a list\n",
    "# so we have a list of list of lists\n",
    "\n",
    "with open('./Dataset/training/train_stripped.txt', 'r', encoding='utf-8') as file:\n",
    "    train_sentences_replace = file.readlines()\n",
    "    \n",
    "list_of_sentences = []\n",
    "for sentence in train_sentences_replace:\n",
    "    list_of_sentences.append(sentence.strip())\n",
    "\n",
    "print(list_of_sentences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Dataset/val/val_stripped.txt', 'r', encoding='utf-8') as file:\n",
    "    val_sentences_replace = file.readlines()\n",
    "    \n",
    "val_list_of_sentences = []\n",
    "for sentence in val_sentences_replace:\n",
    "    val_list_of_sentences.append(sentence.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_diacritics_appearance(list_of_sentences: list) -> list:\n",
    "    list_of_diactitics_appearance_in_sentences = []\n",
    "    for sentence in list_of_sentences:\n",
    "        string_of_diactitics_appearance_in_sentence = \"\"\n",
    "        for letter in sentence:\n",
    "            if letter in arabic_letters:\n",
    "                string_of_diactitics_appearance_in_sentence += dictionary[letter]+\" \"\n",
    "            else:\n",
    "                string_of_diactitics_appearance_in_sentence += '0'*14+'1'+\" \"\n",
    "        list_of_diactitics_appearance_in_sentences.append(string_of_diactitics_appearance_in_sentence.strip())\n",
    "        \n",
    "    return list_of_diactitics_appearance_in_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_diacritics_appearance = get_sentence_diacritics_appearance(list_of_sentences)\n",
    "with open('./pickles/sentence_diacritics_appearance.pickle', 'wb') as file:\n",
    "    pickle.dump(sentence_diacritics_appearance, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sentence_diacritics_appearance = get_sentence_diacritics_appearance(val_list_of_sentences)\n",
    "with open('./pickles/val_sentence_diacritics_appearance.pickle', 'wb') as file:\n",
    "    pickle.dump(val_sentence_diacritics_appearance, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['111111111111111 111111111111111 111111111111111 111111111011101 000000000000001 111111100000001 111111111111111 000000000000001 111111111111111 111111111111111 111111111010111 000000000000001 101010101000001 111111111111111 111111100000001 111111111111111 111111111111111 000000000000001 111111111111111 111111111111111 111111111011101 000000000000001 000011000000001 111111111111111 111111111010111 000000000000001 111111111111111 101010101000001 111111111111111 000000000000001 101010101000001 111111111111111 111111111111111 111111111111111 111111111111111 111111111111111 111111111111111', '101010101000001 111111111111111 111111111111111 000000000000001 111111111010111 111111111111111 111111111111111 111111100000000 000000000000001 111111111111111 111111111111111 111111111111111 111111111011101 000000000000001 111111111111111 111111111111111 111111111111111 111111111111111 000000000000001 111111111111111 111111111111111 111111111111111 111111111111111 111111111111111 111111111011101 000000000000001 111111111111111 000011000000001 111111111111111 111111111111111 101010101000001 111111111111111 000000000000001 111111101010101 111111111111111 111111111111111 000000000000001 111111111010111 111111111111111 111111111111111 111111111111111 000000000000001 111111111111111 101010101000001 111111111111111 000011000000001 111111111111111 111111111111111 101010101000001 111111111111111 000000000000001 111111111111111 111111111111111 111111111111111 111111111111111 000000000000001 111111111111111 101010101000001 000000000000001 111111111010111 111111111111111 111111111111111 000000000000001 111111111111111 111111111111111 111111111111111 111111111111111 111111111011101 000000000000001 111111111111111 111111111111111 000000000000001 101010101000001 111111111111111 111111111111111 111111111111111 111111111111111 000000000000001 111111111111111 111111111111111 111111111111111 111111111111111 111111100000000 000000000000001 111111111111111 000011000000001 111111111111111 111111111111111 101010101000001 111111000000000 000000000000001 111111111111111 111111111111111 111111111010111 111111111111111 000000000000001 111111111111111 111111111111111 111111111111111 111111111111111 000000000000001 111111111111111 111111111111111 111111111111111 000000000000001 111111111111111 111111111111111 101010101000001 111111111111111 000000000000001 101010101000001 111111111111111 111111111111111 000000000000001 111111111010111 111111111111111 111111111111111 111111100000000 000000000000001 111111111111111 111111111111111 111111111111111 000000000000001 101010101000001 111111111111111 111111111111111 000000000000001 111111111111111 101010101000001 111111111111111 000000000000001 111111100000001 111111111111111 000000000000001 111111111111111 111111111111111 111111111010111 111111111111111 000000000000001 111111111111111 111111111111111 111111111111111 111111111111111 111111111111111 111111111011101 000000000000001 111111111011101 111111111111111 000000000000001 111111111111111 111111111111111 111111111111111 111111111111111 000000000000001 101010101000001 111111111111111 111111111111111 111111111111111 101010101000001 111111111111111 000000000000001 111111111111111 000011000000001 111111111111111 111111111111111 101010101000001 111111000000000 000000000000001 101010101000001 111111111111111 111111111111111 111111111111111 111111111010111 111111111111111 000000000000001 111111111111111 111111111111111 000000000000001 111111111111111 111111111111111 111111111111111 111111111010111 000000000000001 101010101000001 111111111111111 111111111111111 111111111111111 101010101000001 111111111111111 111111100000000 000000000000001 111111111111111 101010101000001 111111111111111 111111111111111 111111111111111 111111111111111 111111111111111 000000000000001 111111111111111 111111111111111 111111111111111 111111111111111 111111111111111 000000000000001 111111111111111 111111111111111 111111111010111 111111111111111 000000000000001 111111111111111 111111111111111 111111111111111 000000000000001 111111111111111 111111111111111 111111111010111 111111111111111 000000000000001 111111111111111 111111111010111 111111111111111 111111111111111 000000000000001 111111111111111 111111111111111 111111111111111 000000000000001 111111111111111 101010101000001 111111111111111 111111111111111 000000000000001 111111111111111 111111100000001 111111111111111 111111111010111 101010101000001 111111111111111 111111111011101 000000000000001 111111100000001 111111111111111 000000000000001 101010101000001 111111111111111 111111111111111 101010101000001 111111111010111 111111111111111 000000000000001 111111111111111 101010101000001 111111111111111 111111111111111 000000000000001 111111111111111 101010101000001 111111111111111 111111111111111 111111111011101 000000000000001 111111111111111 111111111010111 101010101000001 111111111111111 101111101011111 000000000000001 111111111111111 101010101000001 111111111111111 000000000000001 111111111111111 101010101000001 111111111111111 111111111111111 000000000000001 111111111011101 111111111111111 000000000000001 111111111111111 101010101000001 111111111111111 111111111111111 111111111111111 111111111111111 111111111111111 111111111111111 000000000000001 000011000000001 111111111111111 101010101000001 000000000000001 111111111010111 111111111111111 111111111111111 000000000000001 101010101000001 111111111111111 111111111111111 111111111010111 111111111111111 000000000000001 111111111111111 111111111111111 111111111111111 111111111111111 111111111011101 000000000000001 111111111111111 111111111111111 111111111111111 000000000000001 111111111111111 111111111111111 111111111111111 000000000000001 111111111111111 111111111111111 111111111111111 111111111111111 111111111111111 000000000000001 000000000000001']\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(sentence_diacritics_appearance[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

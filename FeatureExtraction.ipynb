{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from pyarabic.araby import strip_tashkeel\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import qalsadi.lemmatizer \n",
    "import qalsadi.analex as qa\n",
    "\n",
    "from farasa.pos import FarasaPOSTagger \n",
    "from farasa.ner import FarasaNamedEntityRecognizer \n",
    "from farasa.diacratizer import FarasaDiacritizer \n",
    "from farasa.segmenter import FarasaSegmenter \n",
    "from farasa.stemmer import FarasaStemmer\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment words feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_words(words: list) -> list:\n",
    "    res = []\n",
    "    for word in words:\n",
    "        if len(word) == 1:\n",
    "            res.append('S')\n",
    "        else:\n",
    "            new_word = ''\n",
    "            new_word += 'B'\n",
    "            for i in range(1, len(word)-1):\n",
    "                new_word += 'I'\n",
    "            new_word += 'E'\n",
    "            res.append(new_word)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words_stripped = []\n",
    "\n",
    "train_data = None\n",
    "with open('./Dataset/training/train_words_stripped.txt', 'r', encoding='utf-8') as file:\n",
    "    train_data = file.readlines()\n",
    "for line in train_data:\n",
    "    train_words_stripped.append(line.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BIIE', 'BE', 'BIE', 'BIIIE', 'BIE', 'BIE', 'BIE', 'BIIIIIE', 'BIE', 'BIIE']\n",
      "['عرفة', 'ابن', 'الزركشي', 'قال', 'إلخ', 'يده', 'الأول', 'قطع', 'أو', 'قوله']\n",
      "قوله\n",
      "BIIE\n"
     ]
    }
   ],
   "source": [
    "segmented_train_words = segment_words(train_words_stripped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BIIE', 'BE', 'BIE', 'BIIIE', 'BIE', 'BIE', 'BIE', 'BIIIIIE', 'BIE', 'BIIE']\n",
      "['عرفة', 'ابن', 'الزركشي', 'قال', 'إلخ', 'يده', 'الأول', 'قطع', 'أو', 'قوله']\n",
      "قوله\n",
      "BIIE\n",
      "أو\n",
      "BE\n"
     ]
    }
   ],
   "source": [
    "print(segmented_train_words[:10])\n",
    "print(train_words_stripped[9::-1]) #reverse so English and Arabic align (only printing purpose)\n",
    "print(train_words_stripped[0])\n",
    "print(segmented_train_words[0])\n",
    "print(train_words_stripped[1])\n",
    "print(segmented_train_words[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diacritics\n",
    "# {  ْ   , ّ   ,  ً   ,  َ   ,    ُ   ,  ِ    ,  ٍ   , ٌ    }\n",
    "\n",
    "# Golden\n",
    "## { َ  : 0, ً : 1, ُ : 2, ٌ : 3, ِ  : 4, ٍ  : 5, ْ : 6, ّ  : 7, ّ َ  : 8, ّ ً : 9, ّ ُ : 10, ّ ٌ : 11, ّ ِ  : 12,  ّ ٍ : 13, '': 14}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ْ', 'ّ', 'ً', 'َ', 'ُ', 'ِ', 'ٍ', 'ٌ'}\n",
      "{'َ': 0, 'ً': 1, 'ُ': 2, 'ٌ': 3, 'ِ': 4, 'ٍ': 5, 'ْ': 6, 'ّ': 7, 'َّ': 8, 'ًّ': 9, 'ُّ': 10, 'ٌّ': 11, 'ِّ': 12, 'ٍّ': 13, '': 14}\n",
      "{'ت', 'ث', 'ك', 'ح', 'ب', 'ن', 'غ', 'م', 'ص', 'إ', 'ع', 'س', 'آ', 'ف', 'خ', 'ة', 'ض', 'أ', 'ق', 'ظ', 'ج', 'ى', 'ئ', 'ؤ', 'ء', 'ذ', 'ز', 'ش', 'ا', 'ط', 'د', 'ل', 'و', 'ه', 'ر', 'ي'}\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('./pickles/diacritics.pickle', 'rb') as file:\n",
    "    diacritics = pickle.load(file)\n",
    "\n",
    "with open('./pickles/diacritic2id.pickle', 'rb') as file:\n",
    "    diacritic2id = pickle.load(file)\n",
    "\n",
    "with open('./pickles/arabic_letters.pickle', 'rb') as file:\n",
    "    arabic_letters = pickle.load(file)\n",
    "\n",
    "print(diacritics)\n",
    "print(diacritic2id)\n",
    "print(arabic_letters)\n",
    "\n",
    "print(len(arabic_letters))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is used to get a dictionary of letters and a binary value if a certain dicritic appears after it\n",
    "#length of letters is 36 (28 letters + 8 special characters) and length of dicritics is 14\n",
    "#the function returns a dictionary of letters and a list of 14 binary values\n",
    "#utf-8 encoding for letters is used\n",
    "#for double diacritics we will checkk for arabic numerals \n",
    "#١ is shadda + tanween fatha\n",
    "#٢ is shadda + tanween damma\n",
    "#٣ is shadda + tanween kasra\n",
    "#٤ is shadda + fatha\n",
    "#٥ is shadda + damma\n",
    "#٦ is shadda + kasra\n",
    "\n",
    "#diacritic2id has 15 keys and values from 0 to 14 of the diacritics + \"\" (none)\n",
    "#arabic_letters has 36 keys and values from 0 to 35 of the letters\n",
    "\n",
    "with open('./Dataset/training/train_words_replaced.txt', 'r', encoding='utf-8') as file:\n",
    "    train_replace = file.readlines()\n",
    "list_of_words = []\n",
    "for sentence in train_replace:\n",
    "    list_of_words.append(sentence.strip())\n",
    "\n",
    "def get_letter_diacritics_appearance(list_of_words: list) -> dict:\n",
    "    dictionary = {}\n",
    "    for letters in arabic_letters:\n",
    "        dictionary[letters] = [0 for i in range(15)]\n",
    "\n",
    "\n",
    "    for word in list_of_words:\n",
    "        for i in range(len(word)):\n",
    "            if word[i] in arabic_letters:\n",
    "                if word[i] not in dictionary:# if the letter is not in the dictionary (mesh mohem awy laken mesh damen el dataset be amana)\n",
    "                    dictionary[word[i]] = [0 for i in range(15)]\n",
    "                if i+1 < len(word):\n",
    "                    if word[i+1] in diacritics:\n",
    "                        dictionary[word[i]][diacritic2id[word[i+1]]] = 1\n",
    "                    elif word[i+1] == '١':\n",
    "                        dictionary[word[i]][9] = 1\n",
    "                    elif word[i+1] == '٢':\n",
    "                        dictionary[word[i]][11] = 1\n",
    "                    elif word[i+1] == '٣':\n",
    "                        dictionary[word[i]][13] = 1\n",
    "                    elif word[i+1] == '٤':\n",
    "                        dictionary[word[i]][8] = 1\n",
    "                    elif word[i+1] == '٥':\n",
    "                        dictionary[word[i]][10] = 1\n",
    "                    elif word[i+1] == '٦':\n",
    "                        dictionary[word[i]][12] = 1\n",
    "                    elif word[i+1] not in diacritics:\n",
    "                        dictionary[word[i]][14] = 1\n",
    "    \n",
    "    return dictionary\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Golden\n",
    "## { َ  : 0, ً : 1, ُ : 2, ٌ : 3, ِ  : 4, ٍ  : 5, ْ : 6, ّ  : 7, ّ َ  : 8, ّ ً : 9, ّ ُ : 10, ّ ٌ : 11, ّ ِ  : 12,  ّ ٍ : 13, '': 14}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1]\n",
      "[1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dictionary = get_letter_diacritics_appearance(list_of_words)\n",
    "\n",
    "with open('./pickles/letter_diacritics_appearance.pickle', 'wb') as file:\n",
    "    pickle.dump(dictionary, file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "قوله أو قطع الأول يده إلخ قال الزركشي\n"
     ]
    }
   ],
   "source": [
    "# for each sentence in in stripped sentences for each letter in the sentence we put its corresponding diacritic appearance list in a list\n",
    "# so we have a list of list of lists\n",
    "\n",
    "with open('./Dataset/training/train_stripped.txt', 'r', encoding='utf-8') as file:\n",
    "    train_sentences_replace = file.readlines()\n",
    "list_of_sentences = []\n",
    "for sentence in train_sentences_replace:\n",
    "    list_of_sentences.append(sentence.strip())\n",
    "\n",
    "print(list_of_sentences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_sentence_diacritics_appearance(list_of_sentences: list) -> list:\n",
    "    list_of_diactitics_appearance_in_sentences = []\n",
    "    for sentence in list_of_sentences:\n",
    "        list_of_diactitics_appearance_in_sentence = []\n",
    "        for letter in sentence:\n",
    "            if letter in arabic_letters:\n",
    "                list_of_diactitics_appearance_in_sentence.append(dictionary[letter])\n",
    "            elif letter == ' ':\n",
    "                list_of_diactitics_appearance_in_sentence.append([0 for i in range(15)])\n",
    "        list_of_diactitics_appearance_in_sentences.append(list_of_diactitics_appearance_in_sentence)\n",
    "        \n",
    "    return list_of_diactitics_appearance_in_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_diacritics_appearance = get_sentence_diacritics_appearance(list_of_sentences)\n",
    "\n",
    "with open('./pickles/sentence_diacritics_appearance.pickle', 'wb') as file:\n",
    "    pickle.dump(sentence_diacritics_appearance, file)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

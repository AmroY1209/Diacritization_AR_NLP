{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we read the data and clean it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyarabic\n",
    "# !pip install qalsadi\n",
    "# !pip install farasapy\n",
    "# !pip install diacritization_evaluation\n",
    "# !pip install nltl\n",
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from pyarabic.araby import strip_tashkeel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unicode_sequences(arabic_words: list) -> list:\n",
    "    res = []\n",
    "    for word in arabic_words:\n",
    "        new_word = word\n",
    "        if '\\u0651\\u064B' in new_word:\n",
    "            new_word = (re.sub(r'\\u0651\\u064B', '١', new_word))\n",
    "        if '\\u0651\\u064C' in new_word:\n",
    "            new_word = (re.sub(r'\\u0651\\u064C', '٢', new_word))\n",
    "        if '\\u0651\\u064D' in new_word:\n",
    "            new_word = (re.sub(r'\\u0651\\u064D', '٣', new_word))\n",
    "        if '\\u0651\\u064E' in new_word:\n",
    "            new_word = (re.sub(r'\\u0651\\u064E', '٤', new_word))\n",
    "        if '\\u0651\\u064F' in new_word:\n",
    "            new_word = (re.sub(r'\\u0651\\u064F', '٥', new_word))\n",
    "        if '\\u0651\\u0650' in new_word:\n",
    "            new_word = (re.sub(r'\\u0651\\u0650', '٦', new_word))\n",
    "        res.append(new_word)\n",
    "    return res\n",
    "\n",
    "def replace_unicode_sequences_in_sentence(sentence: str):\n",
    "    words = sentence.split()\n",
    "    replaced_words = replace_unicode_sequences(words)\n",
    "    return ' '.join(replaced_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_words(words: list) -> list:\n",
    "    res = []\n",
    "    for word in words:\n",
    "        if len(word) == 1:\n",
    "            res.append('S')\n",
    "        else:\n",
    "            new_word = ''\n",
    "            new_word += 'B'\n",
    "            for i in range(1, len(word)-1):\n",
    "                new_word += 'I'\n",
    "            new_word += 'E'\n",
    "            res.append(new_word)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = './Dataset/train.txt'\n",
    "\n",
    "# # Read Arabic text from the file\n",
    "# with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#     arabic_text_from_file = file.read()\n",
    "\n",
    "# #print(arabic_text_from_file)\n",
    "\n",
    "# #Split the text into words\n",
    "# arabic_words = arabic_text_from_file.split()\n",
    "\n",
    "# #remove brackets, commas, dots, numbers using regex\n",
    "# arabic_words = [re.sub(r'[^\\u0600-\\u0660\\.]+', '', word) for word in arabic_words]\n",
    "# stop_symbols = ['.', '،', '؟', '؛', '!', ':','...', '?.']\n",
    "# arabic_words = [word for word in arabic_words if word and (len(word) > 1 or word in stop_symbols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./Dataset/words.txt', 'w', encoding='utf-8') as file:\n",
    "#     for word in arabic_words:\n",
    "#         file.write(word + '\\n')\n",
    "\n",
    "# #join the words into a string and split them according to sentences that end with a dot, question mark, or exclamation mark\n",
    "# arabic_sentences = ' '.join(arabic_words)\n",
    "# arabic_sentences = re.split(r'[\\.\\u061B\\u061F]', arabic_sentences) \n",
    "# arabic_sentences = [sentence for sentence in arabic_sentences if sentence]\n",
    "\n",
    "# with open('./Dataset/sentences.txt', 'w', encoding='utf-8') as file:\n",
    "#     for sentence in arabic_sentences:\n",
    "#         file.write(sentence + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words without tashkeel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arabic_words_without_punc = [re.sub(r'([^\\u0600-\\u0660\\.]+)|[،\\.\\u061A-\\u061F]+', '', word) for word in arabic_words]\n",
    "# arabic_words_without_punc = [word for word in arabic_words_without_punc if word and (len(word) > 1 or word in stop_symbols)]\n",
    "# arabic_words_without_punc = list((strip_tashkeel(word) for word in arabic_words_without_punc)) \n",
    "\n",
    "# with open('./Dataset/WordsWithoutTashkeel.txt', 'w', encoding='utf-8') as output_file:\n",
    "#     for word in arabic_words_without_punc:\n",
    "#         output_file.write((word) + '\\n')\n",
    "\n",
    "# with open('./Dataset/SentencesWithoutTashkeel.txt', 'w', encoding='utf-8') as output_file:\n",
    "#     for sentence in arabic_sentences:\n",
    "#         output_file.write(strip_tashkeel(sentence) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = replace_unicode_sequences(arabic_words)\n",
    "# print(len(res))\n",
    "# with open('./Dataset/words_new_approach.txt', 'w', encoding='utf-8') as file:\n",
    "#     for word in res:\n",
    "#         file.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #join the words into a string and split them according to sentences that end with a dot, question mark, or exclamation mark\n",
    "# arabic_sentences = ' '.join(res)\n",
    "# arabic_sentences = re.split(r'[\\.\\u061B\\u061F]', arabic_sentences) \n",
    "# arabic_sentences = [sentence for sentence in arabic_sentences if sentence]\n",
    "\n",
    "# with open('./Dataset/sentences_new_approach.txt', 'w', encoding='utf-8') as file:\n",
    "#     for sentence in arabic_sentences:\n",
    "#         file.write(sentence + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REFACTORED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Dataset/training/train.txt', 'r', encoding='utf-8') as file:\n",
    "    train_txt = file.read()\n",
    "\n",
    "train_words = []\n",
    "with open('./Dataset/training/train_cleaned.txt', 'w', encoding='utf-8') as cleaned_file:\n",
    "    with open('./Dataset/training/train_stripped.txt', 'w', encoding='utf-8') as output_file:\n",
    "        with open('./Dataset/training/train_replace.txt', 'w', encoding='utf-8') as replace_file:\n",
    "            with open('./Dataset/training/train_words.txt', 'w', encoding='utf-8') as words_file:\n",
    "                for sentence in train_txt.split('\\n'):\n",
    "                    sentence = re.sub(r'[^\\u0600-\\u0660 \\.]+', '', sentence)\n",
    "                    sentence = re.sub(r' +', ' ', sentence)\n",
    "                    sentence = sentence.strip()\n",
    "                    cleaned_file.write(sentence + '\\n')\n",
    "                    output_file.write(strip_tashkeel(sentence) + '\\n')\n",
    "                    replace_file.write(replace_unicode_sequences_in_sentence(sentence) + '\\n')\n",
    "                    for word in sentence.split():\n",
    "                        word = re.sub(r'[\\.؛،]', '', word)\n",
    "                        word = word.strip()\n",
    "                        if word:\n",
    "                            train_words.append(word)\n",
    "                            words_file.write(word + '\\n') \n",
    "\n",
    "with open('./Dataset/training/train_words_replaced.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for word in replace_unicode_sequences(train_words):\n",
    "        output_file.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Dataset/val/val.txt', 'r', encoding='utf-8') as file:\n",
    "    val_txt = file.read()\n",
    "\n",
    "val_words = []\n",
    "with open('./Dataset/val/val_cleaned.txt', 'w', encoding='utf-8') as cleaned_file:\n",
    "    with open('./Dataset/val/val_stripped.txt', 'w', encoding='utf-8') as output_file:\n",
    "        with open('./Dataset/val/val_replaced.txt', 'w', encoding='utf-8') as replaced_file:\n",
    "            with open('./Dataset/val/val_words.txt', 'w', encoding='utf-8') as words_file:\n",
    "                for sentence in val_txt.split('\\n'):\n",
    "                    sentence = re.sub(r'[^\\u0600-\\u0660 \\.]+', '', sentence)\n",
    "                    sentence = re.sub(r' +', ' ', sentence)\n",
    "                    sentence = sentence.strip()\n",
    "                    cleaned_file.write(sentence + '\\n')\n",
    "                    output_file.write(strip_tashkeel(sentence) + '\\n')\n",
    "                    replaced_file.write(replace_unicode_sequences_in_sentence(sentence) + '\\n')\n",
    "                    for word in sentence.split():\n",
    "                        word = re.sub(r'[\\.؛،]', '', word)\n",
    "                        word = word.strip()\n",
    "                        if word:\n",
    "                            val_words.append(word)\n",
    "                            words_file.write(word + '\\n') \n",
    "\n",
    "with open('./Dataset/val/val_words_replaced.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for word in replace_unicode_sequences(val_words):\n",
    "        output_file.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmenting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words_stripped = list((strip_tashkeel(word) for word in train_words))\n",
    "with open('./Dataset/training/train_words_stripped.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for word in train_words_stripped:\n",
    "        output_file.write(word + '\\n')\n",
    "val_words_stripped = list((strip_tashkeel(word) for word in val_words))\n",
    "with open('./Dataset/val/val_words_stripped.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for word in val_words_stripped:\n",
    "        output_file.write(word + '\\n')\n",
    "segmented_train_words = segment_words(train_words_stripped)\n",
    "segmented_val_words = segment_words(val_words_stripped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Segmentation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['قَوْلُهُ', 'وَلَا', 'تُكْرَهُ', 'ضِيَافَتُهُ', 'الْفَرْقُ', 'الثَّالِثُ', 'وَالثَّلَاثُونَ', 'بَيْنَ', 'قَاعِدَةِ', 'تَقَدُّمِ']\n",
      "['BIIE', 'BIIIE', 'BIE', 'BIIIIIIIE', 'BIIIIE', 'BIIIE', 'BIIIIE', 'BIIE', 'BIE', 'BIIE']\n"
     ]
    }
   ],
   "source": [
    "print(val_words[:10])\n",
    "print(segmented_val_words[9::-1]) #reverse so English and Arabic align (only printing purpose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['قوله', 'أو', 'قطع', 'الأول', 'يده', 'إلخ', 'قال', 'الزركشي', 'ابن', 'عرفة']\n",
      "['BIIE', 'BIE', 'BIIIIIE', 'BIE', 'BIE', 'BIE', 'BIIIE', 'BIE', 'BE', 'BIIE']\n"
     ]
    }
   ],
   "source": [
    "print(train_words_stripped[:10])\n",
    "print(segmented_train_words[9::-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

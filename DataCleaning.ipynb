{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we read the data and clean it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyarabic\n",
    "# !pip install qalsadi\n",
    "# !pip install farasapy\n",
    "# !pip install diacritization_evaluation\n",
    "# !pip install nltk\n",
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from pyarabic.araby import strip_tashkeel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unicode_sequences(arabic_words: list) -> list:\n",
    "    res = []\n",
    "    for word in arabic_words:\n",
    "        new_word = word\n",
    "        if '\\u0651\\u064B' in new_word:\n",
    "            new_word = (re.sub(r'\\u0651\\u064B', '١', new_word)) # shadda + tanween fatha\n",
    "        if '\\u0651\\u064C' in new_word:\n",
    "            new_word = (re.sub(r'\\u0651\\u064C', '٢', new_word))# shadda + tanween damma\n",
    "        if '\\u0651\\u064D' in new_word:\n",
    "            new_word = (re.sub(r'\\u0651\\u064D', '٣', new_word))# shadda + tanween kasra\n",
    "        if '\\u0651\\u064E' in new_word:\n",
    "            new_word = (re.sub(r'\\u0651\\u064E', '٤', new_word))# shadda + fatha\n",
    "        if '\\u0651\\u064F' in new_word:\n",
    "            new_word = (re.sub(r'\\u0651\\u064F', '٥', new_word))# shadda + damma\n",
    "        if '\\u0651\\u0650' in new_word:\n",
    "            new_word = (re.sub(r'\\u0651\\u0650', '٦', new_word))# shadda + kasra\n",
    "        res.append(new_word)\n",
    "    return res\n",
    "\n",
    "def replace_unicode_sequences_in_sentence(sentence: str):\n",
    "    words = sentence.split()\n",
    "    replaced_words = replace_unicode_sequences(words)\n",
    "    return ' '.join(replaced_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = './Dataset/train.txt'\n",
    "\n",
    "# # Read Arabic text from the file\n",
    "# with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#     arabic_text_from_file = file.read()\n",
    "\n",
    "# #print(arabic_text_from_file)\n",
    "\n",
    "# #Split the text into words\n",
    "# arabic_words = arabic_text_from_file.split()\n",
    "\n",
    "# #remove brackets, commas, dots, numbers using regex\n",
    "# arabic_words = [re.sub(r'[^\\u0600-\\u0660\\.]+', '', word) for word in arabic_words]\n",
    "# stop_symbols = ['.', '،', '؟', '؛', '!', ':','...', '?.']\n",
    "# arabic_words = [word for word in arabic_words if word and (len(word) > 1 or word in stop_symbols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./Dataset/words.txt', 'w', encoding='utf-8') as file:\n",
    "#     for word in arabic_words:\n",
    "#         file.write(word + '\\n')\n",
    "\n",
    "# #join the words into a string and split them according to sentences that end with a dot, question mark, or exclamation mark\n",
    "# arabic_sentences = ' '.join(arabic_words)\n",
    "# arabic_sentences = re.split(r'[\\.\\u061B\\u061F]', arabic_sentences) \n",
    "# arabic_sentences = [sentence for sentence in arabic_sentences if sentence]\n",
    "\n",
    "# with open('./Dataset/sentences.txt', 'w', encoding='utf-8') as file:\n",
    "#     for sentence in arabic_sentences:\n",
    "#         file.write(sentence + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words without tashkeel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arabic_words_without_punc = [re.sub(r'([^\\u0600-\\u0660\\.]+)|[،\\.\\u061A-\\u061F]+', '', word) for word in arabic_words]\n",
    "# arabic_words_without_punc = [word for word in arabic_words_without_punc if word and (len(word) > 1 or word in stop_symbols)]\n",
    "# arabic_words_without_punc = list((strip_tashkeel(word) for word in arabic_words_without_punc)) \n",
    "\n",
    "# with open('./Dataset/WordsWithoutTashkeel.txt', 'w', encoding='utf-8') as output_file:\n",
    "#     for word in arabic_words_without_punc:\n",
    "#         output_file.write((word) + '\\n')\n",
    "\n",
    "# with open('./Dataset/SentencesWithoutTashkeel.txt', 'w', encoding='utf-8') as output_file:\n",
    "#     for sentence in arabic_sentences:\n",
    "#         output_file.write(strip_tashkeel(sentence) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = replace_unicode_sequences(arabic_words)\n",
    "# print(len(res))\n",
    "# with open('./Dataset/words_new_approach.txt', 'w', encoding='utf-8') as file:\n",
    "#     for word in res:\n",
    "#         file.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #join the words into a string and split them according to sentences that end with a dot, question mark, or exclamation mark\n",
    "# arabic_sentences = ' '.join(res)\n",
    "# arabic_sentences = re.split(r'[\\.\\u061B\\u061F]', arabic_sentences) \n",
    "# arabic_sentences = [sentence for sentence in arabic_sentences if sentence]\n",
    "\n",
    "# with open('./Dataset/sentences_new_approach.txt', 'w', encoding='utf-8') as file:\n",
    "#     for sentence in arabic_sentences:\n",
    "#         file.write(sentence + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REFACTORED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Dataset/training/train.txt', 'r', encoding='utf-8') as file:\n",
    "    train_txt = file.read()\n",
    "\n",
    "train_words = []\n",
    "with open('./Dataset/training/train_cleaned.txt', 'w', encoding='utf-8') as cleaned_file:\n",
    "    with open('./Dataset/training/train_stripped.txt', 'w', encoding='utf-8') as output_file:\n",
    "        with open('./Dataset/training/train_replace.txt', 'w', encoding='utf-8') as replace_file:\n",
    "            with open('./Dataset/training/train_words.txt', 'w', encoding='utf-8') as words_file:\n",
    "                for sentence in train_txt.split('\\n'):\n",
    "                    sentence = re.sub(r'[^\\u0600-\\u0660 \\.]+|[؛؟]', '', sentence)\n",
    "                    sentence = re.sub(r' +', ' ', sentence)\n",
    "                    sentence = sentence.strip()\n",
    "                    cleaned_file.write(sentence + '\\n')\n",
    "                    output_file.write(strip_tashkeel(sentence) + '\\n')\n",
    "                    replace_file.write(replace_unicode_sequences_in_sentence(sentence) + '\\n')\n",
    "                    for word in sentence.split():\n",
    "                        word = word.strip()\n",
    "                        if word:\n",
    "                            train_words.append(word)\n",
    "                            words_file.write(word + '\\n') \n",
    "\n",
    "with open('./Dataset/training/train_words_replaced.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for word in replace_unicode_sequences(train_words):\n",
    "        output_file.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Dataset/val/val.txt', 'r', encoding='utf-8') as file:\n",
    "    val_txt = file.read()\n",
    "\n",
    "val_words = []\n",
    "with open('./Dataset/val/val_cleaned.txt', 'w', encoding='utf-8') as cleaned_file:\n",
    "    with open('./Dataset/val/val_stripped.txt', 'w', encoding='utf-8') as output_file:\n",
    "        with open('./Dataset/val/val_replaced.txt', 'w', encoding='utf-8') as replaced_file:\n",
    "            with open('./Dataset/val/val_words.txt', 'w', encoding='utf-8') as words_file:\n",
    "                for sentence in val_txt.split('\\n'):\n",
    "                    sentence = re.sub(r'[^\\u0600-\\u0660 \\.]+|[؛؟]', '', sentence)\n",
    "                    sentence = re.sub(r' +', ' ', sentence)\n",
    "                    sentence = sentence.strip()\n",
    "                    cleaned_file.write(sentence + '\\n')\n",
    "                    output_file.write(strip_tashkeel(sentence) + '\\n')\n",
    "                    replaced_file.write(replace_unicode_sequences_in_sentence(sentence) + '\\n')\n",
    "                    for word in sentence.split():\n",
    "                        word = word.strip()\n",
    "                        if word:\n",
    "                            val_words.append(word)\n",
    "                            words_file.write(word + '\\n') \n",
    "\n",
    "with open('./Dataset/val/val_words_replaced.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for word in replace_unicode_sequences(val_words):\n",
    "        output_file.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strip Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words_stripped = list((strip_tashkeel(word) for word in train_words))\n",
    "with open('./Dataset/training/train_words_stripped.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for word in train_words_stripped:\n",
    "        output_file.write(word + '\\n')\n",
    "val_words_stripped = list((strip_tashkeel(word) for word in val_words))\n",
    "with open('./Dataset/val/val_words_stripped.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for word in val_words_stripped:\n",
    "        output_file.write(word + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "# Encodes categorical labels into numerical format (used for label preprocessing)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Calculates the accuracy of a classification model (used for model evaluation)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Defines a custom dataset class for PyTorch (used for handling data)\n",
    "import torch.utils.data\n",
    "\n",
    "# Creates a DataLoader for efficient batch processing in PyTorch (used for data loading)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Splits a dataset into training and validation sets (used for data splitting)\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Represents a multi-dimensional matrix in PyTorch (used for tensor manipulation)\n",
    "from torch import Tensor\n",
    "\n",
    "# Implements a linear layer in a neural network (used for defining neural network architecture)\n",
    "from torch.nn import Linear\n",
    "\n",
    "# Applies rectified linear unit (ReLU) activation function (used for introducing non-linearity)\n",
    "from torch.nn import ReLU\n",
    "\n",
    "# Applies sigmoid activation function (used for binary classification output)\n",
    "from torch.nn import Sigmoid\n",
    "\n",
    "# Base class for all neural network modules in PyTorch (used for creating custom models)\n",
    "from torch.nn import Module\n",
    "\n",
    "# Stochastic Gradient Descent optimizer (used for model optimization during training)\n",
    "from torch.optim import SGD\n",
    "\n",
    "# Binary Cross Entropy Loss function (used for binary classification problems)\n",
    "from torch.nn import BCELoss\n",
    "\n",
    "# Initializes weights using Kaiming uniform initialization (used for weight initialization)\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "\n",
    "# Initializes weights using Xavier (Glorot) uniform initialization (used for weight initialization)\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('./pickles/word_sequences.pkl', 'rb') as file:\n",
    "    word_sequences = pickle.load(file)\n",
    "\n",
    "with open('./pickles/char_sequences_without_tashkeel.pkl', 'rb') as file:\n",
    "    char_sequences = pickle.load(file)\n",
    "\n",
    "with open('./pickles/tashkeel_sequences.pkl', 'rb') as file:\n",
    "    labels = pickle.load(file)\n",
    "\n",
    "with open('./pickles/val_word_sequences.pkl', 'rb') as file:\n",
    "    val_word_sequences = pickle.load(file)\n",
    "\n",
    "with open('./pickles/val_char_sequences_without_tashkeel.pkl', 'rb') as file:\n",
    "    val_char_sequences = pickle.load(file)\n",
    "\n",
    "with open('./pickles/val_tashkeel_sequences.pkl', 'rb') as file:\n",
    "    val_labels = pickle.load(file)\n",
    "\n",
    "with open('./pickles/sentence_diacritics_appearance_sequences.pickle', 'rb') as file:\n",
    "    test_sentences_diacritics_sequences = pickle.load(file)\n",
    "\n",
    "with open('./pickles/val_sentence_diacritics_appearance_sequences.pickle', 'rb') as file:\n",
    "    val_sentences_diacritics_sequences = pickle.load(file)\n",
    "\n",
    "with open('./pickles/segment_sequences.pickle', 'rb') as file:\n",
    "    train_segment_sequences = pickle.load(file)\n",
    "\n",
    "with open('./pickles/val_segment_sequences.pickle', 'rb') as file:\n",
    "    val_segment_sequences = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word_sequences))\n",
    "print(len(char_sequences[1]))\n",
    "print(len(labels[1]))\n",
    "print(len(test_sentences_diacritics_sequences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_characters(characters):\n",
    "    # Create a tensor of zeros with the same shape as the last subsequence\n",
    "    zeros_tensor = torch.zeros_like(characters[:, 0:1, :])\n",
    "\n",
    "    # Concatenate it to the original tensor along the second dimension\n",
    "    padded_x = torch.cat((characters, zeros_tensor), dim=1)\n",
    "\n",
    "    # Now, padded_x will have zeros padded to the last subsequence\n",
    "\n",
    "    temp1 = padded_x[:, :-1, :]\n",
    "    temp2 = padded_x[:, 1:, :]\n",
    "\n",
    "    # Concatenate along the last dimension\n",
    "    concatenated_characters = torch.cat((temp1, temp2), dim=-1)\n",
    "\n",
    "    return concatenated_characters\n",
    "\n",
    "def concatenate_characters2(characters):\n",
    "    # Create a tensor of zeros with the same shape as the last subsequence\n",
    "    zeros_tensor = torch.zeros_like(characters[:, 0:1])\n",
    "\n",
    "    # Concatenate it to the original tensor along the second dimension\n",
    "    padded_x = torch.cat((characters, zeros_tensor), dim=1)\n",
    "\n",
    "    # Now, padded_x will have zeros padded to the last subsequence\n",
    "\n",
    "    temp1 = padded_x[:, :-1]\n",
    "    temp2 = padded_x[:, 1:]\n",
    "\n",
    "    # Concatenate along the last dimension\n",
    "    concatenated_characters = torch.cat((temp1, temp2), dim=-1)\n",
    "\n",
    "    return concatenated_characters\n",
    "\n",
    "def concatenate_tensors_elementwise(tensor1, tensor2):\n",
    "    result = torch.cat((tensor1, tensor2), dim=-1)\n",
    "    return result\n",
    "\n",
    "def concatenate_tensors_feature3(tensor1, tensor2):\n",
    "    concatenated_tensor = torch.cat((tensor1, tensor2.unsqueeze(2)), dim=2)\n",
    "    return concatenated_tensor\n",
    "\n",
    "def pad_list(list, max_len, val):\n",
    "    for i in range(len(list)):\n",
    "        if len(list[i]) < max_len:\n",
    "            for j in range(max_len - len(list[i])):\n",
    "                list[i].append(val)\n",
    "\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset definition\n",
    "# A custom Dataset class must implement three functions: __init__, __len__, and __getitem__.\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    # load the dataset\n",
    "    # The __init__ function is run once when instantiating the Dataset object\n",
    "    def __init__(self, char_sequences, labels, word_sequences, diacritics_sequence, segment_sequences):\n",
    "        \n",
    "        self.x = torch.tensor(char_sequences)\n",
    "\n",
    "        self.y = torch.tensor(labels)\n",
    "\n",
    "        self.word = torch.tensor(word_sequences)\n",
    "\n",
    "        self.diacritics_sequence = torch.tensor(diacritics_sequence)\n",
    "\n",
    "        self.segment_sequences = torch.tensor(segment_sequences)\n",
    "\n",
    "    # number of rows in the dataset\n",
    "    # The __len__ function returns the number of samples in our dataset.\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx], self.word[idx], self.diacritics_sequence[idx], self.segment_sequences[idx]\n",
    "\n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.x))\n",
    "        train_size = len(self.x) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])\n",
    "\n",
    "\n",
    "# # prepare the dataset\n",
    "# def prepare_data():\n",
    "#     # load the dataset\n",
    "#     dataset = CSVDataset()\n",
    "#     # calculate split\n",
    "#     train, test = dataset.get_splits()\n",
    "#     # prepare data loaders\n",
    "#     # The Dataset retrieves our dataset’s features and labels one sample at a time.\n",
    "#     # While training a model, we typically want to pass samples in “minibatches”,\n",
    "#     # reshuffle the data at every epoch to reduce model overfitting,\n",
    "#     train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
    "#     test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "#     return dataset.encoding_mapping, train_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset definition\n",
    "# A custom Dataset class must implement three functions: __init__, __len__, and __getitem__.\n",
    "class Dataset2(torch.utils.data.Dataset):\n",
    "    # load the dataset\n",
    "    # The __init__ function is run once when instantiating the Dataset object\n",
    "    def __init__(self, words, labels):\n",
    "        \n",
    "        self.x = torch.tensor(words)\n",
    "\n",
    "        self.y = torch.tensor(labels)\n",
    "\n",
    "\n",
    "    # number of rows in the dataset\n",
    "    # The __len__ function returns the number of samples in our dataset.\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.x))\n",
    "        train_size = len(self.x) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert labels to numpy array\n",
    "print(len(labels[3]))\n",
    "print(len(char_sequences[3]))\n",
    "train_ds = Dataset(char_sequences, labels, word_sequences, test_sentences_diacritics_sequences, train_segment_sequences)\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "dg = iter(train_dl)\n",
    "X1, Y1, z1, d1, s1 = next(dg)\n",
    "X2, Y2, z2, d2, s2 = next(dg)\n",
    "print(Y1.shape, X1.shape, z1.shape, d1.shape, s1.shape, Y2.shape, X2.shape, z2.shape, d2.shape, s2.shape)\n",
    "print(X1[0][:], \"\\n\", Y1[0][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Char_model(nn.Module):\n",
    "  def __init__(self, vocab_size=42, embedding_dim=50, hidden_size=50, n_classes=17):\n",
    "    \"\"\"\n",
    "    The constructor of our NER model\n",
    "    Inputs:\n",
    "    - vacab_size: the number of unique words\n",
    "    - embedding_dim: the embedding dimension\n",
    "    - n_classes: the number of final classes (tags)\n",
    "\n",
    "    embedding_dim here: 50 for char embedding + 50 for following char embedding + 1 for feature3 = 101\n",
    "    \"\"\"\n",
    "\n",
    "    super(Char_model, self).__init__()\n",
    "\n",
    "    input_len = 2*embedding_dim + 15 + 1\n",
    "    ####################### TODO: Create the layers of your model #######################################\n",
    "    # (1) Create the embedding layer\n",
    "    self.embedding_char = nn.Embedding(vocab_size, embedding_dim)\n",
    "    self.embedding_diacritics = nn.Embedding(14, 15)\n",
    "\n",
    "    # (2) Create an LSTM layer with hidden size = hidden_size and batch_first = True\n",
    "    self.lstm = nn.LSTM(input_len, hidden_size, batch_first=True)\n",
    "    # batch_first makes the input and output tensors to be of shape (batch_size, seq_length, hidden_size)\n",
    "\n",
    "    # (3) Create a linear layer\n",
    "    self.linear = nn.Linear(hidden_size, n_classes)\n",
    "\n",
    "    #####################################################################################################\n",
    "\n",
    "  def forward(self, sentences, diacritics_list, segments, h_0=None, c_0=None):\n",
    "    \"\"\"\n",
    "    This function does the forward pass of our model\n",
    "    Inputs:\n",
    "    - sentences: tensor of shape (batch_size, max_length)\n",
    "\n",
    "    Returns:\n",
    "    - final_output: tensor of shape (batch_size, max_length, n_classes)\n",
    "    \"\"\"\n",
    "\n",
    "    final_output = None\n",
    "    #############################################################\n",
    "    sentences_embedded = self.embedding_char(sentences) \n",
    "    diacritics_embedded = self.embedding_diacritics(diacritics_list)\n",
    "    \n",
    "    sentences_embedded = concatenate_characters(sentences_embedded) #feature 1: concatenate characters\n",
    "    sentences_embedded = concatenate_tensors_elementwise(sentences_embedded, diacritics_embedded) #feature 2: concatenate diacritics seen before\n",
    "    sentences_embedded = concatenate_tensors_feature3(sentences_embedded, segments) #feature 3: concatenate segment for each character\n",
    "\n",
    "    #check if h_0 and c_0 are provided or not\n",
    "    if h_0 is None or c_0 is None:\n",
    "      final_output, (h_0, c_0) = self.lstm(sentences_embedded)\n",
    "    else:\n",
    "      final_output, _ = self.lstm(sentences_embedded, (h_0, c_0)) \n",
    "      \n",
    "    final_output = self.linear(final_output)  \n",
    "\n",
    "\n",
    "    ############################################################\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word_model(nn.Module):\n",
    "  def __init__(self, vocab_size=2093761, embedding_dim=50, hidden_size=50, n_classes=17):\n",
    "    \"\"\"\n",
    "    The constructor of our NER model\n",
    "    Inputs:\n",
    "    - vacab_size: the number of unique words\n",
    "    - embedding_dim: the embedding dimension\n",
    "    - n_classes: the number of final classes (tags)\n",
    "    \"\"\"\n",
    "    super(Word_model, self).__init__()\n",
    "    ####################### TODO: Create the layers of your model #######################################\n",
    "    # (1) Create the embedding layer\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    # (2) Create an LSTM layer with hidden size = hidden_size and batch_first = True\n",
    "    self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
    "    # batch_first makes the input and output tensors to be of shape (batch_size, seq_length, hidden_size)\n",
    "\n",
    "    # (3) Create a linear layer\n",
    "    self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "    #####################################################################################################\n",
    "\n",
    "  def forward(self, sentences):\n",
    "    \"\"\"\n",
    "    This function does the forward pass of our model\n",
    "    Inputs:\n",
    "    - sentences: tensor of shape (batch_size, max_length)\n",
    "\n",
    "    Returns:\n",
    "    - final_output: tensor of shape (batch_size, max_length, n_classes)\n",
    "    \"\"\"\n",
    "\n",
    "    final_output = None\n",
    "    ######################### TODO: implement the forward pass ####################################\n",
    "    embedded = self.embedding(sentences) \n",
    "    rnn_output, h = self.rnn(embedded)  \n",
    "    final_output = self.linear(rnn_output) \n",
    "\n",
    "    ###############################################################################################\n",
    "    return final_output, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char_model(\n",
      "  (embedding_char): Embedding(42, 50)\n",
      "  (embedding_diacritics): Embedding(14, 15)\n",
      "  (lstm): LSTM(116, 50, batch_first=True)\n",
      "  (linear): Linear(in_features=50, out_features=17, bias=True)\n",
      ")\n",
      "Word_model(\n",
      "  (embedding): Embedding(2093761, 50)\n",
      "  (rnn): RNN(50, 50, batch_first=True)\n",
      "  (linear): Linear(in_features=50, out_features=2093761, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lstm_model = Char_model()\n",
    "word_model = Word_model()\n",
    "# lstm_model.load_state_dict(torch.load('lstm_model_weights.pth'))\n",
    "# word_model.load_state_dict(torch.load('word_model_weights.pth'))\n",
    "print(lstm_model)\n",
    "print(word_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(lstm_model, context_model, train_dataset, batch_size=128, epochs=30, learning_rate=0.012):\n",
    "  \"\"\"\n",
    "  This function implements the training logic\n",
    "  Inputs:\n",
    "  - model: the model ot be trained\n",
    "  - train_dataset: the training set of type NERDataset\n",
    "  - batch_size: integer represents the number of examples per step\n",
    "  - epochs: integer represents the total number of epochs (full training pass)\n",
    "  - learning_rate: the learning rate to be used by the optimizer\n",
    "  \"\"\"\n",
    "\n",
    "  # (1) create the dataloader of the training set (make the shuffle=True)\n",
    "  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  # (2) make the criterion cross entropy loss\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  # (3) create the optimizer (Adam)\n",
    "  optimizer = torch.optim.Adam(list(lstm_model.parameters()) + list(context_model.parameters()), lr=learning_rate)\n",
    "  # GPU configuration\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "  if use_cuda:\n",
    "    lstm_model = lstm_model.cuda()\n",
    "    context_model = context_model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "  # device=\"cpu\"\n",
    "  for epoch_num in range(epochs):\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "\n",
    "    for train_input, train_label, train_context, train_diacritic, train_segments in tqdm(train_dataloader):\n",
    "      \n",
    "\n",
    "      # (4) move the train input to the device\n",
    "      train_label = train_label.long().to(device)\n",
    "\n",
    "      # (5) move the train label to the device\n",
    "      train_input = train_input.long().to(device)\n",
    "\n",
    "      train_context = train_context.long().to(device)\n",
    "\n",
    "      train_diacritic = train_diacritic.long().to(device)\n",
    "\n",
    "      train_segments = train_segments.long().to(device)\n",
    "\n",
    "      # (6) do the forward pass\n",
    "      # context, h_0 = context_model(train_context)\n",
    "      # c_0 = torch.zeros(context.shape[0], 1, context.shape[2])\n",
    "      # h_0 = torch.transpose(h_0, 0, 1)\n",
    "      #h_0 = h_0.permute(1, 0, 2)\n",
    "      #print(h_0.shape)\n",
    "      # print(train_input.shape)\n",
    "      # print(train_diacritic.shape)\n",
    "      output = lstm_model(train_input, train_diacritic, train_segments)\n",
    "      \n",
    "      # (7) loss calculation (you need to think in this part how to calculate the loss correctly)\n",
    "      batch_loss = criterion(output.reshape(-1, 17), train_label.reshape(-1))\n",
    "  \n",
    "      # (8) append the batch loss to the total_loss_train\n",
    "      total_loss_train += batch_loss.item()\n",
    "      \n",
    "      # (9) calculate the batch accuracy (just add the number of correct predictions)\n",
    "      acc = (output.argmax(dim=2) == train_label).sum().item()\n",
    "      total_acc_train += acc\n",
    "\n",
    "      # (10) zero your gradients\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      # (11) do the backward pass\n",
    "      batch_loss.backward()\n",
    "\n",
    "      # (12) update the weights with your optimizer\n",
    "      optimizer.step()\n",
    "      \n",
    "    # epoch loss\n",
    "    epoch_loss = total_loss_train / len(train_dataset)\n",
    "\n",
    "    # (13) calculate the accuracy\n",
    "    epoch_acc = total_acc_train / (len(train_dataset) * len(train_dataset[0][0]))\n",
    "    # ba2sem 3la 3adad el kalemat fy kol el gomal \n",
    "    # kol gomla asln fyha 104 kelma, fa badrab dh fy 3adad el gomal bs\n",
    "\n",
    "    print(\n",
    "        f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n",
    "        | Train Accuracy: {epoch_acc}\\n')\n",
    "\n",
    "  ##############################################################################################################\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the word model \n",
    "def train_word_model(word_model, train_dataset,batch_size=128, epochs=30, learning_rate=0.012):\n",
    "  \"\"\"\n",
    "  This function implements the training logic\n",
    "  Inputs:\n",
    "  - model: the model ot be trained\n",
    "  - train_dataset: the training set of type NERDataset\n",
    "  - batch_size: integer represents the number of examples per step\n",
    "  - epochs: integer represents the total number of epochs (full training pass)\n",
    "  - learning_rate: the learning rate to be used by the optimizer\n",
    "  \"\"\"\n",
    "\n",
    "  # (1) create the dataloader of the training set (make the shuffle=True)\n",
    "  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "  \n",
    "  # (2) make the criterion cross entropy loss\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  # (3) create the optimizer (Adam)\n",
    "  optimizer = torch.optim.Adam(word_model.parameters(), lr=learning_rate)\n",
    "  # GPU configuration\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "  #clear the cache\n",
    "  if use_cuda:\n",
    "    word_model = word_model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "  # device=\"cpu\"\n",
    "  for epoch_num in range(epochs):\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "    # the word model only needs the word sequences and the labels\n",
    "    for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "      # (4) move the train input to the device\n",
    "      train_label = train_label.long().to(device)\n",
    "\n",
    "      # (5) move the train label to the device\n",
    "      train_input = train_input.long().to(device)\n",
    "\n",
    "      # (6) do the forward pass\n",
    "      output, h = word_model(train_input)\n",
    "      \n",
    "      # (7) loss calculation (you need to think in this part how to calculate the loss correctly)\n",
    "      batch_loss = criterion(output.reshape(-1, 2093761), train_label.reshape(-1))\n",
    "  \n",
    "      # (8) append the batch loss to the total_loss_train\n",
    "      total_loss_train += batch_loss.item()\n",
    "      \n",
    "      # (9) calculate the batch accuracy (just add the number of correct predictions)\n",
    "      acc = (output.argmax(dim=2) == train_label).sum().item()\n",
    "      total_acc_train += acc\n",
    "\n",
    "      # (10) zero your gradients\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      # (11) do the backward pass\n",
    "      batch_loss.backward()\n",
    "\n",
    "      # (12) update the weights with your optimizer\n",
    "      optimizer.step()\n",
    "\n",
    "    # epoch loss\n",
    "    epoch_loss = total_loss_train / len(train_dataset)\n",
    "\n",
    "    # (13) calculate the accuracy\n",
    "    epoch_acc = total_acc_train / (len(train_dataset) * len(train_dataset[0][0]))\n",
    "    # ba2sem 3la 3adad el kalemat fy kol el gomal\n",
    "    # kol gomla asln fyha 104 kelma, fa badrab dh fy 3adad el gomal bs\n",
    "\n",
    "    print(\n",
    "        f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n",
    "        | Train Accuracy: {epoch_acc}\\n')\n",
    "    \n",
    "  ##############################################################################################################\n",
    "    \n",
    "      \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1118/1118 [01:49<00:00, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss: 0.1521331397427538         | Train Accuracy: 0.03820284423811055\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1118/1118 [01:50<00:00, 10.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss: 0.11295279734015953         | Train Accuracy: 0.05541649769971893\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1118/1118 [01:50<00:00, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss: 0.0977635446283642         | Train Accuracy: 0.08802595332317201\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1118/1118 [01:49<00:00, 10.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss: 0.08852977041093787         | Train Accuracy: 0.1485184511906926\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1118/1118 [01:49<00:00, 10.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss: 0.08282096668705516         | Train Accuracy: 0.19059471704445347\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1118/1118 [01:49<00:00, 10.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss: 0.07954062211048937         | Train Accuracy: 0.20986394082194845\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1118/1118 [01:49<00:00, 10.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss: 0.07757790415744398         | Train Accuracy: 0.21779256918322543\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1118/1118 [01:49<00:00, 10.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss: 0.07625484499225145         | Train Accuracy: 0.21984813949911206\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1118/1118 [01:49<00:00, 10.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss: 0.07528097667253748         | Train Accuracy: 0.22201557758729182\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1118/1118 [01:49<00:00, 10.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss: 0.07448219254148467         | Train Accuracy: 0.2236376602855425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1118/1118 [01:49<00:00, 10.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 11 | Train Loss: 0.07394138493769344         | Train Accuracy: 0.22395928013088529\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1118/1118 [01:49<00:00, 10.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 12 | Train Loss: 0.07354775917084481         | Train Accuracy: 0.22531567686993972\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1118/1118 [01:49<00:00, 10.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 13 | Train Loss: 0.07323270958717792         | Train Accuracy: 0.22591696614601542\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1118/1118 [01:49<00:00, 10.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 14 | Train Loss: 0.07280980013244752         | Train Accuracy: 0.22563729671528254\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 307/1118 [00:30<01:19, 10.17it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[43mtrain_word_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.012\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[32], line 36\u001b[0m, in \u001b[0;36mtrain_word_model\u001b[1;34m(word_model, train_dataset, batch_size, epochs, learning_rate)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# the word model only needs the word sequences and the labels\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_input, train_label \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dataloader):\n\u001b[0;32m     34\u001b[0m \n\u001b[0;32m     35\u001b[0m   \u001b[38;5;66;03m# (4) move the train input to the device\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m   train_label \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_label\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m   \u001b[38;5;66;03m# (5) move the train label to the device\u001b[39;00m\n\u001b[0;32m     39\u001b[0m   train_input \u001b[38;5;241m=\u001b[39m train_input\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# train the model\n",
    "#the train dataset is the word sequences and the labels the gold labels are the\n",
    "with open('./Dataset/word_level/words.txt', 'r', encoding='utf-8') as file:\n",
    "    words = file.readlines()\n",
    "with open('./Dataset/word_level/labels.txt', 'r', encoding='utf-8') as file:\n",
    "    labels = file.readlines()\n",
    "\n",
    "# remove the \\n from the end of each word\n",
    "words = [word[:-1] for word in words]\n",
    "labels = [label[:-1] for label in labels]\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(words)\n",
    "\n",
    "# encode the words\n",
    "encoded_words = tokenizer.texts_to_sequences(words)\n",
    "#tokenize the labels\n",
    "label_tokenizer = Tokenizer()\n",
    "label_tokenizer.fit_on_texts(labels)\n",
    "encoded_labels = label_tokenizer.texts_to_sequences(labels)\n",
    "# pad sequences pad_sequences is used to ensure that all sequences in a list have the same length\n",
    "max_length = max([len(seq) for seq in encoded_words])\n",
    "encoded_words = pad_sequences(encoded_words, maxlen=max_length, padding='post')\n",
    "encoded_labels = pad_sequences(encoded_labels, maxlen=max_length, padding='post')\n",
    "# split the dataset into train and test\n",
    "train_dataset = Dataset2(encoded_words, encoded_labels)\n",
    "train_dataset, test_dataset = train_dataset.get_splits()\n",
    "\n",
    "#clear the memory of torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# train the model\n",
    "train_word_model(word_model, train_dataset, batch_size=64, epochs=30, learning_rate=0.012)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_dataset = Dataset(char_sequences, labels, word_sequences, test_sentences_diacritics_sequences, train_segment_sequences)\n",
    "train(lstm_model, word_model, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm_model.state_dict(), '30e3_lstm_model_weights.pth')\n",
    "torch.save(word_model.state_dict(), '30e3_word_model_weights.pth')\n",
    "tensor2 = tensor1 = torch.tensor([\n",
    "    [[1, 2, 3], [4, 5, 6], [66, 55, 77]], \n",
    "    [[7, 8, 9], [10, 11, 12], [13, 14, 15]]\n",
    "])\n",
    "\n",
    "x = torch.tensor([\n",
    "    [[1, 2, 3], [4, 5, 6], [66, 55, 77]], \n",
    "    [[7, 8, 9], [10, 11, 12], [13, 14, 15]]\n",
    "])\n",
    "\n",
    "# Get the shape of the input tensor\n",
    "batch_size, sequence_length, feature_size = x.shape\n",
    "\n",
    "# Create a tensor of zeros with the same shape as the last subsequence\n",
    "zeros_tensor = torch.zeros_like(x[:, 0:1, :])\n",
    "\n",
    "# Concatenate it to the original tensor along the second dimension\n",
    "padded_x = torch.cat((x, zeros_tensor), dim=1)\n",
    "\n",
    "# Now, padded_x will have zeros padded to the last subsequence\n",
    "print(padded_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 = torch.tensor([\n",
    "    [[1, 2, 3], [4, 5, 6], [66, 55, 77]], \n",
    "    [[7, 8, 9], [10, 11, 12], [13, 14, 15]]\n",
    "])\n",
    "\n",
    "# Concatenate tensor1 with itself along the last dimension\n",
    "result = torch.cat((tensor1, tensor1), dim=-1)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of strings\n",
    "list_of_strings = [\"hello\", \"world\", \"deep\", \"learning\"]\n",
    "\n",
    "# Convert strings to lists of character indices\n",
    "list_of_lists = [list(map(ord, s)) for s in list_of_strings]\n",
    "\n",
    "# Pad sequences to the same length\n",
    "padded_sequences = pad_sequence([torch.tensor(seq) for seq in list_of_lists], batch_first=True, padding_value=0)\n",
    "\n",
    "print(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have two tensors A and B\n",
    "tensor_A = torch.randn(128, 7183, 50)\n",
    "tensor_B = torch.randint(0, 2, (128, 7183), dtype=torch.float32)  # Example tensor, adjust as needed\n",
    "\n",
    "# Concatenate along the last dimension\n",
    "concatenated_tensor = torch.cat((tensor_A, tensor_B.unsqueeze(2)), dim=2)\n",
    "\n",
    "# Print the shape of the concatenated tensor\n",
    "print(concatenated_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataset, batch_size=256):\n",
    "  \"\"\"\n",
    "  This function takes a NER model and evaluates its performance (accuracy) on a test data\n",
    "  Inputs:\n",
    "  - model: a NER model\n",
    "  - test_dataset: dataset of type NERDataset\n",
    "  \"\"\"\n",
    "  ########################### TODO: Replace the Nones in the following code ##########################\n",
    "\n",
    "  # (1) create the test data loader\n",
    "  test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "  # GPU Configuration\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "  if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "  total_acc_test = 0\n",
    "  \n",
    "  # (2) disable gradients\n",
    "  with torch.no_grad():\n",
    "    # 3mlna disable 3lshan e7na bn-predict (aw evaluate y3ny) b2a dlw2ty, msh bn-train\n",
    "\n",
    "    for test_input, test_label, test_context, test_diacritics, test_segments in tqdm(test_dataloader):\n",
    "      # (3) move the test input to the device\n",
    "      test_label = test_label.to(device)\n",
    "\n",
    "      # (4) move the test label to the device\n",
    "      test_input = test_input.to(device)\n",
    "      # brdo the comments should be reversed \n",
    "      test_context = test_context.long().to(device)\n",
    "      test_diacritics = test_diacritics.long().to(device)\n",
    "      test_segments = test_segments.long().to(device)\n",
    "      # (5) do the forward pass\n",
    "      output = model(test_input, test_diacritics, test_segments)\n",
    "      print(test_input.shape)\n",
    "      print(test_label.shape)\n",
    "      print(output.shape)\n",
    "      # accuracy calculation (just add the correct predicted items to total_acc_test)\n",
    "      acc = (output.argmax(dim=2) == test_label).sum().item()\n",
    "      total_acc_test += acc\n",
    "    \n",
    "    # (6) calculate the over all accuracy\n",
    "    total_acc_test /= (len(test_dataset) * len(test_dataset[0][0]))\n",
    "  ##################################################################################################\n",
    "\n",
    "  \n",
    "  print(f'\\nTest Accuracy: {total_acc_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Dataset(val_char_sequences, val_labels, val_word_sequences, val_sentences_diacritics_sequences, val_segment_sequences)\n",
    "evaluate(lstm_model, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickles/test_segment_sequences.pickle', 'rb') as file:\n",
    "    test_segment_sequences = pickle.load(file)\n",
    "\n",
    "with open('./pickles/test_sentence_diacritics_appearance_sequences.pickle', 'rb') as file:\n",
    "    test_sentences_diacritics_sequences = pickle.load(file)\n",
    "\n",
    "with open('./pickles/test_char_sequences_without_tashkeel.pkl', 'rb') as file:\n",
    "    test_char_sequences = pickle.load(file)\n",
    "\n",
    "print(len(test_segment_sequences[0]))\n",
    "print(len(test_sentences_diacritics_sequences[0]))\n",
    "print(len(test_char_sequences[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "lstm_model.load_state_dict(torch.load('30e3_lstm_model_weights.pth'))\n",
    "\n",
    "\n",
    "output = lstm_model(torch.tensor(test_char_sequences), torch.tensor(test_sentences_diacritics_sequences), torch.tensor(test_segment_sequences))\n",
    "\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting max Probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.7436, -4.6608,  2.0320, 12.3649, 11.4669,  6.2128,  5.5692, -1.1379,\n",
      "        -0.8895, -1.8278, -4.4135, -2.7686, -8.9944, -3.0970, -9.0805, -8.3650,\n",
      "        -7.4706], grad_fn=<SelectBackward0>)\n",
      "tensor([1.9456e-07, 2.8602e-08, 2.3070e-05, 7.0889e-01, 2.8879e-01, 1.5093e-03,\n",
      "        7.9291e-04, 9.6914e-07, 1.2424e-06, 4.8612e-07, 3.6629e-08, 1.8975e-07,\n",
      "        3.7527e-10, 1.3663e-07, 3.4432e-10, 7.0423e-10, 1.7224e-09],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([3])\n"
     ]
    }
   ],
   "source": [
    "outputs = pickle.load(open(\"./pickles/test_output.pkl\", \"rb\"))\n",
    "\n",
    "print(outputs[0][0])\n",
    "\n",
    "softmax_output = nn.functional.softmax(outputs, dim=-1)\n",
    "print(softmax_output[0][0])\n",
    "\n",
    "# Find the index of the maximum value along the last axis\n",
    "max_arg = torch.argmax(softmax_output, dim=-1)\n",
    "\n",
    "# Add a new dimension at the end to make the shape (2000, 1904, 1)\n",
    "new_tensor = torch.unsqueeze(max_arg, dim=-1)\n",
    "\n",
    "print(new_tensor[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import pyarabic.araby as araby\n",
    "from pyarabic.araby import strip_tashkeel\n",
    "import qalsadi.lemmatizer \n",
    "import qalsadi.analex as qa\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "from farasa.pos import FarasaPOSTagger \n",
    "from farasa.ner import FarasaNamedEntityRecognizer \n",
    "from farasa.diacratizer import FarasaDiacritizer \n",
    "from farasa.segmenter import FarasaSegmenter \n",
    "from farasa.stemmer import FarasaStemmer\n",
    "\n",
    "import keras\n",
    "from  diacritization_evaluation import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['قوله', 'أو', 'قطع', 'الأول', 'يده', 'إلخ', 'قال', 'الزركشي', 'ابن', 'عرفة']\n",
      "قوله أو قطع الأول يده إلخ قال الزركشي\n",
      "قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "sentences = []\n",
    "sentences_with_tashkeel = []\n",
    "with open('./Dataset/training/train_words_stripped.txt', 'r', encoding='utf-8') as output_file:\n",
    "    for word in output_file:\n",
    "        words.append(word.strip())\n",
    "\n",
    "with open('./Dataset/training/train_stripped.txt', 'r', encoding='utf-8') as output_file:\n",
    "    for sentence in output_file:\n",
    "        sentences.append(sentence.strip())\n",
    "\n",
    "with open('./Dataset/training/train_cleaned.txt', 'r', encoding='utf-8') as output_file:\n",
    "    for sentence in output_file:\n",
    "        sentences_with_tashkeel.append(sentence.strip())\n",
    "\n",
    "dev_words = []\n",
    "dev_sentences = []\n",
    "dev_sentences_with_tashkeel = []\n",
    "\n",
    "with open('./Dataset/val/val_words_stripped.txt', 'r', encoding='utf-8') as output_file:\n",
    "    for word in output_file:\n",
    "        dev_words.append(word.strip())\n",
    "\n",
    "with open('./Dataset/val/val_stripped.txt', 'r', encoding='utf-8') as output_file:\n",
    "    for sentence in output_file:\n",
    "        dev_sentences.append(sentence.strip())\n",
    "\n",
    "with open('./Dataset/val/val_cleaned.txt', 'r', encoding='utf-8') as output_file:\n",
    "    for sentence in output_file:\n",
    "        dev_sentences_with_tashkeel.append(sentence.strip())\n",
    "\n",
    "print(words[0:10])\n",
    "print(sentences[0])\n",
    "print(sentences_with_tashkeel[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ\n"
     ]
    }
   ],
   "source": [
    "print(sentences_with_tashkeel[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a word-based tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenizer\n",
    "words_tokenizer = Tokenizer()\n",
    "\n",
    "# Fit the tokenizer on the list of words (treat each word as a separate \"sentence\")\n",
    "words_tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Get the word index\n",
    "word_index = words_tokenizer.word_index\n",
    "\n",
    "# Tokenize the words\n",
    "word_sequences = words_tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "dev_word_sequences = words_tokenizer.texts_to_sequences(dev_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the tokenizer on a sample sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['قوله أو قطع الأول يده إلخ قال الزركشي', 'ابن عرفة قوله بلفظ يقتضيه كإنكار غير حديث بالإسلام وجوب ما علم وجوبه من الدين ضرورة كإلقاء مصحف بقذر وشد زنار ابن عرفة قول ابن شاس أو بفعل يتضمنه هو كلبس الزنار وإلقاء المصحف في صريح النجاسة والسجود للصنم ونحو ذلك وسحر محمد قول مالك وأصحابه أن الساحر كافر بالله تعالى قال مالك هو كالزنديق إذا عمل السحر بنفسه قتل ولم يستتب .']\n",
      "7\n",
      "[[7, 3, 276, 90, 190, 46, 13, 907]]\n"
     ]
    }
   ],
   "source": [
    "# Create a sentence tokenizer\n",
    "print((sentences[0:2])) #This way works and the one used below as well\n",
    "print(word_index[\"قوله\"])\n",
    "print(words_tokenizer.texts_to_sequences([sentences[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a character-based with tashkeel tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20, 2, 11, 5, 6, 8, 13, 8, 3, 18, 2, 11, 5, 3, 20, 2, 40, 2, 16, 2, 3, 7, 6, 5, 18, 2, 11, 17, 6, 8, 3, 10, 2, 22, 2, 13, 8, 3, 28, 6, 2, 33, 5, 3, 20, 2, 7, 6, 2, 3, 7, 6, 41, 17, 15, 5, 23, 2, 36, 4, 10, 42], [7, 14, 5, 12, 8, 3, 16, 2, 15, 2, 19, 2, 26, 2, 3, 20, 2, 11, 5, 6, 8, 13, 8, 3, 14, 4, 6, 2, 19, 5, 47, 31, 3, 10, 2, 20, 5, 21, 2, 37, 4, 10, 13, 3, 23, 2, 28, 4, 12, 5, 23, 2, 7, 15, 4, 3, 43, 2, 10, 5, 15, 4, 3, 24, 2, 22, 4, 10, 34, 31, 3, 14, 4, 7, 6, 5, 28, 4, 25, 5, 6, 2, 7, 9, 4, 3, 11, 8, 27, 8, 11, 14, 2, 3, 9, 2, 7, 3, 16, 8, 6, 4, 9, 2, 3, 11, 8, 27, 8, 11, 14, 8, 13, 8, 3, 9, 4, 12, 5, 3, 7, 6, 22, 35, 10, 12, 4, 3, 37, 2, 15, 8, 11, 15, 2, 26, 38, 3, 23, 2, 28, 4, 6, 5, 20, 2, 7, 45, 4, 3, 9, 8, 30, 5, 24, 2, 19, 31, 3, 14, 4, 20, 2, 29, 2, 15, 31, 3, 11, 2, 36, 2, 22, 35, 3, 41, 8, 12, 17, 7, 15, 31, 3, 7, 14, 5, 12, 8, 3, 16, 2, 15, 2, 19, 2, 26, 2, 3, 20, 2, 11, 5, 6, 8, 3, 7, 14, 5, 12, 4, 3, 36, 2, 7, 25, 31, 3, 18, 2, 11, 5, 3, 14, 4, 19, 4, 16, 5, 6, 31, 3, 10, 2, 21, 2, 37, 2, 9, 17, 12, 8, 13, 8, 3, 13, 8, 11, 2, 3, 23, 2, 6, 8, 14, 5, 25, 4, 3, 7, 6, 41, 42, 12, 17, 7, 15, 4, 3, 11, 2, 28, 4, 6, 5, 20, 2, 7, 45, 4, 3, 7, 6, 5, 9, 8, 30, 5, 24, 2, 19, 4, 3, 19, 4, 10, 3, 30, 2, 15, 4, 10, 24, 4, 3, 7, 6, 12, 17, 27, 2, 7, 25, 2, 26, 4, 3, 11, 2, 7, 6, 25, 42, 27, 8, 11, 22, 4, 3, 6, 4, 6, 30, 17, 12, 2, 9, 4, 3, 11, 2, 12, 2, 24, 5, 11, 4, 3, 29, 2, 6, 4, 23, 2, 3, 11, 2, 25, 4, 24, 5, 15, 31, 3, 9, 8, 24, 2, 9, 17, 22, 39, 3, 20, 2, 11, 5, 6, 8, 3, 9, 2, 7, 6, 4, 23, 31, 3, 11, 2, 18, 2, 30, 5, 24, 2, 7, 14, 4, 13, 4, 3, 18, 2, 12, 17, 3, 7, 6, 25, 17, 7, 24, 4, 15, 2, 3, 23, 2, 7, 19, 4, 15, 39, 3, 14, 4, 7, 2, 6, 6, 17, 13, 4, 3, 21, 2, 16, 2, 7, 6, 2, 32, 3, 20, 2, 7, 6, 2, 3, 9, 2, 7, 6, 4, 23, 39, 3, 13, 8, 11, 2, 3, 23, 2, 7, 6, 41, 35, 12, 5, 22, 4, 10, 20, 4, 3, 28, 29, 2, 7, 3, 16, 2, 9, 4, 6, 2, 3, 7, 6, 25, 35, 24, 5, 15, 2, 3, 14, 4, 12, 2, 19, 5, 25, 4, 13, 4, 3, 20, 8, 21, 4, 6, 2, 3, 11, 2, 6, 2, 9, 5, 3, 10, 8, 25, 5, 21, 2, 21, 2, 14, 5, 3, 44], [20, 2, 11, 5, 6, 8, 13, 8, 3, 6, 4, 16, 2, 22, 2, 9, 4, 3, 9, 2, 7, 3, 21, 2, 21, 2, 16, 2, 6, 17, 20, 8, 3, 28, 6, 2, 33, 5, 3, 18, 2, 10, 5, 3, 7, 6, 5, 11, 2, 30, 4, 10, 17, 26, 8, 3, 20, 2, 11, 5, 6, 8, 13, 8, 3, 9, 2, 7, 3, 9, 2, 15, 17, 3, 18, 2, 10, 5, 3, 20, 8, 14, 2, 10, 5, 6, 2, 3, 20, 2, 11, 5, 6, 4, 3, 7, 6, 5, 9, 2, 21, 5, 12, 4, 3, 6, 2, 43, 2, 21, 5, 3, 11, 2, 6, 2, 11, 5, 3, 7, 20, 5, 21, 2, 30, 2, 15, 2, 3, 16, 2, 6, 2, 32, 3, 18, 2, 11, 5, 30, 2, 10, 5, 21, 3, 6, 2, 13, 8, 3, 14, 4, 36, 2, 7, 26, 31, 3, 18, 2, 11, 5, 3, 18, 2, 16, 5, 40, 8, 11, 13, 8, 3, 36, 2, 7, 26, 38, 3, 11, 2, 6, 2, 7, 3, 43, 2, 12, 2, 9, 2, 3, 6, 2, 13, 8, 3, 16, 4, 12, 5, 22, 2, 3, 7, 6, 5, 9, 2, 11, 5, 21, 4, 3, 13, 2, 6, 5, 3, 21, 2, 14, 5, 40, 8, 6, 8, 3, 7, 6, 5, 11, 2, 30, 4, 10, 17, 26, 8, 3, 18, 2, 11, 5, 3, 10, 8, 36, 5, 21, 2, 15, 2, 32, 3, 6, 2, 13, 8, 3, 36, 2, 7, 26, 39, 3, 11, 2, 10, 8, 49, 5, 33, 2, 29, 8, 3, 9, 4, 12, 5, 3, 20, 2, 11, 5, 6, 4, 13, 4, 3, 7, 6, 5, 48, 21, 4, 10, 3, 23, 2, 9, 2, 7, 3, 6, 2, 11, 5, 3, 6, 2, 9, 5, 3, 10, 2, 20, 8, 6, 5, 3, 9, 4, 12, 5, 3, 9, 2, 7, 6, 4, 10, 3, 11, 2, 6, 2, 7, 3, 9, 4, 12, 5, 3, 43, 2, 12, 2, 9, 4, 10, 3, 18, 2, 12, 17, 13, 2, 7, 3, 6, 2, 7, 3, 21, 2, 14, 5, 40, 8, 6, 8, 3, 11, 2, 16, 4, 14, 2, 7, 15, 2, 26, 8, 3, 7, 6, 5, 23, 2, 12, 5, 41, 4, 3, 11, 2, 6, 2, 11, 5, 3, 6, 2, 9, 5, 3, 10, 2, 20, 8, 6, 5, 3, 9, 4, 12, 5, 3, 9, 2, 7, 6, 4, 10, 3, 11, 2, 6, 2, 7, 3, 9, 4, 12, 5, 3, 43, 2, 12, 2, 9, 4, 10, 3, 6, 2, 9, 5, 3, 10, 2, 21, 2, 16, 2, 10, 17, 12, 5, 3, 43, 2, 12, 2, 9, 8, 13, 8, 3, 28, 12, 5, 3, 23, 2, 7, 12, 2, 21, 5, 3, 7, 12, 5, 21, 2, 13, 2, 21, 5, 3, 7, 3, 13, 3, 25, 9, 3, 20, 2, 11, 5, 6, 8, 13, 8, 3, 19, 2, 10, 8, 16, 5, 40, 2, 32, 3, 11, 2, 7, 24, 4, 22, 2, 26, 38, 3, 9, 4, 12, 5, 13, 2, 7, 3, 28, 6, 2, 33, 5, 3, 23, 2, 9, 2, 7, 3, 6, 2, 11, 5, 3, 23, 2, 7, 12, 2, 21, 5, 3, 9, 2, 11, 5, 27, 8, 11, 22, 2, 26, 38, 3, 16, 4, 12, 5, 22, 2, 3, 7, 6, 5, 11, 2, 30, 4, 10, 17, 26, 4, 3, 11, 2, 7, 6, 5, 9, 2, 11, 5, 21, 4, 3, 11, 2, 6, 2, 7, 3, 10, 2, 27, 8, 11, 41, 8, 3, 18, 2, 12, 5, 3, 10, 8, 16, 5, 40, 2, 32, 3, 11, 2, 7, 24, 4, 22, 2, 26, 38, 3, 9, 4, 12, 5, 3, 43, 2, 10, 5, 15, 4, 3, 43, 2, 12, 2, 9, 4, 13, 4, 3, 19, 4, 10, 3, 7, 6, 30, 42, 11, 15, 2, 21, 2, 10, 5, 12, 4, 3, 11, 2, 28, 4, 12, 5, 3, 21, 2, 15, 2, 7, 37, 2, 10, 2, 7, 3, 6, 4, 18, 2, 12, 17, 13, 8, 3, 30, 8, 6, 5, 24, 39, 3, 16, 2, 6, 2, 32, 3, 9, 2, 27, 5, 13, 8, 11, 6, 31, 3, 9, 8, 43, 5, 12, 4, 10, 3, 11, 2, 12, 4, 13, 2, 7, 10, 2, 26, 39, 3, 20, 2, 7, 6, 2, 3, 16, 3, 36, 3, 20, 2, 11, 5, 6, 8, 13, 8, 3, 11, 2, 7, 24, 4, 22, 2, 26, 38, 3, 9, 4, 12, 5, 13, 2, 7, 3, 18, 2, 10, 5, 3, 23, 2, 7, 9, 4, 6, 2, 26, 38, 3, 11, 2, 6, 2, 7, 3, 10, 2, 27, 8, 11, 41, 8, 3, 18, 2, 12, 5, 3, 10, 8, 16, 5, 40, 2, 32, 3, 12, 4, 30, 5, 19, 2, 10, 5, 12, 4, 3, 9, 4, 12, 5, 3, 36, 2, 7, 21, 2, 10, 5, 12, 4, 3, 6, 4, 18, 2, 12, 17, 13, 8, 3, 6, 2, 7, 3, 10, 8, 25, 2, 9, 17, 32, 3, 36, 2, 7, 26, 38, 3, 11, 2, 20, 2, 11, 5, 6, 8, 13, 8, 3, 11, 2, 6, 2, 7, 3, 10, 2, 27, 8, 11, 41, 8, 3, 18, 2, 12, 5, 3, 10, 8, 16, 5, 40, 2, 32, 3, 11, 2, 7, 24, 4, 22, 2, 26, 38, 3, 9, 4, 12, 5, 3, 43, 2, 10, 5, 15, 4, 3, 43, 2, 12, 2, 9, 4, 13, 4, 3, 11, 2, 10, 2, 12, 5, 14, 2, 43, 4, 10, 3, 18, 2, 12, 5, 3, 10, 8, 20, 2, 7, 6, 2, 3, 9, 4, 34, 5, 6, 8, 3, 29, 2, 6, 4, 23, 2, 3, 19, 4, 10, 3, 7, 6, 5, 18, 2, 15, 4, 20, 17, 7, 45, 4, 3, 7, 3, 13, 3, 44], [11, 2, 24, 2, 10, 2, 11, 2, 7, 12, 39, 3, 43, 2, 10, 5, 15, 8, 3, 9, 2, 11, 5, 27, 8, 11, 22, 31, 3, 44], [19, 2, 7, 46, 4, 22, 2, 26, 39, 3, 20, 2, 7, 6, 2, 3, 14, 2, 16, 5, 37, 8, 13, 8, 9, 5, 3, 10, 8, 49, 5, 33, 2, 29, 8, 3, 9, 4, 12, 5, 3, 36, 2, 15, 5, 40, 4, 3, 21, 2, 9, 2, 7, 9, 4, 3, 7, 6, 5, 9, 4, 6, 5, 23, 4, 3, 16, 2, 22, 2, 9, 8, 3, 41, 2, 23, 2, 7, 26, 4, 3, 24, 8, 6, 4, 10, 35, 3, 7, 6, 5, 23, 2, 16, 5, 14, 2, 26, 4, 3, 11, 2, 7, 6, 5, 9, 2, 25, 2, 7, 27, 4, 22, 4, 3, 9, 4, 12, 5, 3, 20, 2, 12, 2, 7, 22, 4, 10, 6, 2, 3, 11, 2, 16, 2, 6, 2, 7, 46, 4, 20, 2, 3, 11, 2, 30, 2, 19, 2, 7, 46, 4, 24, 4, 3, 18, 2, 14, 5, 11, 2, 7, 14, 31, 3, 44]]\n",
      "{'UNK': 1, 'َ': 2, ' ': 3, 'ِ': 4, 'ْ': 5, 'ل': 6, 'ا': 7, 'ُ': 8, 'م': 9, 'ي': 10, 'و': 11, 'ن': 12, 'ه': 13, 'ب': 14, 'ر': 15, 'ع': 16, '٤': 17, 'أ': 18, 'ف': 19, 'ق': 20, 'ت': 21, 'د': 22, 'ك': 23, 'ح': 24, 'س': 25, 'ة': 26, 'ج': 27, 'إ': 28, 'ذ': 29, 'ص': 30, 'ٍ': 31, 'ى': 32, 'خ': 33, 'ث': 34, '٦': 35, 'ش': 36, 'ض': 37, 'ً': 38, 'ٌ': 39, 'ط': 40, 'ز': 41, '٥': 42, 'غ': 43, '.': 44, 'ء': 45, 'ئ': 46, 'ظ': 47, 'آ': 48, 'ؤ': 49, 'ّ': 50, '٣': 51, '٢': 52, '١': 53}\n"
     ]
    }
   ],
   "source": [
    "sentences_new = []\n",
    "with open('./Dataset/training/train_replace.txt', 'r', encoding='utf-8') as output_file:\n",
    "    for sentence in output_file:\n",
    "        sentences_new.append(sentence.strip())\n",
    "\n",
    "dev_sentences_replaced = []\n",
    "with open('./Dataset/val/val_replaced.txt', 'r', encoding='utf-8') as output_file:\n",
    "    for sentence in output_file:\n",
    "        dev_sentences_replaced.append(sentence.strip())\n",
    "\n",
    "char_tokenizer_with_tashkeel = Tokenizer(char_level=True, oov_token='UNK')\n",
    "char_tokenizer_with_tashkeel.fit_on_texts(sentences_new)\n",
    "char_index_with_tashkeel = char_tokenizer_with_tashkeel.word_index\n",
    "char_sequences_with_tashkeel = char_tokenizer_with_tashkeel.texts_to_sequences(sentences_new)\n",
    "dev_char_sequences_with_tashkeel = char_tokenizer_with_tashkeel.texts_to_sequences(dev_sentences_replaced)\n",
    "print(char_sequences_with_tashkeel[0:5])\n",
    "print(char_index_with_tashkeel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a character-based without tashkeel tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_tokenizer_without_tashkeel = Tokenizer(char_level=True)\n",
    "char_tokenizer_without_tashkeel.fit_on_texts(sentences)\n",
    "char_index_without_tashkeel = char_tokenizer_without_tashkeel.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_sequences_without_tashkeel = char_tokenizer_without_tashkeel.texts_to_sequences(sentences)\n",
    "dev_char_sequences_without_tashkeel = char_tokenizer_without_tashkeel.texts_to_sequences(dev_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14, 6, 2, 8, 1, 12, 6, 1, 14, 30, 11, 1, 3, 2, 12, 6, 2, 1, 5, 16, 8, 1, 22, 2, 26, 1, 14, 3, 2, 1, 3, 2, 31, 10, 17, 28, 5]]\n",
      "38\n",
      "{' ': 1, 'ل': 2, 'ا': 3, 'م': 4, 'ي': 5, 'و': 6, 'ن': 7, 'ه': 8, 'ب': 9, 'ر': 10, 'ع': 11, 'أ': 12, 'ف': 13, 'ق': 14, 'ت': 15, 'د': 16, 'ك': 17, 'ح': 18, 'س': 19, 'ة': 20, 'ج': 21, 'إ': 22, 'ذ': 23, 'ص': 24, 'ى': 25, 'خ': 26, 'ث': 27, 'ش': 28, 'ض': 29, 'ط': 30, 'ز': 31, 'غ': 32, '.': 33, 'ء': 34, 'ئ': 35, 'ظ': 36, 'آ': 37, 'ؤ': 38}\n",
      "{'UNK': 1, 'َ': 2, ' ': 3, 'ِ': 4, 'ْ': 5, 'ل': 6, 'ا': 7, 'ُ': 8, 'م': 9, 'ي': 10, 'و': 11, 'ن': 12, 'ه': 13, 'ب': 14, 'ر': 15, 'ع': 16, '٤': 17, 'أ': 18, 'ف': 19, 'ق': 20, 'ت': 21, 'د': 22, 'ك': 23, 'ح': 24, 'س': 25, 'ة': 26, 'ج': 27, 'إ': 28, 'ذ': 29, 'ص': 30, 'ٍ': 31, 'ى': 32, 'خ': 33, 'ث': 34, '٦': 35, 'ش': 36, 'ض': 37, 'ً': 38, 'ٌ': 39, 'ط': 40, 'ز': 41, '٥': 42, 'غ': 43, '.': 44, 'ء': 45, 'ئ': 46, 'ظ': 47, 'آ': 48, 'ؤ': 49, 'ّ': 50, '٣': 51, '٢': 52, '١': 53}\n"
     ]
    }
   ],
   "source": [
    "print(char_sequences_without_tashkeel[0:1])\n",
    "print(len(char_index_without_tashkeel.keys()))\n",
    "print(char_index_without_tashkeel)\n",
    "print(char_index_with_tashkeel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding tests for the character-based tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UNK': 1, 'َ': 2, ' ': 3, 'ِ': 4, 'ْ': 5, 'ل': 6, 'ا': 7, 'ُ': 8, 'م': 9, 'ي': 10, 'و': 11, 'ن': 12, 'ه': 13, 'ب': 14, 'ر': 15, 'ع': 16, '٤': 17, 'أ': 18, 'ف': 19, 'ق': 20, 'ت': 21, 'د': 22, 'ك': 23, 'ح': 24, 'س': 25, 'ة': 26, 'ج': 27, 'إ': 28, 'ذ': 29, 'ص': 30, 'ٍ': 31, 'ى': 32, 'خ': 33, 'ث': 34, '٦': 35, 'ش': 36, 'ض': 37, 'ً': 38, 'ٌ': 39, 'ط': 40, 'ز': 41, '٥': 42, 'غ': 43, '.': 44, 'ء': 45, 'ئ': 46, 'ظ': 47, 'آ': 48, 'ؤ': 49, 'ّ': 50, '٣': 51, '٢': 52, '١': 53}\n",
      "[[11, 2, 20, 2, 7, 6, 2], [36, 2, 10, 5, 33, 8], [7, 6, 5, 28, 4, 25, 5, 6, 2, 7, 9, 4], [18, 2, 10, 5, 37, 38, 7], [11, 2, 20, 2, 22, 5], [25, 8, 46, 4, 6, 5, 21], [16, 2, 12, 5], [12, 2, 47, 2, 15, 4], [13, 2, 29, 4, 13, 4], [7, 6, 5, 9, 2, 25, 5, 18, 2, 6, 2, 26, 4], [11, 2, 13, 8, 11, 2], [15, 2, 27, 8, 6, 39], [21, 2, 16, 2, 15, 17, 37, 2], [6, 4, 7, 9, 5, 15, 2, 18, 2, 26, 4], [43, 2, 10, 5, 15, 4, 13, 4], [19, 2, 41, 2, 12, 2, 32], [14, 4, 13, 2, 7], [34, 8, 9, 17], [21, 2, 7, 14, 2], [9, 4, 12, 5], [29, 2, 6, 4, 23, 2], [11, 2, 25, 2, 18, 2, 6, 2, 13, 8], [41, 2, 11, 5, 27, 8, 13, 2, 7], [16, 2, 12, 5], [29, 2, 6, 4, 23, 2], [19, 2, 18, 2, 12, 5, 23, 2, 15, 2], [19, 2, 40, 2, 6, 2, 14, 2], [7, 25, 5, 21, 4, 24, 5, 6, 2, 7, 19, 2, 13, 8], [19, 2, 28, 4, 12, 5], [24, 2, 6, 2, 19, 2], [16, 2, 6, 2, 32], [12, 2, 19, 5, 10, 4]]\n"
     ]
    }
   ],
   "source": [
    "print(char_index_with_tashkeel)\n",
    "print(char_tokenizer_with_tashkeel.texts_to_sequences(sentences_new[12].split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: مرحبا كيف حالك\n",
      "Tokenized Sequence: [9, 15, 24, 14, 7, 3, 23, 10, 19, 3, 24, 7, 6, 23]\n",
      "\n",
      "Original Text: السلام عليكم\n",
      "Tokenized Sequence: [7, 6, 25, 6, 7, 9, 3, 16, 6, 10, 23, 9]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# New text data for testing\n",
    "new_texts = [\"مرحبا كيف حالك\", \"السلام عليكم\"]\n",
    "\n",
    "# Tokenize the new text data at the character level\n",
    "sequences_new = char_tokenizer_with_tashkeel.texts_to_sequences(new_texts)\n",
    "\n",
    "# Print the results\n",
    "for text, sequence in zip(new_texts, sequences_new):\n",
    "    print(f\"Original Text: {text}\")\n",
    "    print(f\"Tokenized Sequence: {sequence}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addding Padding to the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming word_sequences and char_sequences are the output of the tokenizers\n",
    "word_sequences = words_tokenizer.texts_to_sequences(sentences)\n",
    "char_sequences = char_tokenizer_with_tashkeel.texts_to_sequences(sentences_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add padding\n",
    "word_sequences_padded = pad_sequences(word_sequences, padding='post')\n",
    "char_sequences_with_tashkeel_padded = pad_sequences(char_sequences, padding='post')\n",
    "char_sequences_without_tashkeel_padded = pad_sequences(char_sequences_without_tashkeel, padding='post')\n",
    "\n",
    "dev_word_sequences_padded = pad_sequences(dev_word_sequences, padding='post')\n",
    "dev_char_sequences_with_tashkeel_padded = pad_sequences(dev_char_sequences_with_tashkeel, padding='post')\n",
    "dev_char_sequences_without_tashkeel_padded = pad_sequences(dev_char_sequences_without_tashkeel, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n",
      "11740\n"
     ]
    }
   ],
   "source": [
    "print(len(char_sequences[5]))\n",
    "print(len(char_sequences_with_tashkeel_padded[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenized sequences\n",
    "with open('./pickles/word_sequences.pkl', 'wb') as file:\n",
    "    pickle.dump(word_sequences_padded, file)\n",
    "\n",
    "with open('./pickles/char_sequences_with_tashkeel.pkl', 'wb') as file:\n",
    "    pickle.dump(char_sequences_with_tashkeel_padded, file)\n",
    "\n",
    "with open('./pickles/char_sequences_without_tashkeel.pkl', 'wb') as file:\n",
    "    pickle.dump(char_sequences_without_tashkeel_padded, file)\n",
    "\n",
    "with open('./pickles/val_word_sequences.pkl', 'wb') as file:\n",
    "    pickle.dump(dev_word_sequences_padded, file)\n",
    "\n",
    "with open('./pickles/val_char_sequences_with_tashkeel.pkl', 'wb') as file:\n",
    "    pickle.dump(dev_char_sequences_with_tashkeel_padded, file)\n",
    "\n",
    "with open('./pickles/val_char_sequences_without_tashkeel.pkl', 'wb') as file:\n",
    "    pickle.dump(dev_char_sequences_without_tashkeel_padded, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences_with_tashkeel\n",
    "tashkeel_list = []\n",
    "dev_tashkeel_list = []\n",
    "for sentence in sentences_with_tashkeel:\n",
    "    text, txt_list, harakat_list = util.extract_haraqat(sentence)   \n",
    "    for i in range(len(harakat_list)):\n",
    "        if len(harakat_list[i]) == 2:\n",
    "            if '\\u0651\\u064B' in harakat_list[i]:\n",
    "                harakat_list[i] = '١'\n",
    "            if '\\u0651\\u064C' in harakat_list[i]:\n",
    "                harakat_list[i] = '٢'\n",
    "            if '\\u0651\\u064D' in harakat_list[i]:\n",
    "                harakat_list[i] = '٣'\n",
    "            if '\\u0651\\u064E' in harakat_list[i]:\n",
    "                harakat_list[i] = '٤'\n",
    "            if '\\u0651\\u064F' in harakat_list[i]:\n",
    "                harakat_list[i] = '٥'\n",
    "            if '\\u0651\\u0650' in harakat_list[i]:\n",
    "                harakat_list[i] = '٦'\n",
    "\n",
    "    tashkeel_list.append(harakat_list)\n",
    "\n",
    "for sentence in dev_sentences_with_tashkeel:\n",
    "    text, txt_list, harakat_list = util.extract_haraqat(sentence)   \n",
    "    for i in range(len(harakat_list)):\n",
    "        if len(harakat_list[i]) == 2:\n",
    "            if '\\u0651\\u064B' in harakat_list[i]:\n",
    "                harakat_list[i] = '١'\n",
    "            if '\\u0651\\u064C' in harakat_list[i]:\n",
    "                harakat_list[i] = '٢'\n",
    "            if '\\u0651\\u064D' in harakat_list[i]:\n",
    "                harakat_list[i] = '٣'\n",
    "            if '\\u0651\\u064E' in harakat_list[i]:\n",
    "                harakat_list[i] = '٤'\n",
    "            if '\\u0651\\u064F' in harakat_list[i]:\n",
    "                harakat_list[i] = '٥'\n",
    "            if '\\u0651\\u0650' in harakat_list[i]:\n",
    "                harakat_list[i] = '٦'\n",
    "\n",
    "    dev_tashkeel_list.append(harakat_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tashkeel_sequences = char_tokenizer_with_tashkeel.texts_to_sequences(tashkeel_list)\n",
    "tashkeel_sequences_padded = pad_sequences(tashkeel_sequences, padding='post')\n",
    "\n",
    "with open('tashkeel_sequences.pkl', 'wb') as file:\n",
    "    pickle.dump(tashkeel_sequences_padded, file)\n",
    "\n",
    "dev_tashkeel_sequences = char_tokenizer_with_tashkeel.texts_to_sequences(dev_tashkeel_list)\n",
    "dev_tashkeel_sequences_padded = pad_sequences(dev_tashkeel_sequences, padding='post')\n",
    "\n",
    "with open('val_tashkeel_sequences.pkl', 'wb') as file:\n",
    "    pickle.dump(dev_tashkeel_sequences_padded, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['َ', 'ْ', 'ُ', 'ُ', '', 'َ', 'ْ', '', 'َ', 'َ', 'َ', '', '', 'ْ', 'َ', '٤', 'ُ', '', 'َ', 'َ', 'ُ', '', '', 'َ', 'ْ', '', 'َ', '', 'َ', '', '', '', '٤', 'ْ', 'َ', 'ِ', '٥']\n"
     ]
    }
   ],
   "source": [
    "print(tashkeel_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7183\n",
      "7183\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(tashkeel_sequences_padded[0]))\n",
    "print(len(tashkeel_sequences_padded[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324\n",
      "324\n",
      "['َ ْ ُ ُ UNK َ ْ UNK َ َ َ UNK UNK ْ َ ٤ ُ UNK َ َ ُ UNK UNK َ ْ UNK َ UNK َ UNK UNK UNK ٤ ْ َ ِ ٥']\n",
      "['ب ل َ ُ UNK ن ل UNK ب ص و UNK   َ ن ل َ UNK ْ ع ُ UNK د َ ة UNK ب   َ UNK   َ ٍ ي ٤ إ ْ']\n"
     ]
    }
   ],
   "source": [
    "print(len(tashkeel_sequences[1]))\n",
    "print(len(char_sequences_without_tashkeel[1]))\n",
    "\n",
    "print(char_tokenizer_with_tashkeel.sequences_to_texts([tashkeel_sequences[0]]))\n",
    "print(char_tokenizer_with_tashkeel.sequences_to_texts([char_sequences_without_tashkeel[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Tashkeel only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tashkeel_tokenizer = Tokenizer(char_level=True, oov_token='UNK')\n",
    "tashkeel_tokenizer.fit_on_texts(tashkeel_list)\n",
    "tashkeel_index = tashkeel_tokenizer.word_index\n",
    "tashkeel_list_sequences = tashkeel_tokenizer.texts_to_sequences(tashkeel_list)\n",
    "dev_tashkeel_list_sequences = tashkeel_tokenizer.texts_to_sequences(dev_tashkeel_list)\n",
    "\n",
    "tashkeel_list_sequences_padded = pad_sequences(tashkeel_list_sequences, padding='post')\n",
    "dev_tashkeel_list_sequences_padded = pad_sequences(dev_tashkeel_list_sequences, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tashkeel_list_sequences[0:3])\n",
    "# print(tashkeel_index)\n",
    "# print(len(tashkeel_list_sequences_padded[0]))\n",
    "\n",
    "for i in range(len(dev_tashkeel_sequences)):\n",
    "    if len(dev_tashkeel_sequences[i]) != len(dev_char_sequences_without_tashkeel[i]):\n",
    "        print(i)\n",
    "# print(len(dev_tashkeel_list[2]))\n",
    "# print(len(dev_sentences[2]))\n",
    "\n",
    "# print(len(dev_tashkeel_sequences[2]))\n",
    "# print(char_index_without_tashkeel)\n",
    "# print(len(dev_char_sequences_without_tashkeel[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickles/tashkeel_sequences.pkl', 'wb') as file:\n",
    "    pickle.dump(tashkeel_list_sequences_padded, file)\n",
    "\n",
    "with open('./pickles/val_tashkeel_sequences.pkl', 'wb') as file:\n",
    "    pickle.dump(dev_tashkeel_list_sequences_padded, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 5, 6, 6, 2, 3, 5, 2, 3, 3, 3, 2, 2, 5, 3, 7, 6, 2, 3, 3, 6, 2, 2, 3, 5, 2, 3, 2, 3, 2, 2, 2, 7, 5, 3, 4, 12]\n"
     ]
    }
   ],
   "source": [
    "print(tashkeel_list_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Diacritics list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickles/sentence_diacritics_appearance.pickle', 'rb') as file:\n",
    "    sentence_diacritics_appearance = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickles/val_sentence_diacritics_appearance.pickle', 'rb') as file:\n",
    "    val_sentence_diacritics_appearance = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UNK': 1, '111111111111111': 2, '000000000000001': 3, '101010101000001': 4, '111111111010111': 5, '111111111011101': 6, '111111100000001': 7, '111111100000000': 8, '000011000000001': 9, '101111101011111': 10, '111111101010101': 11, '111111000000000': 12, '001000000000001': 13}\n"
     ]
    }
   ],
   "source": [
    "sentence_diacritics_appearance_tokenizer = Tokenizer(oov_token='UNK')\n",
    "sentence_diacritics_appearance_tokenizer.fit_on_texts(sentence_diacritics_appearance)\n",
    "sentence_diacritics_appearance_word_index = sentence_diacritics_appearance_tokenizer.word_index\n",
    "sentence_diacritics_appearance_sequences = sentence_diacritics_appearance_tokenizer.texts_to_sequences(sentence_diacritics_appearance)\n",
    "\n",
    "#padding\n",
    "sentence_diacritics_appearance_sequences_padded = pad_sequences(sentence_diacritics_appearance_sequences, padding='post')\n",
    "print(sentence_diacritics_appearance_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sentence_diacritics_appearance_sequences = Tokenizer(oov_token='UNK')\n",
    "val_sentence_diacritics_appearance_sequences.fit_on_texts(val_sentence_diacritics_appearance)\n",
    "val_sentence_diacritics_appearance_word_index = val_sentence_diacritics_appearance_sequences.word_index\n",
    "val_sentence_diacritics_appearance_sequences = sentence_diacritics_appearance_tokenizer.texts_to_sequences(val_sentence_diacritics_appearance)\n",
    "\n",
    "#padding\n",
    "val_sentence_diacritics_appearance_sequences_padded = pad_sequences(val_sentence_diacritics_appearance_sequences, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "with open('./pickles/sentence_diacritics_appearance_sequences.pickle', 'wb') as file:\n",
    "    pickle.dump(sentence_diacritics_appearance_sequences_padded, file)\n",
    "\n",
    "with open('./pickles/val_sentence_diacritics_appearance_sequences.pickle', 'wb') as file:\n",
    "    pickle.dump(val_sentence_diacritics_appearance_sequences_padded, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "print(len(sentence_diacritics_appearance_sequences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

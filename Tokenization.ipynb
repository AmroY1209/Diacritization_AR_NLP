{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Anaconda\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import pyarabic.araby as araby\n",
    "from pyarabic.araby import strip_tashkeel\n",
    "import qalsadi.lemmatizer \n",
    "import qalsadi.analex as qa\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "from farasa.pos import FarasaPOSTagger \n",
    "from farasa.ner import FarasaNamedEntityRecognizer \n",
    "from farasa.diacratizer import FarasaDiacritizer \n",
    "from farasa.segmenter import FarasaSegmenter \n",
    "from farasa.stemmer import FarasaStemmer\n",
    "\n",
    "import keras\n",
    "from  diacritization_evaluation import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['قوله', 'أو', 'قطع', 'الأول', 'يده', 'إلخ', 'قال', 'الزركشي', 'ابن', 'عرفة']\n",
      "قوله أو قطع الأول يده إلخ قال الزركشي ابن عرفة قوله بلفظ يقتضيه كإنكار غير حديث بالإسلام وجوب ما علم وجوبه من الدين ضرورة كإلقاء مصحف بقذر وشد زنار ابن عرفة قول ابن شاس أو بفعل يتضمنه هو كلبس الزنار وإلقاء المصحف في صريح النجاسة والسجود للصنم ونحو ذلك وسحر محمد قول مالك وأصحابه أن الساحر كافر بالله تعالى قال مالك هو كالزنديق إذا عمل السحر بنفسه قتل ولم يستتب\n",
      "قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ ابْنُ عَرَفَةَ قَوْلُهُ بِلَفْظٍ يَقْتَضِيه كَإِنْكَارِ غَيْرِ حَدِيثٍ بِالْإِسْلَامِ وُجُوبَ مَا عُلِمَ وُجُوبُهُ مِنْ الدِّينِ ضَرُورَةً كَإِلْقَاءِ مُصْحَفٍ بِقَذَرٍ وَشَدِّ زُنَّارٍ ابْنُ عَرَفَةَ قَوْلُ ابْنِ شَاسٍ أَوْ بِفِعْلٍ يَتَضَمَّنُهُ هُوَ كَلُبْسِ الزُّنَّارِ وَإِلْقَاءِ الْمُصْحَفِ فِي صَرِيحِ النَّجَاسَةِ وَالسُّجُودِ لِلصَّنَمِ وَنَحْوِ ذَلِكَ وَسِحْرٍ مُحَمَّدٌ قَوْلُ مَالِكٍ وَأَصْحَابِهِ أَنَّ السَّاحِرَ كَافِرٌ بِاَللَّهِ تَعَالَى قَالَ مَالِكٌ هُوَ كَالزِّنْدِيقِ إذَا عَمِلَ السِّحْرَ بِنَفْسِهِ قُتِلَ وَلَمْ يُسْتَتَبْ\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "sentences = []\n",
    "sentences_with_tashkeel = []\n",
    "with open('./Dataset/WordsWithoutTashkeel.txt', 'r', encoding='utf-8') as output_file:\n",
    "    for word in output_file:\n",
    "        words.append(word.strip())\n",
    "\n",
    "with open('./Dataset/SentencesWithoutTashkeel.txt', 'r', encoding='utf-8') as output_file:\n",
    "    for sentence in output_file:\n",
    "        sentences.append(sentence.strip())\n",
    "\n",
    "with open('./Dataset/sentences.txt', 'r', encoding='utf-8') as output_file:\n",
    "    for sentence in output_file:\n",
    "        sentences_with_tashkeel.append(sentence.strip())\n",
    "\n",
    "print(words[0:10])\n",
    "print(sentences[0])\n",
    "print(sentences_with_tashkeel[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ ابْنُ عَرَفَةَ قَوْلُهُ بِلَفْظٍ يَقْتَضِيه كَإِنْكَارِ غَيْرِ حَدِيثٍ بِالْإِسْلَامِ وُجُوبَ مَا عُلِمَ وُجُوبُهُ مِنْ الدِّينِ ضَرُورَةً كَإِلْقَاءِ مُصْحَفٍ بِقَذَرٍ وَشَدِّ زُنَّارٍ ابْنُ عَرَفَةَ قَوْلُ ابْنِ شَاسٍ أَوْ بِفِعْلٍ يَتَضَمَّنُهُ هُوَ كَلُبْسِ الزُّنَّارِ وَإِلْقَاءِ الْمُصْحَفِ فِي صَرِيحِ النَّجَاسَةِ وَالسُّجُودِ لِلصَّنَمِ وَنَحْوِ ذَلِكَ وَسِحْرٍ مُحَمَّدٌ قَوْلُ مَالِكٍ وَأَصْحَابِهِ أَنَّ السَّاحِرَ كَافِرٌ بِاَللَّهِ تَعَالَى قَالَ مَالِكٌ هُوَ كَالزِّنْدِيقِ إذَا عَمِلَ السِّحْرَ بِنَفْسِهِ قُتِلَ وَلَمْ يُسْتَتَبْ\n"
     ]
    }
   ],
   "source": [
    "print(sentences_with_tashkeel[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a word-based tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenizer\n",
    "words_tokenizer = Tokenizer()\n",
    "\n",
    "# Fit the tokenizer on the list of words (treat each word as a separate \"sentence\")\n",
    "words_tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Get the word index\n",
    "word_index = words_tokenizer.word_index\n",
    "\n",
    "# Tokenize the words\n",
    "word_sequences = words_tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the tokenizer on a sample sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['قوله', 'أو', 'قطع', 'الأول', 'يده', 'إلخ', 'قال', 'الزركشي', 'ابن', 'عرفة', 'قوله', 'بلفظ', 'يقتضيه', 'كإنكار', 'غير', 'حديث', 'بالإسلام', 'وجوب', 'ما', 'علم', 'وجوبه', 'من', 'الدين', 'ضرورة', 'كإلقاء', 'مصحف', 'بقذر', 'وشد', 'زنار', 'ابن', 'عرفة', 'قول', 'ابن', 'شاس', 'أو', 'بفعل', 'يتضمنه', 'هو', 'كلبس', 'الزنار', 'وإلقاء', 'المصحف', 'في', 'صريح', 'النجاسة', 'والسجود', 'للصنم', 'ونحو', 'ذلك', 'وسحر', 'محمد', 'قول', 'مالك', 'وأصحابه', 'أن', 'الساحر', 'كافر', 'بالله', 'تعالى', 'قال', 'مالك', 'هو', 'كالزنديق', 'إذا', 'عمل', 'السحر', 'بنفسه', 'قتل', 'ولم', 'يستتب']\n",
      "8\n",
      "[[8, 4, 273, 91, 189, 47, 14, 901, 37, 458, 8, 1184, 3641, 41299, 39, 249, 4154, 475, 12, 163, 1738, 3, 158, 1175, 27872, 6099, 41300, 5827, 41301, 37, 458, 63, 37, 2652, 4, 1362, 24279, 43, 17883, 57670, 41302, 2961, 2, 1430, 2148, 9577, 57671, 1189, 18, 21681, 86, 63, 94, 3359, 6, 16422, 1190, 1185, 108, 14, 94, 43, 27873, 28, 520, 8288, 415, 264, 51, 57672]]\n"
     ]
    }
   ],
   "source": [
    "# Create a sentence tokenizer\n",
    "print((sentences[0].split(\" \"))) #This way works and the one used below as well\n",
    "print(word_index[\"قوله\"])\n",
    "print(words_tokenizer.texts_to_sequences([sentences[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a character-based with tashkeel tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20, 2, 11, 5, 6, 8, 13, 8, 3, 18, 2, 11, 5, 3, 20, 2, 41, 2, 16, 2, 3, 7, 6, 5, 18, 2, 11, 17, 6, 8, 3, 10, 2, 22, 2, 13, 8, 3, 28, 6, 2, 34, 5, 3, 20, 2, 7, 6, 2, 3, 7, 6, 42, 17, 15, 5, 23, 2, 37, 4, 10, 43, 3, 7, 14, 5, 12, 8, 3, 16, 2, 15, 2, 19, 2, 26, 2, 3, 20, 2, 11, 5, 6, 8, 13, 8, 3, 14, 4, 6, 2, 19, 5, 47, 31, 3, 10, 2, 20, 5, 21, 2, 38, 4, 10, 13, 3, 23, 2, 28, 4, 12, 5, 23, 2, 7, 15, 4, 3, 44, 2, 10, 5, 15, 4, 3, 24, 2, 22, 4, 10, 35, 31, 3, 14, 4, 7, 6, 5, 28, 4, 25, 5, 6, 2, 7, 9, 4, 3, 11, 8, 27, 8, 11, 14, 2, 3, 9, 2, 7, 3, 16, 8, 6, 4, 9, 2, 3, 11, 8, 27, 8, 11, 14, 8, 13, 8, 3, 9, 4, 12, 5, 3, 7, 6, 22, 36, 10, 12, 4, 3, 38, 2, 15, 8, 11, 15, 2, 26, 39, 3, 23, 2, 28, 4, 6, 5, 20, 2, 7, 45, 4, 3, 9, 8, 30, 5, 24, 2, 19, 31, 3, 14, 4, 20, 2, 29, 2, 15, 31, 3, 11, 2, 37, 2, 22, 36, 3, 42, 8, 12, 17, 7, 15, 31, 3, 7, 14, 5, 12, 8, 3, 16, 2, 15, 2, 19, 2, 26, 2, 3, 20, 2, 11, 5, 6, 8, 3, 7, 14, 5, 12, 4, 3, 37, 2, 7, 25, 31, 3, 18, 2, 11, 5, 3, 14, 4, 19, 4, 16, 5, 6, 31, 3, 10, 2, 21, 2, 38, 2, 9, 17, 12, 8, 13, 8, 3, 13, 8, 11, 2, 3, 23, 2, 6, 8, 14, 5, 25, 4, 3, 7, 6, 42, 43, 12, 17, 7, 15, 4, 3, 11, 2, 28, 4, 6, 5, 20, 2, 7, 45, 4, 3, 7, 6, 5, 9, 8, 30, 5, 24, 2, 19, 4, 3, 19, 4, 10, 3, 30, 2, 15, 4, 10, 24, 4, 3, 7, 6, 12, 17, 27, 2, 7, 25, 2, 26, 4, 3, 11, 2, 7, 6, 25, 43, 27, 8, 11, 22, 4, 3, 6, 4, 6, 30, 17, 12, 2, 9, 4, 3, 11, 2, 12, 2, 24, 5, 11, 4, 3, 29, 2, 6, 4, 23, 2, 3, 11, 2, 25, 4, 24, 5, 15, 31, 3, 9, 8, 24, 2, 9, 17, 22, 40, 3, 20, 2, 11, 5, 6, 8, 3, 9, 2, 7, 6, 4, 23, 31, 3, 11, 2, 18, 2, 30, 5, 24, 2, 7, 14, 4, 13, 4, 3, 18, 2, 12, 17, 3, 7, 6, 25, 17, 7, 24, 4, 15, 2, 3, 23, 2, 7, 19, 4, 15, 40, 3, 14, 4, 7, 2, 6, 6, 17, 13, 4, 3, 21, 2, 16, 2, 7, 6, 2, 33, 3, 20, 2, 7, 6, 2, 3, 9, 2, 7, 6, 4, 23, 40, 3, 13, 8, 11, 2, 3, 23, 2, 7, 6, 42, 36, 12, 5, 22, 4, 10, 20, 4, 3, 28, 29, 2, 7, 3, 16, 2, 9, 4, 6, 2, 3, 7, 6, 25, 36, 24, 5, 15, 2, 3, 14, 4, 12, 2, 19, 5, 25, 4, 13, 4, 3, 20, 8, 21, 4, 6, 2, 3, 11, 2, 6, 2, 9, 5, 3, 10, 8, 25, 5, 21, 2, 21, 2, 14, 5], [20, 2, 11, 5, 6, 8, 13, 8, 3, 6, 4, 16, 2, 22, 2, 9, 4, 3, 9, 2, 7, 3, 21, 2, 21, 2, 16, 2, 6, 17, 20, 8, 3, 28, 6, 2, 34, 5, 3, 18, 2, 10, 5, 3, 7, 6, 5, 11, 2, 30, 4, 10, 17, 26, 8, 3, 20, 2, 11, 5, 6, 8, 13, 8, 3, 9, 2, 7, 3, 9, 2, 15, 17, 3, 18, 2, 10, 5, 3, 20, 8, 14, 2, 10, 5, 6, 2, 3, 20, 2, 11, 5, 6, 4, 3, 7, 6, 5, 9, 2, 21, 5, 12, 4, 3, 6, 2, 44, 2, 21, 5, 3, 11, 2, 6, 2, 11, 5, 3, 7, 20, 5, 21, 2, 30, 2, 15, 2, 3, 16, 2, 6, 2, 33, 3, 18, 2, 11, 5, 30, 2, 10, 5, 21, 3, 6, 2, 13, 8, 3, 14, 4, 37, 2, 7, 26, 31, 3, 18, 2, 11, 5, 3, 18, 2, 16, 5, 41, 8, 11, 13, 8, 3, 37, 2, 7, 26, 39, 3, 11, 2, 6, 2, 7, 3, 44, 2, 12, 2, 9, 2, 3, 6, 2, 13, 8, 3, 16, 4, 12, 5, 22, 2, 3, 7, 6, 5, 9, 2, 11, 5, 21, 4, 3, 13, 2, 6, 5, 3, 21, 2, 14, 5, 41, 8, 6, 8, 3, 7, 6, 5, 11, 2, 30, 4, 10, 17, 26, 8, 3, 18, 2, 11, 5, 3, 10, 8, 37, 5, 21, 2, 15, 2, 33, 3, 6, 2, 13, 8, 3, 37, 2, 7, 26, 40, 3, 11, 2, 10, 8, 49, 5, 34, 2, 29, 8, 3, 9, 4, 12, 5, 3, 20, 2, 11, 5, 6, 4, 13, 4, 3, 7, 6, 5, 48, 21, 4, 10, 3, 23, 2, 9, 2, 7, 3, 6, 2, 11, 5, 3, 6, 2, 9, 5, 3, 10, 2, 20, 8, 6, 5, 3, 9, 4, 12, 5, 3, 9, 2, 7, 6, 4, 10, 3, 11, 2, 6, 2, 7, 3, 9, 4, 12, 5, 3, 44, 2, 12, 2, 9, 4, 10, 3, 18, 2, 12, 17, 13, 2, 7, 3, 6, 2, 7, 3, 21, 2, 14, 5, 41, 8, 6, 8, 3, 32, 3, 11, 2, 16, 4, 14, 2, 7, 15, 2, 26, 8, 3, 7, 6, 5, 23, 2, 12, 5, 42, 4, 3, 11, 2, 6, 2, 11, 5, 3, 6, 2, 9, 5, 3, 10, 2, 20, 8, 6, 5, 3, 9, 4, 12, 5, 3, 9, 2, 7, 6, 4, 10, 3, 11, 2, 6, 2, 7, 3, 9, 4, 12, 5, 3, 44, 2, 12, 2, 9, 4, 10, 3, 6, 2, 9, 5, 3, 10, 2, 21, 2, 16, 2, 10, 17, 12, 5, 3, 44, 2, 12, 2, 9, 8, 13, 8, 3, 28, 12, 5, 3, 23, 2, 7, 12, 2, 21, 5, 3, 7, 12, 5, 21, 2, 13, 2, 21, 5, 3, 25, 9, 3, 20, 2, 11, 5, 6, 8, 13, 8, 3, 19, 2, 10, 8, 16, 5, 41, 2, 33, 3, 11, 2, 7, 24, 4, 22, 2, 26, 39, 3, 9, 4, 12, 5, 13, 2, 7, 3, 28, 6, 2, 34, 5, 3, 23, 2, 9, 2, 7, 3, 6, 2, 11, 5, 3, 23, 2, 7, 12, 2, 21, 5, 3, 9, 2, 11, 5, 27, 8, 11, 22, 2, 26, 39, 3, 16, 4, 12, 5, 22, 2, 3, 7, 6, 5, 11, 2, 30, 4, 10, 17, 26, 4, 3, 11, 2, 7, 6, 5, 9, 2, 11, 5, 21, 4, 3, 32, 3, 11, 2, 6, 2, 7, 3, 10, 2, 27, 8, 11, 42, 8, 3, 18, 2, 12, 5, 3, 10, 8, 16, 5, 41, 2, 33, 3, 11, 2, 7, 24, 4, 22, 2, 26, 39, 3, 9, 4, 12, 5, 3, 44, 2, 10, 5, 15, 4, 3, 44, 2, 12, 2, 9, 4, 13, 4, 3, 19, 4, 10, 3, 7, 6, 30, 43, 11, 15, 2, 21, 2, 10, 5, 12, 4, 3, 11, 2, 28, 4, 12, 5, 3, 21, 2, 15, 2, 7, 38, 2, 10, 2, 7], [6, 4, 18, 2, 12, 17, 13, 8, 3, 30, 8, 6, 5, 24, 40, 3, 16, 2, 6, 2, 33, 3, 9, 2, 27, 5, 13, 8, 11, 6, 31, 3, 9, 8, 44, 5, 12, 4, 10, 3, 11, 2, 12, 4, 13, 2, 7, 10, 2, 26, 40, 3, 20, 2, 7, 6, 2, 3, 20, 2, 11, 5, 6, 8, 13, 8, 3, 11, 2, 7, 24, 4, 22, 2, 26, 39, 3, 9, 4, 12, 5, 13, 2, 7, 3, 18, 2, 10, 5, 3, 23, 2, 7, 9, 4, 6, 2, 26, 39, 3, 32, 3, 11, 2, 6, 2, 7, 3, 10, 2, 27, 8, 11, 42, 8, 3, 18, 2, 12, 5, 3, 10, 8, 16, 5, 41, 2, 33, 3, 12, 4, 30, 5, 19, 2, 10, 5, 12, 4, 3, 9, 4, 12, 5, 3, 37, 2, 7, 21, 2, 10, 5, 12, 4], [6, 4, 18, 2, 12, 17, 13, 8, 3, 6, 2, 7, 3, 10, 8, 25, 2, 9, 17, 33, 3, 37, 2, 7, 26, 39, 3, 11, 2, 20, 2, 11, 5, 6, 8, 13, 8, 3, 11, 2, 6, 2, 7, 3, 10, 2, 27, 8, 11, 42, 8, 3, 18, 2, 12, 5, 3, 10, 8, 16, 5, 41, 2, 33, 3, 11, 2, 7, 24, 4, 22, 2, 26, 39, 3, 9, 4, 12, 5, 3, 44, 2, 10, 5, 15, 4, 3, 44, 2, 12, 2, 9, 4, 13, 4, 3, 11, 2, 10, 2, 12, 5, 14, 2, 44, 4, 10, 3, 18, 2, 12, 5, 3, 10, 8, 20, 2, 7, 6, 2, 3, 9, 4, 35, 5, 6, 8, 3, 29, 2, 6, 4, 23, 2, 3, 19, 4, 10, 3, 7, 6, 5, 18, 2, 15, 4, 20, 17, 7, 45, 4], [11, 2, 24, 2, 10, 2, 11, 2, 7, 12, 40, 3, 44, 2, 10, 5, 15, 8, 3, 9, 2, 11, 5, 27, 8, 11, 22, 31]]\n",
      "{'UNK': 1, 'َ': 2, ' ': 3, 'ِ': 4, 'ْ': 5, 'ل': 6, 'ا': 7, 'ُ': 8, 'م': 9, 'ي': 10, 'و': 11, 'ن': 12, 'ه': 13, 'ب': 14, 'ر': 15, 'ع': 16, 'd': 17, 'أ': 18, 'ف': 19, 'ق': 20, 'ت': 21, 'د': 22, 'ك': 23, 'ح': 24, 'س': 25, 'ة': 26, 'ج': 27, 'إ': 28, 'ذ': 29, 'ص': 30, 'ٍ': 31, '،': 32, 'ى': 33, 'خ': 34, 'ث': 35, 'f': 36, 'ش': 37, 'ض': 38, 'ً': 39, 'ٌ': 40, 'ط': 41, 'ز': 42, 'e': 43, 'غ': 44, 'ء': 45, 'ئ': 46, 'ظ': 47, 'آ': 48, 'ؤ': 49, 'ّ': 50, 'c': 51, 'b': 52, 'a': 53}\n"
     ]
    }
   ],
   "source": [
    "sentences_new = []\n",
    "with open('./Dataset/sentences_new_approach.txt', 'r', encoding='utf-8') as output_file:\n",
    "    for sentence in output_file:\n",
    "        sentences_new.append(sentence.strip())\n",
    "\n",
    "char_tokenizer_with_tashkeel = Tokenizer(char_level=True, oov_token='UNK')\n",
    "char_tokenizer_with_tashkeel.fit_on_texts(sentences_new)\n",
    "char_index_with_tashkeel = char_tokenizer_with_tashkeel.word_index\n",
    "char_sequences_with_tashkeel = char_tokenizer_with_tashkeel.texts_to_sequences(sentences_new)\n",
    "print(char_sequences_with_tashkeel[0:5])\n",
    "print(char_index_with_tashkeel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a character-based without tashkeel tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_tokenizer_without_tashkeel = Tokenizer(char_level=True)\n",
    "char_tokenizer_without_tashkeel.fit_on_texts(sentences)\n",
    "char_index_without_tashkeel = char_tokenizer_without_tashkeel.word_index\n",
    "char_sequences_without_tashkeel = char_tokenizer_without_tashkeel.texts_to_sequences(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_sequences_without_tashkeel = char_tokenizer_without_tashkeel.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14, 6, 2, 8, 1, 12, 6, 1, 14, 31, 11, 1, 3, 2, 12, 6, 2, 1, 5, 16, 8, 1, 22, 2, 27, 1, 14, 3, 2, 1, 3, 2, 32, 10, 17, 29, 5, 1, 3, 9, 7, 1, 11, 10, 13, 20, 1, 14, 6, 2, 8, 1, 9, 2, 13, 36, 1, 5, 14, 15, 30, 5, 8, 1, 17, 22, 7, 17, 3, 10, 1, 33, 5, 10, 1, 18, 16, 5, 28, 1, 9, 3, 2, 22, 19, 2, 3, 4, 1, 6, 21, 6, 9, 1, 4, 3, 1, 11, 2, 4, 1, 6, 21, 6, 9, 8, 1, 4, 7, 1, 3, 2, 16, 5, 7, 1, 30, 10, 6, 10, 20, 1, 17, 22, 2, 14, 3, 34, 1, 4, 24, 18, 13, 1, 9, 14, 23, 10, 1, 6, 29, 16, 1, 32, 7, 3, 10, 1, 3, 9, 7, 1, 11, 10, 13, 20, 1, 14, 6, 2, 1, 3, 9, 7, 1, 29, 3, 19, 1, 12, 6, 1, 9, 13, 11, 2, 1, 5, 15, 30, 4, 7, 8, 1, 8, 6, 1, 17, 2, 9, 19, 1, 3, 2, 32, 7, 3, 10, 1, 6, 22, 2, 14, 3, 34, 1, 3, 2, 4, 24, 18, 13, 1, 13, 5, 1, 24, 10, 5, 18, 1, 3, 2, 7, 21, 3, 19, 20, 1, 6, 3, 2, 19, 21, 6, 16, 1, 2, 2, 24, 7, 4, 1, 6, 7, 18, 6, 1, 23, 2, 17, 1, 6, 19, 18, 10, 1, 4, 18, 4, 16, 1, 14, 6, 2, 1, 4, 3, 2, 17, 1, 6, 12, 24, 18, 3, 9, 8, 1, 12, 7, 1, 3, 2, 19, 3, 18, 10, 1, 17, 3, 13, 10, 1, 9, 3, 2, 2, 8, 1, 15, 11, 3, 2, 26, 1, 14, 3, 2, 1, 4, 3, 2, 17, 1, 8, 6, 1, 17, 3, 2, 32, 7, 16, 5, 14, 1, 22, 23, 3, 1, 11, 4, 2, 1, 3, 2, 19, 18, 10, 1, 9, 7, 13, 19, 8, 1, 14, 15, 2, 1, 6, 2, 4, 1, 5, 19, 15, 15, 9]]\n",
      "38\n",
      "{' ': 1, 'ل': 2, 'ا': 3, 'م': 4, 'ي': 5, 'و': 6, 'ن': 7, 'ه': 8, 'ب': 9, 'ر': 10, 'ع': 11, 'أ': 12, 'ف': 13, 'ق': 14, 'ت': 15, 'د': 16, 'ك': 17, 'ح': 18, 'س': 19, 'ة': 20, 'ج': 21, 'إ': 22, 'ذ': 23, 'ص': 24, '،': 25, 'ى': 26, 'خ': 27, 'ث': 28, 'ش': 29, 'ض': 30, 'ط': 31, 'ز': 32, 'غ': 33, 'ء': 34, 'ئ': 35, 'ظ': 36, 'آ': 37, 'ؤ': 38}\n",
      "{'UNK': 1, 'َ': 2, ' ': 3, 'ِ': 4, 'ْ': 5, 'ل': 6, 'ا': 7, 'ُ': 8, 'م': 9, 'ي': 10, 'و': 11, 'ن': 12, 'ه': 13, 'ب': 14, 'ر': 15, 'ع': 16, 'd': 17, 'أ': 18, 'ف': 19, 'ق': 20, 'ت': 21, 'د': 22, 'ك': 23, 'ح': 24, 'س': 25, 'ة': 26, 'ج': 27, 'إ': 28, 'ذ': 29, 'ص': 30, 'ٍ': 31, '،': 32, 'ى': 33, 'خ': 34, 'ث': 35, 'f': 36, 'ش': 37, 'ض': 38, 'ً': 39, 'ٌ': 40, 'ط': 41, 'ز': 42, 'e': 43, 'غ': 44, 'ء': 45, 'ئ': 46, 'ظ': 47, 'آ': 48, 'ؤ': 49, 'ّ': 50, 'c': 51, 'b': 52, 'a': 53}\n"
     ]
    }
   ],
   "source": [
    "print(char_sequences_without_tashkeel[0:1])\n",
    "print(len(char_index_without_tashkeel.keys()))\n",
    "print(char_index_without_tashkeel)\n",
    "print(char_index_with_tashkeel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding tests for the character-based tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UNK': 1, 'َ': 2, ' ': 3, 'ِ': 4, 'ْ': 5, 'ل': 6, 'ا': 7, 'ُ': 8, 'م': 9, 'ي': 10, 'و': 11, 'ن': 12, 'ه': 13, 'ب': 14, 'ر': 15, 'ع': 16, 'd': 17, 'أ': 18, 'ف': 19, 'ق': 20, 'ت': 21, 'د': 22, 'ك': 23, 'ح': 24, 'س': 25, 'ة': 26, 'ج': 27, 'إ': 28, 'ذ': 29, 'ص': 30, 'ٍ': 31, '،': 32, 'ى': 33, 'خ': 34, 'ث': 35, 'f': 36, 'ش': 37, 'ض': 38, 'ً': 39, 'ٌ': 40, 'ط': 41, 'ز': 42, 'e': 43, 'غ': 44, 'ء': 45, 'ئ': 46, 'ظ': 47, 'آ': 48, 'ؤ': 49, 'ّ': 50, 'c': 51, 'b': 52, 'a': 53}\n",
      "[[20, 2, 7, 6, 8, 11, 7], [6, 4, 18, 2, 12, 17, 13, 8], [6, 2, 11, 5], [11, 2, 27, 2, 14, 2], [16, 2, 6, 2, 10, 5, 13, 4, 9, 5], [24, 2, 9, 5, 6, 8, 13, 2, 7], [6, 2, 14, 2, 41, 2, 6, 2, 21, 5], [7, 6, 30, 17, 6, 2, 7, 26, 8], [14, 4, 21, 2, 15, 5, 23, 4, 13, 2, 7]]\n"
     ]
    }
   ],
   "source": [
    "print(char_index_with_tashkeel)\n",
    "print(char_tokenizer_with_tashkeel.texts_to_sequences(sentences_new[12].split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: مرحبا كيف حالك\n",
      "Tokenized Sequence: [9, 15, 24, 14, 7, 3, 23, 10, 19, 3, 24, 7, 6, 23]\n",
      "\n",
      "Original Text: السلام عليكم\n",
      "Tokenized Sequence: [7, 6, 25, 6, 7, 9, 3, 16, 6, 10, 23, 9]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# New text data for testing\n",
    "new_texts = [\"مرحبا كيف حالك\", \"السلام عليكم\"]\n",
    "\n",
    "# Tokenize the new text data at the character level\n",
    "sequences_new = char_tokenizer_with_tashkeel.texts_to_sequences(new_texts)\n",
    "\n",
    "# Print the results\n",
    "for text, sequence in zip(new_texts, sequences_new):\n",
    "    print(f\"Original Text: {text}\")\n",
    "    print(f\"Tokenized Sequence: {sequence}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addding Padding to the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming word_sequences and char_sequences are the output of the tokenizers\n",
    "word_sequences = words_tokenizer.texts_to_sequences(sentences)\n",
    "char_sequences = char_tokenizer_with_tashkeel.texts_to_sequences(sentences_new)\n",
    "\n",
    "# Add padding\n",
    "word_sequences_padded = pad_sequences(word_sequences, padding='post')\n",
    "char_sequences_with_tashkeel_padded = pad_sequences(char_sequences, padding='post')\n",
    "char_sequences_without_tashkeel_padded = pad_sequences(char_sequences_without_tashkeel, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n",
      "4263\n"
     ]
    }
   ],
   "source": [
    "print(len(char_sequences[5]))\n",
    "print(len(char_sequences_with_tashkeel_padded[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenized sequences\n",
    "with open('./pickles/word_sequences.pkl', 'wb') as file:\n",
    "    pickle.dump(word_sequences_padded, file)\n",
    "\n",
    "with open('./pickles/char_sequences_with_tashkeel.pkl', 'wb') as file:\n",
    "    pickle.dump(char_sequences_with_tashkeel_padded, file)\n",
    "\n",
    "with open('./pickles/char_sequences_without_tashkeel.pkl', 'wb') as file:\n",
    "    pickle.dump(char_sequences_without_tashkeel_padded, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences_with_tashkeel\n",
    "tashkeel_list = []\n",
    "for sentence in sentences_with_tashkeel:\n",
    "    text, txt_list, harakat_list = util.extract_haraqat(sentence)   \n",
    "    for i in range(len(harakat_list)):\n",
    "        if len(harakat_list[i]) == 2:\n",
    "            if '\\u0651\\u064B' in harakat_list[i]:\n",
    "                harakat_list[i] = 'a'\n",
    "            if '\\u0651\\u064C' in harakat_list[i]:\n",
    "                harakat_list[i] = 'b'\n",
    "            if '\\u0651\\u064D' in harakat_list[i]:\n",
    "                harakat_list[i] = 'c'\n",
    "            if '\\u0651\\u064E' in harakat_list[i]:\n",
    "                harakat_list[i] = 'd'\n",
    "            if '\\u0651\\u064F' in harakat_list[i]:\n",
    "                harakat_list[i] = 'e'\n",
    "            if '\\u0651\\u0650' in harakat_list[i]:\n",
    "                harakat_list[i] = 'f'\n",
    "\n",
    "    tashkeel_list.append(harakat_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tashkeel_sequences = char_tokenizer_with_tashkeel.texts_to_sequences(tashkeel_list)\n",
    "tashkeel_sequences_padded = pad_sequences(tashkeel_sequences, padding='post')\n",
    "with open('tashkeel_sequences.pkl', 'wb') as file:\n",
    "    pickle.dump(tashkeel_sequences_padded, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['َ', 'ْ', 'ُ', 'ُ', '', 'َ', 'ْ', '', 'َ', 'َ', 'َ', '', '', 'ْ', 'َ', 'd', 'ُ', '', 'َ', 'َ', 'ُ', '', '', 'َ', 'ْ', '', 'َ', '', 'َ', '', '', '', 'd', 'ْ', 'َ', 'ِ', 'e', '', '', 'ْ', 'ُ', '', 'َ', 'َ', 'َ', 'َ', '', 'َ', 'ْ', 'ُ', 'ُ', '', 'ِ', 'َ', 'ْ', 'ٍ', '', 'َ', 'ْ', 'َ', 'ِ', '', '', '', 'َ', 'ِ', 'ْ', 'َ', '', 'ِ', '', 'َ', 'ْ', 'ِ', '', 'َ', 'ِ', '', 'ٍ', '', 'ِ', '', 'ْ', 'ِ', 'ْ', 'َ', '', 'ِ', '', 'ُ', 'ُ', '', 'َ', '', 'َ', '', '', 'ُ', 'ِ', 'َ', '', 'ُ', 'ُ', '', 'ُ', 'ُ', '', 'ِ', 'ْ', '', '', '', 'f', '', 'ِ', '', 'َ', 'ُ', '', 'َ', 'ً', '', 'َ', 'ِ', 'ْ', 'َ', '', 'ِ', '', 'ُ', 'ْ', 'َ', 'ٍ', '', 'ِ', 'َ', 'َ', 'ٍ', '', 'َ', 'َ', 'f', '', 'ُ', 'd', '', 'ٍ', '', '', 'ْ', 'ُ', '', 'َ', 'َ', 'َ', 'َ', '', 'َ', 'ْ', 'ُ', '', '', 'ْ', 'ِ', '', 'َ', '', 'ٍ', '', 'َ', 'ْ', '', 'ِ', 'ِ', 'ْ', 'ٍ', '', 'َ', 'َ', 'َ', 'd', 'ُ', 'ُ', '', 'ُ', 'َ', '', 'َ', 'ُ', 'ْ', 'ِ', '', '', '', 'e', 'd', '', 'ِ', '', 'َ', 'ِ', 'ْ', 'َ', '', 'ِ', '', '', 'ْ', 'ُ', 'ْ', 'َ', 'ِ', '', 'ِ', '', '', 'َ', 'ِ', '', 'ِ', '', '', '', 'd', 'َ', '', 'َ', 'ِ', '', 'َ', '', '', 'e', 'ُ', '', 'ِ', '', 'ِ', '', 'd', 'َ', 'ِ', '', 'َ', 'َ', 'ْ', 'ِ', '', 'َ', 'ِ', 'َ', '', 'َ', 'ِ', 'ْ', 'ٍ', '', 'ُ', 'َ', 'd', 'ٌ', '', 'َ', 'ْ', 'ُ', '', 'َ', '', 'ِ', 'ٍ', '', 'َ', 'َ', 'ْ', 'َ', '', 'ِ', 'ِ', '', 'َ', 'd', '', '', '', 'd', '', 'ِ', 'َ', '', 'َ', '', 'ِ', 'ٌ', '', 'ِ', 'َ', '', 'd', 'ِ', '', 'َ', 'َ', '', 'َ', '', '', 'َ', '', 'َ', '', 'َ', '', 'ِ', 'ٌ', '', 'ُ', 'َ', '', 'َ', '', '', 'f', 'ْ', 'ِ', '', 'ِ', '', '', 'َ', '', '', 'َ', 'ِ', 'َ', '', '', '', 'f', 'ْ', 'َ', '', 'ِ', 'َ', 'ْ', 'ِ', 'ِ', '', 'ُ', 'ِ', 'َ', '', 'َ', 'َ', 'ْ', '', 'ُ', 'ْ', 'َ', 'َ', 'ْ']\n"
     ]
    }
   ],
   "source": [
    "print(tashkeel_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2592\n",
      "2592\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(tashkeel_sequences_padded[0]))\n",
    "print(len(tashkeel_sequences_padded[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425\n",
      "425\n",
      "['َ ْ ُ ُ UNK َ ْ UNK َ َ َ UNK UNK ْ َ d ُ UNK َ َ ُ UNK UNK َ ْ UNK َ UNK َ UNK UNK UNK d ْ َ ِ e UNK UNK ْ ُ UNK َ َ َ َ UNK َ ْ ُ ُ UNK ِ َ ْ ٍ UNK َ ْ َ ِ UNK UNK UNK َ ِ ْ َ UNK ِ UNK َ ْ ِ UNK َ ِ UNK ٍ UNK ِ UNK ْ ِ ْ َ UNK ِ UNK ُ ُ UNK َ UNK َ UNK UNK ُ ِ َ UNK ُ ُ UNK ُ ُ UNK ِ ْ UNK UNK UNK f UNK ِ UNK َ ُ UNK َ ً UNK َ ِ ْ َ UNK ِ UNK ُ ْ َ ٍ UNK ِ َ َ ٍ UNK َ َ f UNK ُ d UNK ٍ UNK UNK ْ ُ UNK َ َ َ َ UNK َ ْ ُ UNK UNK ْ ِ UNK َ UNK ٍ UNK َ ْ UNK ِ ِ ْ ٍ UNK َ َ َ d ُ ُ UNK ُ َ UNK َ ُ ْ ِ UNK UNK UNK e d UNK ِ UNK َ ِ ْ َ UNK ِ UNK UNK ْ ُ ْ َ ِ UNK ِ UNK UNK َ ِ UNK ِ UNK UNK UNK d َ UNK َ ِ UNK َ UNK UNK e ُ UNK ِ UNK ِ UNK d َ ِ UNK َ َ ْ ِ UNK َ ِ َ UNK َ ِ ْ ٍ UNK ُ َ d ٌ UNK َ ْ ُ UNK َ UNK ِ ٍ UNK َ َ ْ َ UNK ِ ِ UNK َ d UNK UNK UNK d UNK ِ َ UNK َ UNK ِ ٌ UNK ِ َ UNK d ِ UNK َ َ UNK َ UNK UNK َ UNK َ UNK َ UNK ِ ٌ UNK ُ َ UNK َ UNK UNK f ْ ِ UNK ِ UNK UNK َ UNK UNK َ ِ َ UNK UNK UNK f ْ َ UNK ِ َ ْ ِ ِ UNK ُ ِ َ UNK َ َ ْ UNK ُ ْ َ َ ْ']\n",
      "['ب ل َ ُ UNK ن ل UNK ب ٍ و UNK   َ ن ل َ UNK ْ ع ُ UNK د َ ج UNK ب   َ UNK   َ ، ي d ذ ْ UNK   م ا UNK و ي ه ق UNK ب ل َ ُ UNK م َ ه f UNK ْ ب ر ص ْ ُ UNK d د ا d   ي UNK ى ْ ي UNK أ ع ْ إ UNK م   َ د ف َ   ِ UNK ل ت ل م UNK ِ   UNK و َ ِ UNK ل ت ل م ُ UNK ِ ا UNK   َ ع ْ ا UNK ص ي ل ي ق UNK d د َ ب   خ UNK ِ ح أ ه UNK م ب ك ي UNK ل ذ ع UNK ، ا   ي UNK   م ا UNK و ي ه ق UNK ب ل َ UNK   م ا UNK ذ   ف UNK ن ل UNK م ه و َ UNK ْ ر ص ِ ا ُ UNK ُ ل UNK d َ م ف UNK   َ ، ا   ي UNK ل د َ ب   خ UNK   َ ِ ح أ ه UNK ه ْ UNK ح ي ْ أ UNK   َ ا ت   ف ق UNK ل   َ ف ت ل ع UNK َ َ ح ا ِ UNK ل ا أ ل UNK ك َ d UNK ل ف أ ي UNK ِ أ ِ ع UNK ب ل َ UNK ِ   َ d UNK ل ن ح أ   م ُ UNK ن ا UNK   َ ف   أ ي UNK d   ه ي UNK م   َ َ ُ UNK ر و   َ ة UNK ب   َ UNK ِ   َ d UNK ُ ل UNK d   َ ، ا ع ْ ب UNK د ك   UNK و ِ َ UNK   َ ف أ ي UNK م ا ه ف ُ UNK ب ر َ UNK ل َ ِ UNK ْ ف ر ر م']\n"
     ]
    }
   ],
   "source": [
    "print(len(tashkeel_sequences[1]))\n",
    "print(len(char_sequences_without_tashkeel[1]))\n",
    "\n",
    "print(char_tokenizer_with_tashkeel.sequences_to_texts([tashkeel_sequences[0]]))\n",
    "print(char_tokenizer_with_tashkeel.sequences_to_texts([char_sequences_without_tashkeel[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Tashkeel only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tashkeel_tokenizer = Tokenizer(char_level=True, oov_token='UNK')\n",
    "tashkeel_tokenizer.fit_on_texts(tashkeel_list)\n",
    "tashkeel_index = tashkeel_tokenizer.word_index\n",
    "tashkeel_list_sequences = tashkeel_tokenizer.texts_to_sequences(tashkeel_list)\n",
    "\n",
    "tashkeel_list_sequences_padded = pad_sequences(tashkeel_list_sequences, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 5, 6, 6, 2, 3, 5, 2, 3, 3, 3, 2, 2, 5, 3, 7, 6, 2, 3, 3, 6, 2, 2, 3, 5, 2, 3, 2, 3, 2, 2, 2, 7, 5, 3, 4, 12, 2, 2, 5, 6, 2, 3, 3, 3, 3, 2, 3, 5, 6, 6, 2, 4, 3, 5, 8, 2, 3, 5, 3, 4, 2, 2, 2, 3, 4, 5, 3, 2, 4, 2, 3, 5, 4, 2, 3, 4, 2, 8, 2, 4, 2, 5, 4, 5, 3, 2, 4, 2, 6, 6, 2, 3, 2, 3, 2, 2, 6, 4, 3, 2, 6, 6, 2, 6, 6, 2, 4, 5, 2, 2, 2, 9, 2, 4, 2, 3, 6, 2, 3, 10, 2, 3, 4, 5, 3, 2, 4, 2, 6, 5, 3, 8, 2, 4, 3, 3, 8, 2, 3, 3, 9, 2, 6, 7, 2, 8, 2, 2, 5, 6, 2, 3, 3, 3, 3, 2, 3, 5, 6, 2, 2, 5, 4, 2, 3, 2, 8, 2, 3, 5, 2, 4, 4, 5, 8, 2, 3, 3, 3, 7, 6, 6, 2, 6, 3, 2, 3, 6, 5, 4, 2, 2, 2, 12, 7, 2, 4, 2, 3, 4, 5, 3, 2, 4, 2, 2, 5, 6, 5, 3, 4, 2, 4, 2, 2, 3, 4, 2, 4, 2, 2, 2, 7, 3, 2, 3, 4, 2, 3, 2, 2, 12, 6, 2, 4, 2, 4, 2, 7, 3, 4, 2, 3, 3, 5, 4, 2, 3, 4, 3, 2, 3, 4, 5, 8, 2, 6, 3, 7, 11, 2, 3, 5, 6, 2, 3, 2, 4, 8, 2, 3, 3, 5, 3, 2, 4, 4, 2, 3, 7, 2, 2, 2, 7, 2, 4, 3, 2, 3, 2, 4, 11, 2, 4, 3, 2, 7, 4, 2, 3, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 4, 11, 2, 6, 3, 2, 3, 2, 2, 9, 5, 4, 2, 4, 2, 2, 3, 2, 2, 3, 4, 3, 2, 2, 2, 9, 5, 3, 2, 4, 3, 5, 4, 4, 2, 6, 4, 3, 2, 3, 3, 5, 2, 6, 5, 3, 3, 5], [3, 5, 6, 6, 2, 4, 3, 3, 4, 2, 3, 2, 2, 3, 3, 3, 7, 6, 2, 2, 3, 5, 2, 3, 5, 2, 2, 5, 3, 4, 7, 6, 2, 3, 5, 6, 6, 2, 3, 2, 2, 3, 7, 2, 3, 5, 2, 6, 3, 5, 3, 2, 3, 5, 4, 2, 2, 5, 3, 5, 4, 2, 3, 3, 5, 2, 3, 3, 5, 2, 2, 5, 3, 3, 3, 2, 3, 3, 2, 2, 3, 5, 3, 5, 2, 2, 3, 6, 2, 4, 3, 2, 8, 2, 3, 5, 2, 3, 5, 6, 2, 6, 2, 3, 2, 10, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 6, 2, 4, 5, 3, 2, 2, 5, 3, 5, 4, 2, 3, 5, 2, 3, 5, 6, 6, 2, 2, 5, 3, 4, 7, 6, 2, 3, 5, 2, 6, 5, 3, 3, 2, 2, 3, 6, 2, 3, 2, 11, 2, 3, 6, 5, 3, 6, 2, 4, 5, 2, 3, 5, 4, 4, 2, 2, 5, 2, 4, 2, 2, 3, 3, 2, 2, 3, 5, 2, 3, 5, 2, 3, 6, 5, 2, 4, 5, 2, 3, 2, 4, 2, 2, 3, 3, 2, 2, 4, 5, 2, 3, 3, 4, 2, 2, 3, 7, 3, 2, 2, 3, 2, 2, 3, 5, 6, 6, 2, 2, 2, 3, 4, 3, 2, 3, 6, 2, 2, 5, 3, 5, 4, 2, 3, 3, 5, 2, 3, 5, 2, 3, 6, 5, 2, 4, 5, 2, 3, 2, 4, 2, 2, 3, 3, 2, 2, 4, 5, 2, 3, 3, 4, 2, 2, 3, 5, 2, 3, 3, 3, 7, 5, 2, 3, 3, 6, 6, 2, 2, 5, 2, 3, 2, 3, 5, 2, 2, 5, 3, 3, 5, 2, 2, 2, 2, 3, 5, 6, 6, 2, 3, 6, 5, 3, 2, 2, 3, 2, 4, 3, 10, 2, 4, 5, 3, 2, 2, 2, 3, 5, 2, 3, 3, 2, 2, 3, 5, 2, 3, 2, 3, 5, 2, 3, 5, 6, 2, 3, 10, 2, 4, 5, 3, 2, 2, 5, 3, 4, 7, 4, 2, 3, 2, 5, 3, 5, 4, 2, 2, 2, 3, 3, 2, 2, 3, 6, 2, 6, 2, 3, 5, 2, 6, 5, 3, 2, 2, 3, 2, 4, 3, 10, 2, 4, 5, 2, 3, 5, 4, 2, 3, 3, 4, 4, 2, 4, 2, 2, 2, 2, 12, 2, 3, 3, 5, 4, 2, 3, 4, 5, 2, 3, 3, 2, 3, 3, 2], [4, 3, 7, 6, 2, 6, 5, 11, 2, 3, 3, 2, 2, 3, 5, 6, 2, 8, 2, 6, 5, 4, 2, 2, 3, 4, 3, 2, 3, 11, 2, 3, 2, 3, 2, 3, 5, 6, 6, 2, 3, 2, 4, 3, 10, 2, 4, 5, 3, 2, 2, 3, 5, 2, 3, 2, 4, 3, 10, 2, 2, 2, 3, 3, 2, 2, 3, 6, 2, 6, 2, 3, 5, 2, 6, 5, 3, 2, 2, 4, 5, 3, 5, 4, 2, 4, 5, 2, 3, 2, 3, 5, 4]]\n",
      "{'UNK': 1, '': 2, 'َ': 3, 'ِ': 4, 'ْ': 5, 'ُ': 6, 'd': 7, 'ٍ': 8, 'f': 9, 'ً': 10, 'ٌ': 11, 'e': 12, 'ّ': 13, 'c': 14, 'b': 15, 'a': 16}\n",
      "2592\n"
     ]
    }
   ],
   "source": [
    "print(tashkeel_list_sequences[0:3])\n",
    "print(tashkeel_index)\n",
    "print(len(tashkeel_list_sequences_padded[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickles/tashkeel_sequences.pkl', 'wb') as file:\n",
    "    pickle.dump(tashkeel_list_sequences_padded, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 5, 6, 6, 2, 3, 5, 2, 3, 3, 3, 2, 2, 5, 3, 7, 6, 2, 3, 3, 6, 2, 2, 3, 5, 2, 3, 2, 3, 2, 2, 2, 7, 5, 3, 4, 12, 2, 2, 5, 6, 2, 3, 3, 3, 3, 2, 3, 5, 6, 6, 2, 4, 3, 5, 8, 2, 3, 5, 3, 4, 2, 2, 2, 3, 4, 5, 3, 2, 4, 2, 3, 5, 4, 2, 3, 4, 2, 8, 2, 4, 2, 5, 4, 5, 3, 2, 4, 2, 6, 6, 2, 3, 2, 3, 2, 2, 6, 4, 3, 2, 6, 6, 2, 6, 6, 2, 4, 5, 2, 2, 2, 9, 2, 4, 2, 3, 6, 2, 3, 10, 2, 3, 4, 5, 3, 2, 4, 2, 6, 5, 3, 8, 2, 4, 3, 3, 8, 2, 3, 3, 9, 2, 6, 7, 2, 8, 2, 2, 5, 6, 2, 3, 3, 3, 3, 2, 3, 5, 6, 2, 2, 5, 4, 2, 3, 2, 8, 2, 3, 5, 2, 4, 4, 5, 8, 2, 3, 3, 3, 7, 6, 6, 2, 6, 3, 2, 3, 6, 5, 4, 2, 2, 2, 12, 7, 2, 4, 2, 3, 4, 5, 3, 2, 4, 2, 2, 5, 6, 5, 3, 4, 2, 4, 2, 2, 3, 4, 2, 4, 2, 2, 2, 7, 3, 2, 3, 4, 2, 3, 2, 2, 12, 6, 2, 4, 2, 4, 2, 7, 3, 4, 2, 3, 3, 5, 4, 2, 3, 4, 3, 2, 3, 4, 5, 8, 2, 6, 3, 7, 11, 2, 3, 5, 6, 2, 3, 2, 4, 8, 2, 3, 3, 5, 3, 2, 4, 4, 2, 3, 7, 2, 2, 2, 7, 2, 4, 3, 2, 3, 2, 4, 11, 2, 4, 3, 2, 7, 4, 2, 3, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 4, 11, 2, 6, 3, 2, 3, 2, 2, 9, 5, 4, 2, 4, 2, 2, 3, 2, 2, 3, 4, 3, 2, 2, 2, 9, 5, 3, 2, 4, 3, 5, 4, 4, 2, 6, 4, 3, 2, 3, 3, 5, 2, 6, 5, 3, 3, 5]\n"
     ]
    }
   ],
   "source": [
    "print(tashkeel_list_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

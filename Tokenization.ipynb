{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import pyarabic.araby as araby\n",
    "from pyarabic.araby import strip_tashkeel\n",
    "import qalsadi.lemmatizer \n",
    "import qalsadi.analex as qa\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "from farasa.pos import FarasaPOSTagger \n",
    "from farasa.ner import FarasaNamedEntityRecognizer \n",
    "from farasa.diacratizer import FarasaDiacritizer \n",
    "from farasa.segmenter import FarasaSegmenter \n",
    "from farasa.stemmer import FarasaStemmer\n",
    "\n",
    "import keras\n",
    "from  diacritization_evaluation import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['قوله', 'أو', 'قطع', 'الأول', 'يده', 'إلخ', 'قال', 'الزركشي', 'ابن', 'عرفة']\n",
      "قوله أو قطع الأول يده إلخ قال الزركشي\n",
      "قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "sentences = []\n",
    "sentences_with_tashkeel = []\n",
    "with open('./Dataset/training/train_words_stripped.txt', 'r', encoding='utf-8') as output_file:\n",
    "    for word in output_file:\n",
    "        words.append(word.strip())\n",
    "\n",
    "with open('./Dataset/training/train_stripped.txt', 'r', encoding='utf-8') as output_file:\n",
    "    for sentence in output_file:\n",
    "        sentences.append(sentence.strip())\n",
    "\n",
    "with open('./Dataset/training/train_cleaned.txt', 'r', encoding='utf-8') as output_file:\n",
    "    for sentence in output_file:\n",
    "        sentences_with_tashkeel.append(sentence.strip())\n",
    "\n",
    "print(words[0:10])\n",
    "print(sentences[0])\n",
    "print(sentences_with_tashkeel[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ\n"
     ]
    }
   ],
   "source": [
    "print(sentences_with_tashkeel[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a word-based tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenizer\n",
    "words_tokenizer = Tokenizer()\n",
    "\n",
    "# Fit the tokenizer on the list of words (treat each word as a separate \"sentence\")\n",
    "words_tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Get the word index\n",
    "word_index = words_tokenizer.word_index\n",
    "\n",
    "# Tokenize the words\n",
    "word_sequences = words_tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the tokenizer on a sample sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['قوله', 'أو', 'قطع', 'الأول', 'يده', 'إلخ', 'قال', 'الزركشي']\n",
      "8\n",
      "[[8, 4, 278, 92, 193, 48, 14, 910]]\n"
     ]
    }
   ],
   "source": [
    "# Create a sentence tokenizer\n",
    "print((sentences[0].split(\" \"))) #This way works and the one used below as well\n",
    "print(word_index[\"قوله\"])\n",
    "print(words_tokenizer.texts_to_sequences([sentences[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a character-based with tashkeel tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20, 2, 11, 5, 6, 8, 13, 8, 3, 18, 2, 11, 5, 3, 20, 2, 41, 2, 16, 2, 3, 7, 6, 5, 18, 2, 11, 17, 6, 8, 3, 10, 2, 22, 2, 13, 8, 3, 28, 6, 2, 34, 5, 3, 20, 2, 7, 6, 2, 3, 7, 6, 42, 17, 15, 5, 23, 2, 37, 4, 10, 43], [7, 14, 5, 12, 8, 3, 16, 2, 15, 2, 19, 2, 26, 2, 3, 20, 2, 11, 5, 6, 8, 13, 8, 3, 14, 4, 6, 2, 19, 5, 48, 31, 3, 10, 2, 20, 5, 21, 2, 38, 4, 10, 13, 3, 23, 2, 28, 4, 12, 5, 23, 2, 7, 15, 4, 3, 44, 2, 10, 5, 15, 4, 3, 24, 2, 22, 4, 10, 35, 31, 3, 14, 4, 7, 6, 5, 28, 4, 25, 5, 6, 2, 7, 9, 4, 3, 11, 8, 27, 8, 11, 14, 2, 3, 9, 2, 7, 3, 16, 8, 6, 4, 9, 2, 3, 11, 8, 27, 8, 11, 14, 8, 13, 8, 3, 9, 4, 12, 5, 3, 7, 6, 22, 36, 10, 12, 4, 3, 38, 2, 15, 8, 11, 15, 2, 26, 39, 3, 23, 2, 28, 4, 6, 5, 20, 2, 7, 46, 4, 3, 9, 8, 30, 5, 24, 2, 19, 31, 3, 14, 4, 20, 2, 29, 2, 15, 31, 3, 11, 2, 37, 2, 22, 36, 3, 42, 8, 12, 17, 7, 15, 31, 3, 7, 14, 5, 12, 8, 3, 16, 2, 15, 2, 19, 2, 26, 2, 3, 20, 2, 11, 5, 6, 8, 3, 7, 14, 5, 12, 4, 3, 37, 2, 7, 25, 31, 3, 18, 2, 11, 5, 3, 14, 4, 19, 4, 16, 5, 6, 31, 3, 10, 2, 21, 2, 38, 2, 9, 17, 12, 8, 13, 8, 3, 13, 8, 11, 2, 3, 23, 2, 6, 8, 14, 5, 25, 4, 3, 7, 6, 42, 43, 12, 17, 7, 15, 4, 3, 11, 2, 28, 4, 6, 5, 20, 2, 7, 46, 4, 3, 7, 6, 5, 9, 8, 30, 5, 24, 2, 19, 4, 3, 19, 4, 10, 3, 30, 2, 15, 4, 10, 24, 4, 3, 7, 6, 12, 17, 27, 2, 7, 25, 2, 26, 4, 3, 11, 2, 7, 6, 25, 43, 27, 8, 11, 22, 4, 3, 6, 4, 6, 30, 17, 12, 2, 9, 4, 3, 11, 2, 12, 2, 24, 5, 11, 4, 3, 29, 2, 6, 4, 23, 2, 3, 11, 2, 25, 4, 24, 5, 15, 31, 3, 9, 8, 24, 2, 9, 17, 22, 40, 3, 20, 2, 11, 5, 6, 8, 3, 9, 2, 7, 6, 4, 23, 31, 3, 11, 2, 18, 2, 30, 5, 24, 2, 7, 14, 4, 13, 4, 3, 18, 2, 12, 17, 3, 7, 6, 25, 17, 7, 24, 4, 15, 2, 3, 23, 2, 7, 19, 4, 15, 40, 3, 14, 4, 7, 2, 6, 6, 17, 13, 4, 3, 21, 2, 16, 2, 7, 6, 2, 33, 3, 20, 2, 7, 6, 2, 3, 9, 2, 7, 6, 4, 23, 40, 3, 13, 8, 11, 2, 3, 23, 2, 7, 6, 42, 36, 12, 5, 22, 4, 10, 20, 4, 3, 28, 29, 2, 7, 3, 16, 2, 9, 4, 6, 2, 3, 7, 6, 25, 36, 24, 5, 15, 2, 3, 14, 4, 12, 2, 19, 5, 25, 4, 13, 4, 3, 20, 8, 21, 4, 6, 2, 3, 11, 2, 6, 2, 9, 5, 3, 10, 8, 25, 5, 21, 2, 21, 2, 14, 5, 3, 45], [20, 2, 11, 5, 6, 8, 13, 8, 3, 6, 4, 16, 2, 22, 2, 9, 4, 3, 9, 2, 7, 3, 21, 2, 21, 2, 16, 2, 6, 17, 20, 8, 3, 28, 6, 2, 34, 5, 3, 18, 2, 10, 5, 3, 7, 6, 5, 11, 2, 30, 4, 10, 17, 26, 8, 3, 20, 2, 11, 5, 6, 8, 13, 8, 3, 9, 2, 7, 3, 9, 2, 15, 17, 3, 18, 2, 10, 5, 3, 20, 8, 14, 2, 10, 5, 6, 2, 3, 20, 2, 11, 5, 6, 4, 3, 7, 6, 5, 9, 2, 21, 5, 12, 4, 3, 6, 2, 44, 2, 21, 5, 3, 11, 2, 6, 2, 11, 5, 3, 7, 20, 5, 21, 2, 30, 2, 15, 2, 3, 16, 2, 6, 2, 33, 3, 18, 2, 11, 5, 30, 2, 10, 5, 21, 3, 6, 2, 13, 8, 3, 14, 4, 37, 2, 7, 26, 31, 3, 18, 2, 11, 5, 3, 18, 2, 16, 5, 41, 8, 11, 13, 8, 3, 37, 2, 7, 26, 39, 3, 11, 2, 6, 2, 7, 3, 44, 2, 12, 2, 9, 2, 3, 6, 2, 13, 8, 3, 16, 4, 12, 5, 22, 2, 3, 7, 6, 5, 9, 2, 11, 5, 21, 4, 3, 13, 2, 6, 5, 3, 21, 2, 14, 5, 41, 8, 6, 8, 3, 7, 6, 5, 11, 2, 30, 4, 10, 17, 26, 8, 3, 18, 2, 11, 5, 3, 10, 8, 37, 5, 21, 2, 15, 2, 33, 3, 6, 2, 13, 8, 3, 37, 2, 7, 26, 40, 3, 11, 2, 10, 8, 51, 5, 34, 2, 29, 8, 3, 9, 4, 12, 5, 3, 20, 2, 11, 5, 6, 4, 13, 4, 3, 7, 6, 5, 50, 21, 4, 10, 3, 23, 2, 9, 2, 7, 3, 6, 2, 11, 5, 3, 6, 2, 9, 5, 3, 10, 2, 20, 8, 6, 5, 3, 9, 4, 12, 5, 3, 9, 2, 7, 6, 4, 10, 3, 11, 2, 6, 2, 7, 3, 9, 4, 12, 5, 3, 44, 2, 12, 2, 9, 4, 10, 3, 18, 2, 12, 17, 13, 2, 7, 3, 6, 2, 7, 3, 21, 2, 14, 5, 41, 8, 6, 8, 3, 32, 3, 11, 2, 16, 4, 14, 2, 7, 15, 2, 26, 8, 3, 7, 6, 5, 23, 2, 12, 5, 42, 4, 3, 11, 2, 6, 2, 11, 5, 3, 6, 2, 9, 5, 3, 10, 2, 20, 8, 6, 5, 3, 9, 4, 12, 5, 3, 9, 2, 7, 6, 4, 10, 3, 11, 2, 6, 2, 7, 3, 9, 4, 12, 5, 3, 44, 2, 12, 2, 9, 4, 10, 3, 6, 2, 9, 5, 3, 10, 2, 21, 2, 16, 2, 10, 17, 12, 5, 3, 44, 2, 12, 2, 9, 8, 13, 8, 3, 28, 12, 5, 3, 23, 2, 7, 12, 2, 21, 5, 3, 7, 12, 5, 21, 2, 13, 2, 21, 5, 3, 7, 3, 13, 3, 25, 9, 3, 20, 2, 11, 5, 6, 8, 13, 8, 3, 19, 2, 10, 8, 16, 5, 41, 2, 33, 3, 11, 2, 7, 24, 4, 22, 2, 26, 39, 3, 9, 4, 12, 5, 13, 2, 7, 3, 28, 6, 2, 34, 5, 3, 23, 2, 9, 2, 7, 3, 6, 2, 11, 5, 3, 23, 2, 7, 12, 2, 21, 5, 3, 9, 2, 11, 5, 27, 8, 11, 22, 2, 26, 39, 3, 16, 4, 12, 5, 22, 2, 3, 7, 6, 5, 11, 2, 30, 4, 10, 17, 26, 4, 3, 11, 2, 7, 6, 5, 9, 2, 11, 5, 21, 4, 3, 32, 3, 11, 2, 6, 2, 7, 3, 10, 2, 27, 8, 11, 42, 8, 3, 18, 2, 12, 5, 3, 10, 8, 16, 5, 41, 2, 33, 3, 11, 2, 7, 24, 4, 22, 2, 26, 39, 3, 9, 4, 12, 5, 3, 44, 2, 10, 5, 15, 4, 3, 44, 2, 12, 2, 9, 4, 13, 4, 3, 19, 4, 10, 3, 7, 6, 30, 43, 11, 15, 2, 21, 2, 10, 5, 12, 4, 3, 11, 2, 28, 4, 12, 5, 3, 21, 2, 15, 2, 7, 38, 2, 10, 2, 7, 3, 49, 3, 6, 4, 18, 2, 12, 17, 13, 8, 3, 30, 8, 6, 5, 24, 40, 3, 16, 2, 6, 2, 33, 3, 9, 2, 27, 5, 13, 8, 11, 6, 31, 3, 9, 8, 44, 5, 12, 4, 10, 3, 11, 2, 12, 4, 13, 2, 7, 10, 2, 26, 40, 3, 20, 2, 7, 6, 2, 3, 16, 3, 37, 3, 20, 2, 11, 5, 6, 8, 13, 8, 3, 11, 2, 7, 24, 4, 22, 2, 26, 39, 3, 9, 4, 12, 5, 13, 2, 7, 3, 18, 2, 10, 5, 3, 23, 2, 7, 9, 4, 6, 2, 26, 39, 3, 32, 3, 11, 2, 6, 2, 7, 3, 10, 2, 27, 8, 11, 42, 8, 3, 18, 2, 12, 5, 3, 10, 8, 16, 5, 41, 2, 33, 3, 12, 4, 30, 5, 19, 2, 10, 5, 12, 4, 3, 9, 4, 12, 5, 3, 37, 2, 7, 21, 2, 10, 5, 12, 4, 3, 49, 3, 6, 4, 18, 2, 12, 17, 13, 8, 3, 6, 2, 7, 3, 10, 8, 25, 2, 9, 17, 33, 3, 37, 2, 7, 26, 39, 3, 11, 2, 20, 2, 11, 5, 6, 8, 13, 8, 3, 11, 2, 6, 2, 7, 3, 10, 2, 27, 8, 11, 42, 8, 3, 18, 2, 12, 5, 3, 10, 8, 16, 5, 41, 2, 33, 3, 11, 2, 7, 24, 4, 22, 2, 26, 39, 3, 9, 4, 12, 5, 3, 44, 2, 10, 5, 15, 4, 3, 44, 2, 12, 2, 9, 4, 13, 4, 3, 11, 2, 10, 2, 12, 5, 14, 2, 44, 4, 10, 3, 18, 2, 12, 5, 3, 10, 8, 20, 2, 7, 6, 2, 3, 9, 4, 35, 5, 6, 8, 3, 29, 2, 6, 4, 23, 2, 3, 19, 4, 10, 3, 7, 6, 5, 18, 2, 15, 4, 20, 17, 7, 46, 4, 3, 7, 3, 13, 3, 45], [11, 2, 24, 2, 10, 2, 11, 2, 7, 12, 40, 3, 44, 2, 10, 5, 15, 8, 3, 9, 2, 11, 5, 27, 8, 11, 22, 31, 3, 45], [19, 2, 7, 47, 4, 22, 2, 26, 40, 3, 20, 2, 7, 6, 2, 3, 14, 2, 16, 5, 38, 8, 13, 8, 9, 5, 3, 10, 8, 51, 5, 34, 2, 29, 8, 3, 9, 4, 12, 5, 3, 37, 2, 15, 5, 41, 4, 3, 21, 2, 9, 2, 7, 9, 4, 3, 7, 6, 5, 9, 4, 6, 5, 23, 4, 3, 16, 2, 22, 2, 9, 8, 3, 42, 2, 23, 2, 7, 26, 4, 3, 24, 8, 6, 4, 10, 36, 3, 7, 6, 5, 23, 2, 16, 5, 14, 2, 26, 4, 3, 11, 2, 7, 6, 5, 9, 2, 25, 2, 7, 27, 4, 22, 4, 3, 9, 4, 12, 5, 3, 20, 2, 12, 2, 7, 22, 4, 10, 6, 2, 3, 11, 2, 16, 2, 6, 2, 7, 47, 4, 20, 2, 3, 11, 2, 30, 2, 19, 2, 7, 47, 4, 24, 4, 3, 18, 2, 14, 5, 11, 2, 7, 14, 31, 3, 45]]\n",
      "{'UNK': 1, 'َ': 2, ' ': 3, 'ِ': 4, 'ْ': 5, 'ل': 6, 'ا': 7, 'ُ': 8, 'م': 9, 'ي': 10, 'و': 11, 'ن': 12, 'ه': 13, 'ب': 14, 'ر': 15, 'ع': 16, '٤': 17, 'أ': 18, 'ف': 19, 'ق': 20, 'ت': 21, 'د': 22, 'ك': 23, 'ح': 24, 'س': 25, 'ة': 26, 'ج': 27, 'إ': 28, 'ذ': 29, 'ص': 30, 'ٍ': 31, '،': 32, 'ى': 33, 'خ': 34, 'ث': 35, '٦': 36, 'ش': 37, 'ض': 38, 'ً': 39, 'ٌ': 40, 'ط': 41, 'ز': 42, '٥': 43, 'غ': 44, '.': 45, 'ء': 46, 'ئ': 47, 'ظ': 48, '؛': 49, 'آ': 50, 'ؤ': 51, 'ّ': 52, '٣': 53, '٢': 54, '١': 55, '؟': 56}\n"
     ]
    }
   ],
   "source": [
    "sentences_new = []\n",
    "with open('./Dataset/training/train_replace.txt', 'r', encoding='utf-8') as output_file:\n",
    "    for sentence in output_file:\n",
    "        sentences_new.append(sentence.strip())\n",
    "\n",
    "char_tokenizer_with_tashkeel = Tokenizer(char_level=True, oov_token='UNK')\n",
    "char_tokenizer_with_tashkeel.fit_on_texts(sentences_new)\n",
    "char_index_with_tashkeel = char_tokenizer_with_tashkeel.word_index\n",
    "char_sequences_with_tashkeel = char_tokenizer_with_tashkeel.texts_to_sequences(sentences_new)\n",
    "print(char_sequences_with_tashkeel[0:5])\n",
    "print(char_index_with_tashkeel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a character-based without tashkeel tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_tokenizer_without_tashkeel = Tokenizer(char_level=True)\n",
    "char_tokenizer_without_tashkeel.fit_on_texts(sentences)\n",
    "char_index_without_tashkeel = char_tokenizer_without_tashkeel.word_index\n",
    "char_sequences_without_tashkeel = char_tokenizer_without_tashkeel.texts_to_sequences(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_sequences_without_tashkeel = char_tokenizer_without_tashkeel.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14, 6, 2, 8, 1, 12, 6, 1, 14, 31, 11, 1, 3, 2, 12, 6, 2, 1, 5, 16, 8, 1, 22, 2, 27, 1, 14, 3, 2, 1, 3, 2, 32, 10, 17, 29, 5]]\n",
      "41\n",
      "{' ': 1, 'ل': 2, 'ا': 3, 'م': 4, 'ي': 5, 'و': 6, 'ن': 7, 'ه': 8, 'ب': 9, 'ر': 10, 'ع': 11, 'أ': 12, 'ف': 13, 'ق': 14, 'ت': 15, 'د': 16, 'ك': 17, 'ح': 18, 'س': 19, 'ة': 20, 'ج': 21, 'إ': 22, 'ذ': 23, 'ص': 24, '،': 25, 'ى': 26, 'خ': 27, 'ث': 28, 'ش': 29, 'ض': 30, 'ط': 31, 'ز': 32, 'غ': 33, '.': 34, 'ء': 35, 'ئ': 36, 'ظ': 37, '؛': 38, 'آ': 39, 'ؤ': 40, '؟': 41}\n",
      "{'UNK': 1, 'َ': 2, ' ': 3, 'ِ': 4, 'ْ': 5, 'ل': 6, 'ا': 7, 'ُ': 8, 'م': 9, 'ي': 10, 'و': 11, 'ن': 12, 'ه': 13, 'ب': 14, 'ر': 15, 'ع': 16, '٤': 17, 'أ': 18, 'ف': 19, 'ق': 20, 'ت': 21, 'د': 22, 'ك': 23, 'ح': 24, 'س': 25, 'ة': 26, 'ج': 27, 'إ': 28, 'ذ': 29, 'ص': 30, 'ٍ': 31, '،': 32, 'ى': 33, 'خ': 34, 'ث': 35, '٦': 36, 'ش': 37, 'ض': 38, 'ً': 39, 'ٌ': 40, 'ط': 41, 'ز': 42, '٥': 43, 'غ': 44, '.': 45, 'ء': 46, 'ئ': 47, 'ظ': 48, '؛': 49, 'آ': 50, 'ؤ': 51, 'ّ': 52, '٣': 53, '٢': 54, '١': 55, '؟': 56}\n"
     ]
    }
   ],
   "source": [
    "print(char_sequences_without_tashkeel[0:1])\n",
    "print(len(char_index_without_tashkeel.keys()))\n",
    "print(char_index_without_tashkeel)\n",
    "print(char_index_with_tashkeel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding tests for the character-based tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UNK': 1, 'َ': 2, ' ': 3, 'ِ': 4, 'ْ': 5, 'ل': 6, 'ا': 7, 'ُ': 8, 'م': 9, 'ي': 10, 'و': 11, 'ن': 12, 'ه': 13, 'ب': 14, 'ر': 15, 'ع': 16, '٤': 17, 'أ': 18, 'ف': 19, 'ق': 20, 'ت': 21, 'د': 22, 'ك': 23, 'ح': 24, 'س': 25, 'ة': 26, 'ج': 27, 'إ': 28, 'ذ': 29, 'ص': 30, 'ٍ': 31, '،': 32, 'ى': 33, 'خ': 34, 'ث': 35, '٦': 36, 'ش': 37, 'ض': 38, 'ً': 39, 'ٌ': 40, 'ط': 41, 'ز': 42, '٥': 43, 'غ': 44, '.': 45, 'ء': 46, 'ئ': 47, 'ظ': 48, '؛': 49, 'آ': 50, 'ؤ': 51, 'ّ': 52, '٣': 53, '٢': 54, '١': 55, '؟': 56}\n",
      "[[11, 2, 20, 2, 7, 6, 2], [37, 2, 10, 5, 34, 8], [7, 6, 5, 28, 4, 25, 5, 6, 2, 7, 9, 4], [18, 2, 10, 5, 38, 39, 7], [11, 2, 20, 2, 22, 5], [25, 8, 47, 4, 6, 5, 21], [16, 2, 12, 5], [12, 2, 48, 2, 15, 4], [13, 2, 29, 4, 13, 4], [7, 6, 5, 9, 2, 25, 5, 18, 2, 6, 2, 26, 4], [11, 2, 13, 8, 11, 2], [15, 2, 27, 8, 6, 40], [21, 2, 16, 2, 15, 17, 38, 2], [6, 4, 7, 9, 5, 15, 2, 18, 2, 26, 4], [44, 2, 10, 5, 15, 4, 13, 4], [19, 2, 42, 2, 12, 2, 33], [14, 4, 13, 2, 7], [35, 8, 9, 17], [21, 2, 7, 14, 2], [9, 4, 12, 5], [29, 2, 6, 4, 23, 2], [32], [11, 2, 25, 2, 18, 2, 6, 2, 13, 8], [42, 2, 11, 5, 27, 8, 13, 2, 7], [16, 2, 12, 5], [29, 2, 6, 4, 23, 2], [19, 2, 18, 2, 12, 5, 23, 2, 15, 2], [19, 2, 41, 2, 6, 2, 14, 2], [7, 25, 5, 21, 4, 24, 5, 6, 2, 7, 19, 2, 13, 8], [32], [19, 2, 28, 4, 12, 5], [24, 2, 6, 2, 19, 2], [16, 2, 6, 2, 33], [12, 2, 19, 5, 10, 4]]\n"
     ]
    }
   ],
   "source": [
    "print(char_index_with_tashkeel)\n",
    "print(char_tokenizer_with_tashkeel.texts_to_sequences(sentences_new[12].split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: مرحبا كيف حالك\n",
      "Tokenized Sequence: [9, 15, 24, 14, 7, 3, 23, 10, 19, 3, 24, 7, 6, 23]\n",
      "\n",
      "Original Text: السلام عليكم\n",
      "Tokenized Sequence: [7, 6, 25, 6, 7, 9, 3, 16, 6, 10, 23, 9]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# New text data for testing\n",
    "new_texts = [\"مرحبا كيف حالك\", \"السلام عليكم\"]\n",
    "\n",
    "# Tokenize the new text data at the character level\n",
    "sequences_new = char_tokenizer_with_tashkeel.texts_to_sequences(new_texts)\n",
    "\n",
    "# Print the results\n",
    "for text, sequence in zip(new_texts, sequences_new):\n",
    "    print(f\"Original Text: {text}\")\n",
    "    print(f\"Tokenized Sequence: {sequence}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addding Padding to the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming word_sequences and char_sequences are the output of the tokenizers\n",
    "word_sequences = words_tokenizer.texts_to_sequences(sentences)\n",
    "char_sequences = char_tokenizer_with_tashkeel.texts_to_sequences(sentences_new)\n",
    "\n",
    "# Add padding\n",
    "word_sequences_padded = pad_sequences(word_sequences, padding='post')\n",
    "char_sequences_with_tashkeel_padded = pad_sequences(char_sequences, padding='post')\n",
    "char_sequences_without_tashkeel_padded = pad_sequences(char_sequences_without_tashkeel, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n",
      "11760\n"
     ]
    }
   ],
   "source": [
    "print(len(char_sequences[5]))\n",
    "print(len(char_sequences_with_tashkeel_padded[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenized sequences\n",
    "with open('./pickles/word_sequences.pkl', 'wb') as file:\n",
    "    pickle.dump(word_sequences_padded, file)\n",
    "\n",
    "with open('./pickles/char_sequences_with_tashkeel.pkl', 'wb') as file:\n",
    "    pickle.dump(char_sequences_with_tashkeel_padded, file)\n",
    "\n",
    "with open('./pickles/char_sequences_without_tashkeel.pkl', 'wb') as file:\n",
    "    pickle.dump(char_sequences_without_tashkeel_padded, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences_with_tashkeel\n",
    "tashkeel_list = []\n",
    "for sentence in sentences_with_tashkeel:\n",
    "    text, txt_list, harakat_list = util.extract_haraqat(sentence)   \n",
    "    for i in range(len(harakat_list)):\n",
    "        if len(harakat_list[i]) == 2:\n",
    "            if '\\u0651\\u064B' in harakat_list[i]:\n",
    "                harakat_list[i] = '١'\n",
    "            if '\\u0651\\u064C' in harakat_list[i]:\n",
    "                harakat_list[i] = '٢'\n",
    "            if '\\u0651\\u064D' in harakat_list[i]:\n",
    "                harakat_list[i] = '٣'\n",
    "            if '\\u0651\\u064E' in harakat_list[i]:\n",
    "                harakat_list[i] = '٤'\n",
    "            if '\\u0651\\u064F' in harakat_list[i]:\n",
    "                harakat_list[i] = '٥'\n",
    "            if '\\u0651\\u0650' in harakat_list[i]:\n",
    "                harakat_list[i] = '٦'\n",
    "\n",
    "    tashkeel_list.append(harakat_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tashkeel_sequences = char_tokenizer_with_tashkeel.texts_to_sequences(tashkeel_list)\n",
    "tashkeel_sequences_padded = pad_sequences(tashkeel_sequences, padding='post')\n",
    "with open('tashkeel_sequences.pkl', 'wb') as file:\n",
    "    pickle.dump(tashkeel_sequences_padded, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['َ', 'ْ', 'ُ', 'ُ', '', 'َ', 'ْ', '', 'َ', 'َ', 'َ', '', '', 'ْ', 'َ', '٤', 'ُ', '', 'َ', 'َ', 'ُ', '', '', 'َ', 'ْ', '', 'َ', '', 'َ', '', '', '', '٤', 'ْ', 'َ', 'ِ', '٥']\n"
     ]
    }
   ],
   "source": [
    "print(tashkeel_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7203\n",
      "7203\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(tashkeel_sequences_padded[0]))\n",
    "print(len(tashkeel_sequences_padded[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324\n",
      "324\n",
      "['َ ْ ُ ُ UNK َ ْ UNK َ َ َ UNK UNK ْ َ ٤ ُ UNK َ َ ُ UNK UNK َ ْ UNK َ UNK َ UNK UNK UNK ٤ ْ َ ِ ٥']\n",
      "['ب ل َ ُ UNK ن ل UNK ب ٍ و UNK   َ ن ل َ UNK ْ ع ُ UNK د َ ج UNK ب   َ UNK   َ ، ي ٤ ذ ْ']\n"
     ]
    }
   ],
   "source": [
    "print(len(tashkeel_sequences[1]))\n",
    "print(len(char_sequences_without_tashkeel[1]))\n",
    "\n",
    "print(char_tokenizer_with_tashkeel.sequences_to_texts([tashkeel_sequences[0]]))\n",
    "print(char_tokenizer_with_tashkeel.sequences_to_texts([char_sequences_without_tashkeel[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Tashkeel only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tashkeel_tokenizer = Tokenizer(char_level=True, oov_token='UNK')\n",
    "tashkeel_tokenizer.fit_on_texts(tashkeel_list)\n",
    "tashkeel_index = tashkeel_tokenizer.word_index\n",
    "tashkeel_list_sequences = tashkeel_tokenizer.texts_to_sequences(tashkeel_list)\n",
    "\n",
    "tashkeel_list_sequences_padded = pad_sequences(tashkeel_list_sequences, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 5, 6, 6, 2, 3, 5, 2, 3, 3, 3, 2, 2, 5, 3, 7, 6, 2, 3, 3, 6, 2, 2, 3, 5, 2, 3, 2, 3, 2, 2, 2, 7, 5, 3, 4, 12], [2, 5, 6, 2, 3, 3, 3, 3, 2, 3, 5, 6, 6, 2, 4, 3, 5, 8, 2, 3, 5, 3, 4, 2, 2, 2, 3, 4, 5, 3, 2, 4, 2, 3, 5, 4, 2, 3, 4, 2, 8, 2, 4, 2, 5, 4, 5, 3, 2, 4, 2, 6, 6, 2, 3, 2, 3, 2, 2, 6, 4, 3, 2, 6, 6, 2, 6, 6, 2, 4, 5, 2, 2, 2, 9, 2, 4, 2, 3, 6, 2, 3, 10, 2, 3, 4, 5, 3, 2, 4, 2, 6, 5, 3, 8, 2, 4, 3, 3, 8, 2, 3, 3, 9, 2, 6, 7, 2, 8, 2, 2, 5, 6, 2, 3, 3, 3, 3, 2, 3, 5, 6, 2, 2, 5, 4, 2, 3, 2, 8, 2, 3, 5, 2, 4, 4, 5, 8, 2, 3, 3, 3, 7, 6, 6, 2, 6, 3, 2, 3, 6, 5, 4, 2, 2, 2, 12, 7, 2, 4, 2, 3, 4, 5, 3, 2, 4, 2, 2, 5, 6, 5, 3, 4, 2, 4, 2, 2, 3, 4, 2, 4, 2, 2, 2, 7, 3, 2, 3, 4, 2, 3, 2, 2, 12, 6, 2, 4, 2, 4, 2, 7, 3, 4, 2, 3, 3, 5, 4, 2, 3, 4, 3, 2, 3, 4, 5, 8, 2, 6, 3, 7, 11, 2, 3, 5, 6, 2, 3, 2, 4, 8, 2, 3, 3, 5, 3, 2, 4, 4, 2, 3, 7, 2, 2, 2, 7, 2, 4, 3, 2, 3, 2, 4, 11, 2, 4, 3, 2, 7, 4, 2, 3, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 4, 11, 2, 6, 3, 2, 3, 2, 2, 9, 5, 4, 2, 4, 2, 2, 3, 2, 2, 3, 4, 3, 2, 2, 2, 9, 5, 3, 2, 4, 3, 5, 4, 4, 2, 6, 4, 3, 2, 3, 3, 5, 2, 6, 5, 3, 3, 5, 2, 2], [3, 5, 6, 6, 2, 4, 3, 3, 4, 2, 3, 2, 2, 3, 3, 3, 7, 6, 2, 2, 3, 5, 2, 3, 5, 2, 2, 5, 3, 4, 7, 6, 2, 3, 5, 6, 6, 2, 3, 2, 2, 3, 7, 2, 3, 5, 2, 6, 3, 5, 3, 2, 3, 5, 4, 2, 2, 5, 3, 5, 4, 2, 3, 3, 5, 2, 3, 3, 5, 2, 2, 5, 3, 3, 3, 2, 3, 3, 2, 2, 3, 5, 3, 5, 2, 2, 3, 6, 2, 4, 3, 2, 8, 2, 3, 5, 2, 3, 5, 6, 2, 6, 2, 3, 2, 10, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 6, 2, 4, 5, 3, 2, 2, 5, 3, 5, 4, 2, 3, 5, 2, 3, 5, 6, 6, 2, 2, 5, 3, 4, 7, 6, 2, 3, 5, 2, 6, 5, 3, 3, 2, 2, 3, 6, 2, 3, 2, 11, 2, 3, 6, 5, 3, 6, 2, 4, 5, 2, 3, 5, 4, 4, 2, 2, 5, 2, 4, 2, 2, 3, 3, 2, 2, 3, 5, 2, 3, 5, 2, 3, 6, 5, 2, 4, 5, 2, 3, 2, 4, 2, 2, 3, 3, 2, 2, 4, 5, 2, 3, 3, 4, 2, 2, 3, 7, 3, 2, 2, 3, 2, 2, 3, 5, 6, 6, 2, 2, 2, 3, 4, 3, 2, 3, 6, 2, 2, 5, 3, 5, 4, 2, 3, 3, 5, 2, 3, 5, 2, 3, 6, 5, 2, 4, 5, 2, 3, 2, 4, 2, 2, 3, 3, 2, 2, 4, 5, 2, 3, 3, 4, 2, 2, 3, 5, 2, 3, 3, 3, 7, 5, 2, 3, 3, 6, 6, 2, 2, 5, 2, 3, 2, 3, 5, 2, 2, 5, 3, 3, 5, 2, 2, 2, 2, 2, 2, 2, 2, 3, 5, 6, 6, 2, 3, 6, 5, 3, 2, 2, 3, 2, 4, 3, 10, 2, 4, 5, 3, 2, 2, 2, 3, 5, 2, 3, 3, 2, 2, 3, 5, 2, 3, 2, 3, 5, 2, 3, 5, 6, 2, 3, 10, 2, 4, 5, 3, 2, 2, 5, 3, 4, 7, 4, 2, 3, 2, 5, 3, 5, 4, 2, 2, 2, 3, 3, 2, 2, 3, 6, 2, 6, 2, 3, 5, 2, 6, 5, 3, 2, 2, 3, 2, 4, 3, 10, 2, 4, 5, 2, 3, 5, 4, 2, 3, 3, 4, 4, 2, 4, 2, 2, 2, 2, 12, 2, 3, 3, 5, 4, 2, 3, 4, 5, 2, 3, 3, 2, 3, 3, 2, 2, 2, 2, 4, 3, 7, 6, 2, 6, 5, 11, 2, 3, 3, 2, 2, 3, 5, 6, 2, 8, 2, 6, 5, 4, 2, 2, 3, 4, 3, 2, 3, 11, 2, 3, 2, 3, 2, 2, 2, 2, 2, 3, 5, 6, 6, 2, 3, 2, 4, 3, 10, 2, 4, 5, 3, 2, 2, 3, 5, 2, 3, 2, 4, 3, 10, 2, 2, 2, 3, 3, 2, 2, 3, 6, 2, 6, 2, 3, 5, 2, 6, 5, 3, 2, 2, 4, 5, 3, 5, 4, 2, 4, 5, 2, 3, 2, 3, 5, 4, 2, 2, 2, 4, 3, 7, 6, 2, 3, 2, 2, 6, 3, 7, 2, 2, 3, 2, 10, 2, 3, 3, 5, 6, 6, 2, 3, 3, 2, 2, 3, 6, 2, 6, 2, 3, 5, 2, 6, 5, 3, 2, 2, 3, 2, 4, 3, 10, 2, 4, 5, 2, 3, 5, 4, 2, 3, 3, 4, 4, 2, 3, 3, 5, 3, 4, 2, 2, 3, 5, 2, 6, 3, 2, 3, 2, 4, 5, 6, 2, 3, 4, 3, 2, 4, 2, 2, 2, 5, 3, 4, 7, 2, 4, 2, 2, 2, 2, 2, 2]]\n",
      "{'UNK': 1, '': 2, 'َ': 3, 'ِ': 4, 'ْ': 5, 'ُ': 6, '٤': 7, 'ٍ': 8, '٦': 9, 'ً': 10, 'ٌ': 11, '٥': 12, 'ّ': 13, '٣': 14, '٢': 15, '١': 16}\n",
      "7203\n"
     ]
    }
   ],
   "source": [
    "print(tashkeel_list_sequences[0:3])\n",
    "print(tashkeel_index)\n",
    "print(len(tashkeel_list_sequences_padded[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickles/tashkeel_sequences.pkl', 'wb') as file:\n",
    "    pickle.dump(tashkeel_list_sequences_padded, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 5, 6, 6, 2, 3, 5, 2, 3, 3, 3, 2, 2, 5, 3, 7, 6, 2, 3, 3, 6, 2, 2, 3, 5, 2, 3, 2, 3, 2, 2, 2, 7, 5, 3, 4, 12]\n"
     ]
    }
   ],
   "source": [
    "print(tashkeel_list_sequences[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

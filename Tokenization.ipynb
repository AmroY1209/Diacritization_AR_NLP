{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import pyarabic.araby as araby\n",
    "from pyarabic.araby import strip_tashkeel\n",
    "import qalsadi.lemmatizer \n",
    "import qalsadi.analex as qa\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "from farasa.pos import FarasaPOSTagger \n",
    "from farasa.ner import FarasaNamedEntityRecognizer \n",
    "from farasa.diacratizer import FarasaDiacritizer \n",
    "from farasa.segmenter import FarasaSegmenter \n",
    "from farasa.stemmer import FarasaStemmer\n",
    "\n",
    "import keras\n",
    "from  diacritization_evaluation import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting diacritization-evaluation\n",
      "  Downloading diacritization_evaluation-0.5-py3-none-any.whl (7.2 kB)\n",
      "Installing collected packages: diacritization-evaluation\n",
      "Successfully installed diacritization-evaluation-0.5\n"
     ]
    }
   ],
   "source": [
    "!pip install diacritization-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['قوله', 'أو', 'قطع', 'الأول', 'يده', 'إلخ', 'قال', 'الزركشي', 'ابن', 'عرفة']\n",
      "قوله أو قطع الأول يده إلخ قال الزركشي ابن عرفة قوله بلفظ يقتضيه كإنكار غير حديث بالإسلام وجوب ما علم وجوبه من الدين ضرورة كإلقاء مصحف بقذر وشد زنار ابن عرفة قول ابن شاس أو بفعل يتضمنه هو كلبس الزنار وإلقاء المصحف في صريح النجاسة والسجود للصنم ونحو ذلك وسحر محمد قول مالك وأصحابه أن الساحر كافر بالله تعالى قال مالك هو كالزنديق إذا عمل السحر بنفسه قتل ولم يستتب\n",
      "قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ ابْنُ عَرَفَةَ قَوْلُهُ بِلَفْظٍ يَقْتَضِيه كَإِنْكَارِ غَيْرِ حَدِيثٍ بِالْإِسْلَامِ وُجُوبَ مَا عُلِمَ وُجُوبُهُ مِنْ الدِّينِ ضَرُورَةً كَإِلْقَاءِ مُصْحَفٍ بِقَذَرٍ وَشَدِّ زُنَّارٍ ابْنُ عَرَفَةَ قَوْلُ ابْنِ شَاسٍ أَوْ بِفِعْلٍ يَتَضَمَّنُهُ هُوَ كَلُبْسِ الزُّنَّارِ وَإِلْقَاءِ الْمُصْحَفِ فِي صَرِيحِ النَّجَاسَةِ وَالسُّجُودِ لِلصَّنَمِ وَنَحْوِ ذَلِكَ وَسِحْرٍ مُحَمَّدٌ قَوْلُ مَالِكٍ وَأَصْحَابِهِ أَنَّ السَّاحِرَ كَافِرٌ بِاَللَّهِ تَعَالَى قَالَ مَالِكٌ هُوَ كَالزِّنْدِيقِ إذَا عَمِلَ السِّحْرَ بِنَفْسِهِ قُتِلَ وَلَمْ يُسْتَتَبْ\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "sentences = []\n",
    "sentences_with_tashkeel = []\n",
    "with open('./Dataset/WordsWithoutTashkeel.txt', 'r', encoding='utf-8') as output_file:\n",
    "    for word in output_file:\n",
    "        words.append(word.strip())\n",
    "\n",
    "with open('./Dataset/SentencesWithoutTashkeel.txt', 'r', encoding='utf-8') as output_file:\n",
    "    for sentence in output_file:\n",
    "        sentences.append(sentence.strip())\n",
    "\n",
    "with open('./Dataset/sentences.txt', 'r', encoding='utf-8') as output_file:\n",
    "    for sentence in output_file:\n",
    "        sentences_with_tashkeel.append(sentence.strip())\n",
    "\n",
    "print(words[0:10])\n",
    "print(sentences[0])\n",
    "print(sentences_with_tashkeel[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ ابْنُ عَرَفَةَ قَوْلُهُ بِلَفْظٍ يَقْتَضِيه كَإِنْكَارِ غَيْرِ حَدِيثٍ بِالْإِسْلَامِ وُجُوبَ مَا عُلِمَ وُجُوبُهُ مِنْ الدِّينِ ضَرُورَةً كَإِلْقَاءِ مُصْحَفٍ بِقَذَرٍ وَشَدِّ زُنَّارٍ ابْنُ عَرَفَةَ قَوْلُ ابْنِ شَاسٍ أَوْ بِفِعْلٍ يَتَضَمَّنُهُ هُوَ كَلُبْسِ الزُّنَّارِ وَإِلْقَاءِ الْمُصْحَفِ فِي صَرِيحِ النَّجَاسَةِ وَالسُّجُودِ لِلصَّنَمِ وَنَحْوِ ذَلِكَ وَسِحْرٍ مُحَمَّدٌ قَوْلُ مَالِكٍ وَأَصْحَابِهِ أَنَّ السَّاحِرَ كَافِرٌ بِاَللَّهِ تَعَالَى قَالَ مَالِكٌ هُوَ كَالزِّنْدِيقِ إذَا عَمِلَ السِّحْرَ بِنَفْسِهِ قُتِلَ وَلَمْ يُسْتَتَبْ\n"
     ]
    }
   ],
   "source": [
    "print(sentences_with_tashkeel[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a word-based tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenizer\n",
    "words_tokenizer = Tokenizer()\n",
    "\n",
    "# Fit the tokenizer on the list of words (treat each word as a separate \"sentence\")\n",
    "words_tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Get the word index\n",
    "word_index = words_tokenizer.word_index\n",
    "\n",
    "# Tokenize the words\n",
    "word_sequences = words_tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the tokenizer on a sample sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['قوله', 'أو', 'قطع', 'الأول', 'يده', 'إلخ', 'قال', 'الزركشي', 'ابن', 'عرفة', 'قوله', 'بلفظ', 'يقتضيه', 'كإنكار', 'غير', 'حديث', 'بالإسلام', 'وجوب', 'ما', 'علم', 'وجوبه', 'من', 'الدين', 'ضرورة', 'كإلقاء', 'مصحف', 'بقذر', 'وشد', 'زنار', 'ابن', 'عرفة', 'قول', 'ابن', 'شاس', 'أو', 'بفعل', 'يتضمنه', 'هو', 'كلبس', 'الزنار', 'وإلقاء', 'المصحف', 'في', 'صريح', 'النجاسة', 'والسجود', 'للصنم', 'ونحو', 'ذلك', 'وسحر', 'محمد', 'قول', 'مالك', 'وأصحابه', 'أن', 'الساحر', 'كافر', 'بالله', 'تعالى', 'قال', 'مالك', 'هو', 'كالزنديق', 'إذا', 'عمل', 'السحر', 'بنفسه', 'قتل', 'ولم', 'يستتب']\n",
      "8\n",
      "[[8, 4, 273, 91, 189, 47, 14, 901, 37, 458, 8, 1184, 3641, 41299, 39, 249, 4154, 475, 12, 163, 1738, 3, 158, 1175, 27872, 6099, 41300, 5827, 41301, 37, 458, 63, 37, 2652, 4, 1362, 24279, 43, 17883, 57670, 41302, 2961, 2, 1430, 2148, 9577, 57671, 1189, 18, 21681, 86, 63, 94, 3359, 6, 16422, 1190, 1185, 108, 14, 94, 43, 27873, 28, 520, 8288, 415, 264, 51, 57672]]\n"
     ]
    }
   ],
   "source": [
    "# Create a sentence tokenizer\n",
    "print((sentences[0].split(\" \"))) #This way works and the one used below as well\n",
    "print(word_index[\"قوله\"])\n",
    "print(words_tokenizer.texts_to_sequences([sentences[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a character-based tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19, 10, 5, 12, 2, 17, 10, 2, 19, 39, 16, 2, 6, 5, 17, 10, 5, 2, 9, 21, 12, 2, 27, 5, 33, 2, 19, 6, 5, 2, 6, 5, 40, 15, 22, 36, 9, 2, 6, 14, 11, 2, 16, 15, 18, 25, 2, 19, 10, 5, 12, 2, 14, 5, 18, 44, 2, 9, 19, 20, 38, 9, 12, 2, 22, 27, 11, 22, 6, 15, 2, 41, 9, 15, 2, 23, 21, 9, 34, 2, 14, 6, 5, 27, 24, 5, 6, 8, 2, 10, 26, 10, 14, 2, 8, 6, 2, 16, 5, 8, 2, 10, 26, 10, 14, 12, 2, 8, 11, 2, 6, 5, 21, 9, 11, 2, 38, 15, 10, 15, 25, 2, 22, 27, 5, 19, 6, 42, 2, 8, 30, 23, 18, 2, 14, 19, 29, 15, 2, 10, 36, 21, 2, 40, 11, 6, 15, 2, 6, 14, 11, 2, 16, 15, 18, 25, 2, 19, 10, 5, 2, 6, 14, 11, 2, 36, 6, 24, 2, 17, 10, 2, 14, 18, 16, 5, 2, 9, 20, 38, 8, 11, 12, 2, 12, 10, 2, 22, 5, 14, 24, 2, 6, 5, 40, 11, 6, 15, 2, 10, 27, 5, 19, 6, 42, 2, 6, 5, 8, 30, 23, 18, 2, 18, 9, 2, 30, 15, 9, 23, 2, 6, 5, 11, 26, 6, 24, 25, 2, 10, 6, 5, 24, 26, 10, 21, 2, 5, 5, 30, 11, 8, 2, 10, 11, 23, 10, 2, 29, 5, 22, 2, 10, 24, 23, 15, 2, 8, 23, 8, 21, 2, 19, 10, 5, 2, 8, 6, 5, 22, 2, 10, 17, 30, 23, 6, 14, 12, 2, 17, 11, 2, 6, 5, 24, 6, 23, 15, 2, 22, 6, 18, 15, 2, 14, 6, 5, 5, 12, 2, 20, 16, 6, 5, 32, 2, 19, 6, 5, 2, 8, 6, 5, 22, 2, 12, 10, 2, 22, 6, 5, 40, 11, 21, 9, 19, 2, 27, 29, 6, 2, 16, 8, 5, 2, 6, 5, 24, 23, 15, 2, 14, 11, 18, 24, 12, 2, 19, 20, 5, 2, 10, 5, 8, 2, 9, 24, 20, 20, 14], [19, 10, 5, 12, 2, 5, 16, 21, 8, 2, 8, 6, 2, 20, 20, 16, 5, 19, 2, 27, 5, 33, 2, 17, 9, 2, 6, 5, 10, 30, 9, 25, 2, 19, 10, 5, 12, 2, 8, 6, 2, 8, 15, 2, 17, 9, 2, 19, 14, 9, 5, 2, 19, 10, 5, 2, 6, 5, 8, 20, 11, 2, 5, 41, 20, 2, 10, 5, 10, 2, 6, 19, 20, 30, 15, 2, 16, 5, 32, 2, 17, 10, 30, 9, 20, 2, 5, 12, 2, 14, 36, 6, 25, 2, 17, 10, 2, 17, 16, 39, 10, 12, 2, 36, 6, 25, 2, 10, 5, 6, 2, 41, 11, 8, 2, 5, 12, 2, 16, 11, 21, 2, 6, 5, 8, 10, 20, 2, 12, 5, 2, 20, 14, 39, 5, 2, 6, 5, 10, 30, 9, 25, 2, 17, 10, 2, 9, 36, 20, 15, 32, 2, 5, 12, 2, 36, 6, 25, 2, 10, 9, 46, 33, 29, 2, 8, 11, 2, 19, 10, 5, 12, 2, 6, 5, 45, 20, 9, 2, 22, 8, 6, 2, 5, 10, 2, 5, 8, 2, 9, 19, 5, 2, 8, 11, 2, 8, 6, 5, 9, 2, 10, 5, 6, 2, 8, 11, 2, 41, 11, 8, 9, 2, 17, 11, 12, 6, 2, 5, 6, 2, 20, 14, 39, 5, 2, 31, 2, 10, 16, 14, 6, 15, 25, 2, 6, 5, 22, 11, 40, 2, 10, 5, 10, 2, 5, 8, 2, 9, 19, 5, 2, 8, 11, 2, 8, 6, 5, 9, 2, 10, 5, 6, 2, 8, 11, 2, 41, 11, 8, 9, 2, 5, 8, 2, 9, 20, 16, 9, 11, 2, 41, 11, 8, 12, 2, 27, 11, 2, 22, 6, 11, 20, 2, 6, 11, 20, 12, 20, 2, 24, 8, 2, 19, 10, 5, 12, 2, 18, 9, 16, 39, 32, 2, 10, 6, 23, 21, 25, 2, 8, 11, 12, 6, 2, 27, 5, 33, 2, 22, 8, 6, 2, 5, 10, 2, 22, 6, 11, 20, 2, 8, 10, 26, 10, 21, 25, 2, 16, 11, 21, 2, 6, 5, 10, 30, 9, 25, 2, 10, 6, 5, 8, 10, 20, 2, 31, 2, 10, 5, 6, 2, 9, 26, 10, 40, 2, 17, 11, 2, 9, 16, 39, 32, 2, 10, 6, 23, 21, 25, 2, 8, 11, 2, 41, 9, 15, 2, 41, 11, 8, 12, 2, 18, 9, 2, 6, 5, 30, 10, 15, 20, 9, 11, 2, 10, 27, 11, 2, 20, 15, 6, 38, 9, 6], [5, 17, 11, 12, 2, 30, 5, 23, 2, 16, 5, 32, 2, 8, 26, 12, 10, 5, 2, 8, 41, 11, 9, 2, 10, 11, 12, 6, 9, 25, 2, 19, 6, 5, 2, 19, 10, 5, 12, 2, 10, 6, 23, 21, 25, 2, 8, 11, 12, 6, 2, 17, 9, 2, 22, 6, 8, 5, 25, 2, 31, 2, 10, 5, 6, 2, 9, 26, 10, 40, 2, 17, 11, 2, 9, 16, 39, 32, 2, 11, 30, 18, 9, 11, 2, 8, 11, 2, 36, 6, 20, 9, 11], [5, 17, 11, 12, 2, 5, 6, 2, 9, 24, 8, 32, 2, 36, 6, 25, 2, 10, 19, 10, 5, 12, 2, 10, 5, 6, 2, 9, 26, 10, 40, 2, 17, 11, 2, 9, 16, 39, 32, 2, 10, 6, 23, 21, 25, 2, 8, 11, 2, 41, 9, 15, 2, 41, 11, 8, 12, 2, 10, 9, 11, 14, 41, 9, 2, 17, 11, 2, 9, 19, 6, 5, 2, 8, 34, 5, 2, 29, 5, 22, 2, 18, 9, 2, 6, 5, 17, 15, 19, 6, 42], [10, 23, 9, 10, 6, 11, 2, 41, 9, 15, 2, 8, 10, 26, 10, 21]]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "char_tokenizer = Tokenizer(char_level=True)\n",
    "char_tokenizer.fit_on_texts(sentences_with_tashkeel)\n",
    "char_index = char_tokenizer.word_index\n",
    "char_sequences = char_tokenizer.texts_to_sequences(sentences)\n",
    "print(char_sequences[0:5])\n",
    "print(char_index.get(\".\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding tests for the character-based tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "[[19, 10, 5, 12], [17, 10], [19, 39, 16], [6, 5, 17, 10, 5], [9, 21, 12], [27, 5, 33], [19, 6, 5], [6, 5, 40, 15, 22, 36, 9], [6, 14, 11], [16, 15, 18, 25], [19, 10, 5, 12], [14, 5, 18, 44], [9, 19, 20, 38, 9, 12], [22, 27, 11, 22, 6, 15], [41, 9, 15], [23, 21, 9, 34], [14, 6, 5, 27, 24, 5, 6, 8], [10, 26, 10, 14], [8, 6], [16, 5, 8], [10, 26, 10, 14, 12], [8, 11], [6, 5, 21, 9, 11], [38, 15, 10, 15, 25], [22, 27, 5, 19, 6, 42], [8, 30, 23, 18], [14, 19, 29, 15], [10, 36, 21], [40, 11, 6, 15], [6, 14, 11], [16, 15, 18, 25], [19, 10, 5], [6, 14, 11], [36, 6, 24], [17, 10], [14, 18, 16, 5], [9, 20, 38, 8, 11, 12], [12, 10], [22, 5, 14, 24], [6, 5, 40, 11, 6, 15], [10, 27, 5, 19, 6, 42], [6, 5, 8, 30, 23, 18], [18, 9], [30, 15, 9, 23], [6, 5, 11, 26, 6, 24, 25], [10, 6, 5, 24, 26, 10, 21], [5, 5, 30, 11, 8], [10, 11, 23, 10], [29, 5, 22], [10, 24, 23, 15], [8, 23, 8, 21], [19, 10, 5], [8, 6, 5, 22], [10, 17, 30, 23, 6, 14, 12], [17, 11], [6, 5, 24, 6, 23, 15], [22, 6, 18, 15], [14, 6, 5, 5, 12], [20, 16, 6, 5, 32], [19, 6, 5], [8, 6, 5, 22], [12, 10], [22, 6, 5, 40, 11, 21, 9, 19], [27, 29, 6], [16, 8, 5], [6, 5, 24, 23, 15], [14, 11, 18, 24, 12], [19, 20, 5], [10, 5, 8], [9, 24, 20, 20, 14]]\n"
     ]
    }
   ],
   "source": [
    "print(char_index[\"ط\"])\n",
    "print(char_tokenizer.texts_to_sequences(sentences[0].split(\" \")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: مرحبا كيف حالك\n",
      "Tokenized Sequence: [8, 15, 23, 14, 6, 2, 22, 9, 18, 2, 23, 6, 5, 22]\n",
      "\n",
      "Original Text: السلام عليكم\n",
      "Tokenized Sequence: [6, 5, 24, 5, 6, 8, 2, 16, 5, 9, 22, 8]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# New text data for testing\n",
    "new_texts = [\"مرحبا كيف حالك\", \"السلام عليكم\"]\n",
    "\n",
    "# Tokenize the new text data at the character level\n",
    "sequences_new = char_tokenizer.texts_to_sequences(new_texts)\n",
    "\n",
    "# Print the results\n",
    "for text, sequence in zip(new_texts, sequences_new):\n",
    "    print(f\"Original Text: {text}\")\n",
    "    print(f\"Tokenized Sequence: {sequence}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addding Padding to the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming word_sequences and char_sequences are the output of the tokenizers\n",
    "word_sequences = words_tokenizer.texts_to_sequences(sentences)\n",
    "char_sequences = char_tokenizer.texts_to_sequences(sentences_with_tashkeel)\n",
    "\n",
    "# Add padding\n",
    "word_sequences_padded = pad_sequences(word_sequences, padding='post')\n",
    "char_sequences_padded = pad_sequences(char_sequences, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n",
      "4320\n"
     ]
    }
   ],
   "source": [
    "sequence_lengths = [len(seq) for seq in word_sequences_padded]\n",
    "print(len(char_sequences[5]))\n",
    "print(len(char_sequences_padded[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenized sequences\n",
    "with open('word_sequences.pkl', 'wb') as file:\n",
    "    pickle.dump(word_sequences_padded, file)\n",
    "\n",
    "with open('char_sequences.pkl', 'wb') as file:\n",
    "    pickle.dump(char_sequences_padded, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    8     4   273    91   189    47    14   901    37   458     8  1184\n",
      "  3641 41299    39   249  4154   475    12   163  1738     3   158  1175\n",
      " 27872  6099 41300  5827 41301    37   458    63    37  2652     4  1362\n",
      " 24279    43 17883 57670 41302  2961     2  1430  2148  9577 57671  1189\n",
      "    18 21681    86    63    94  3359     6 16422  1190  1185   108    14\n",
      "    94    43 27873    28   520  8288   415   264    51 57672     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0]\n",
      "[    8     4   273    91   189    47    14   901    37   458     8  1184\n",
      "  3641 41299    39   249  4154   475    12   163  1738     3   158  1175\n",
      " 27872  6099 41300  5827 41301    37   458    63    37  2652     4  1362\n",
      " 24279    43 17883 57670 41302  2961     2  1430  2148  9577 57671  1189\n",
      "    18 21681    86    63    94  3359     6 16422  1190  1185   108    14\n",
      "    94    43 27873    28   520  8288   415   264    51 57672     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "#Testing if pickle works\n",
    "with open('word_sequences.pkl', 'rb') as file:\n",
    "    sequences_padded = pickle.load(file)\n",
    "\n",
    "print(sequences_padded[0])\n",
    "print(word_sequences_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   32     7  1380  1110    88    21   100     6  2543   180     3    39\n",
      "  7575   798     6   179   147    18     2 16423     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0]\n",
      "[   32     7  1380  1110    88    21   100     6  2543   180     3    39\n",
      "  7575   798     6   179   147    18     2 16423     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "print(sequences_padded[3])\n",
    "print(word_sequences_padded[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "['']\n"
     ]
    }
   ],
   "source": [
    "with open('char_sequences.pkl', 'rb') as file:\n",
    "    char_sequences = pickle.load(file)\n",
    "\n",
    "print(char_index.get(\"ُ\"))\n",
    "w = char_sequences[1]\n",
    "print(char_tokenizer.sequences_to_texts([\"19 2 3 4 5 6 7 8\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(char_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTF-8 Encoding Numbers: [1608, 1614, 1588, 1614, 1583, 1617, 1616]\n"
     ]
    }
   ],
   "source": [
    "word = \"فِي\"\n",
    "\n",
    "utf8_encoding_numbers = [ord(char) for char in word]\n",
    "print(\"UTF-8 Encoding Numbers:\", utf8_encoding_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "قوله أو قطع الأوّلت\n",
      "َُُْـَْـَََــََُْْـ\n"
     ]
    }
   ],
   "source": [
    "letters, harakat = araby.separate(\"قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُت\")\n",
    "print(letters)\n",
    "print(len(harakat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ\n",
      "['ق', 'و', 'ل', 'ه', ' ', 'أ', 'و', ' ', 'ق', 'ط', 'ع', ' ', 'ا', 'ل', 'أ', 'و', 'ل']\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "text, txt_list, haraqat_list = util.extract_haraqat(\"قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ\") \n",
    "\n",
    "print(text)\n",
    "print(txt_list)\n",
    "haraqat_list\n",
    "for i in haraqat_list:\n",
    "    print(len(i))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
